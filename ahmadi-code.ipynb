{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14089447,"sourceType":"datasetVersion","datasetId":8971208}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install vmdpy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T05:53:32.696920Z","iopub.execute_input":"2025-12-11T05:53:32.697322Z","iopub.status.idle":"2025-12-11T05:53:37.963653Z","shell.execute_reply.started":"2025-12-11T05:53:32.697288Z","shell.execute_reply":"2025-12-11T05:53:37.962625Z"}},"outputs":[{"name":"stdout","text":"Collecting vmdpy\n  Downloading vmdpy-0.2-py2.py3-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from vmdpy) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->vmdpy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->vmdpy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->vmdpy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->vmdpy) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->vmdpy) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->vmdpy) (2.4.1)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->vmdpy) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->vmdpy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->vmdpy) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->vmdpy) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->vmdpy) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->vmdpy) (2024.2.0)\nDownloading vmdpy-0.2-py2.py3-none-any.whl (6.5 kB)\nInstalling collected packages: vmdpy\nSuccessfully installed vmdpy-0.2\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from pathlib import Path\nimport numpy as np\nimport pandas as pd\n\nimport mne\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nDATA_FOLDER = \"/kaggle/input/ahmadi-dataset\"\nINFO_CSV = f\"{DATA_FOLDER}/subject-info.csv\"\n\nRESAMPLE_TO = 128\n\nRANDOM_STATE = 42\n\nEPOCHS = 20\nBATCH_SIZE = 8\n\n\ndef load_task_edf_to_tensor(folder_path, info_csv_path, resample_to=None):\n    folder = Path(folder_path)\n    if not folder.is_dir():\n        raise NotADirectoryError(f\"{folder_path} is not a valid directory\")\n\n    info_df = pd.read_csv(info_csv_path)\n    label_map = dict(zip(info_df[\"Subject\"], info_df[\"Count quality\"]))\n\n    X_list = []\n    y_list = []\n    subjects = []\n    sfreq = None\n\n    for file_path in sorted(folder.glob(\"Subject*_2.edf\")):\n        subj = file_path.stem.split(\"_\")[0]\n        if subj not in label_map:\n            print(f\"Warning: {subj} not in subject-info, skipping.\")\n            continue\n\n        print(f\"Loading {file_path.name} for {subj}...\")\n        raw = mne.io.read_raw_edf(file_path, preload=True, verbose=False)\n\n        if resample_to is not None:\n            raw.resample(resample_to)\n\n        if sfreq is None:\n            sfreq = raw.info[\"sfreq\"]\n\n        data = raw.get_data()\n        X_list.append(data)\n        y_list.append(int(label_map[subj]))\n        subjects.append(subj)\n\n    if not X_list:\n        raise ValueError(\"No *_2.edf files loaded. Check folder path and file pattern.\")\n\n    lengths = [d.shape[1] for d in X_list]\n    min_len = min(lengths)\n\n    X = np.stack([d[:, :min_len] for d in X_list], axis=0)\n    y = np.array(y_list, dtype=int)\n    subjects = np.array(subjects)\n\n    print(\"Final tensor X shape (N, C, T):\", X.shape)\n    print(\"Labels y shape:\", y.shape)\n\n    return X, y, subjects, sfreq\n\n\ndef build_lstm_model(timesteps, n_features):\n    model = Sequential()\n    model.add(LSTM(64, input_shape=(timesteps, n_features), return_sequences=False))\n    model.add(Dropout(0.5))\n    model.add(Dense(32, activation=\"relu\"))\n    model.add(Dense(1, activation=\"sigmoid\"))\n    model.compile(\n        optimizer=Adam(learning_rate=1e-3),\n        loss=\"binary_crossentropy\",\n        metrics=[\"accuracy\"]\n    )\n    return model\n\n\ndef compute_channel_importance_with_rf(X_train_raw, y_train, random_state):\n    N, C, T = X_train_raw.shape\n    X_rf = X_train_raw.reshape(N, C * T)\n    rf = RandomForestClassifier(\n        n_estimators=300,\n        max_depth=None,\n        random_state=random_state,\n        n_jobs=-1\n    )\n    rf.fit(X_rf, y_train)\n    importances = rf.feature_importances_\n    importances_2d = importances.reshape(C, T)\n    channel_importance = importances_2d.mean(axis=1)\n    chan_perm = np.argsort(-channel_importance)\n    print(\"\\nChannel importance (sorted):\")\n    for rank, ch in enumerate(chan_perm):\n        print(f\"Rank {rank+1}: Channel {ch} -> importance {channel_importance[ch]:.6f}\")\n    return chan_perm, channel_importance\n\n\ndef main():\n    X, y, subjects, sfreq = load_task_edf_to_tensor(\n        DATA_FOLDER,\n        INFO_CSV,\n        resample_to=RESAMPLE_TO\n    )\n\n    N, C, T = X.shape\n    unique_subjects = np.array(subjects)\n    n_subjects = len(unique_subjects)\n\n    results_lstm = []\n    results_models = {\n        \"KNN\": [],\n        \"RandomForest\": [],\n        \"DecisionTree\": [],\n        \"XGBoost\": []\n    }\n\n    gpus = tf.config.list_physical_devices(\"GPU\")\n    if gpus:\n        device_name = \"/GPU:0\"\n        print(\"\\nUsing GPU\")\n    else:\n        device_name = \"/CPU:0\"\n        print(\"\\nGPU not found, using CPU\")\n\n    for fold_idx in range(n_subjects):\n        test_subj = unique_subjects[fold_idx]\n        val_subj = unique_subjects[(fold_idx + 1) % n_subjects]\n\n        test_mask = (subjects == test_subj)\n        val_mask = (subjects == val_subj)\n        train_mask = ~(test_mask | val_mask)\n\n        X_train_raw = X[train_mask]\n        X_val_raw = X[val_mask]\n        X_test_raw = X[test_mask]\n\n        y_train = y[train_mask]\n        y_val = y[val_mask]\n        y_test = y[test_mask]\n\n        print(\"\\n\" + \"=\" * 60)\n        print(f\"Fold {fold_idx+1}/{n_subjects}\")\n        print(f\"Train subjects: {subjects[train_mask]}\")\n        print(f\"Val subject   : {subjects[val_mask]}\")\n        print(f\"Test subject  : {subjects[test_mask]}\")\n\n        chan_perm, chan_scores = compute_channel_importance_with_rf(\n            X_train_raw, y_train, RANDOM_STATE\n        )\n\n        X_train_raw_perm = X_train_raw[:, chan_perm, :]\n        X_val_raw_perm = X_val_raw[:, chan_perm, :]\n        X_test_raw_perm = X_test_raw[:, chan_perm, :]\n\n        X_train_flat_perm = X_train_raw_perm.reshape(X_train_raw_perm.shape[0], C * T)\n        X_test_flat_perm = X_test_raw_perm.reshape(X_test_raw_perm.shape[0], C * T)\n\n        models = {\n            \"KNN\": KNeighborsClassifier(n_neighbors=3),\n            \"RandomForest\": RandomForestClassifier(\n                n_estimators=300,\n                max_depth=None,\n                random_state=RANDOM_STATE,\n                n_jobs=-1\n            ),\n            \"DecisionTree\": DecisionTreeClassifier(\n                max_depth=None,\n                random_state=RANDOM_STATE\n            ),\n            \"XGBoost\": XGBClassifier(\n                n_estimators=300,\n                max_depth=4,\n                learning_rate=0.05,\n                subsample=0.8,\n                colsample_bytree=0.8,\n                objective=\"binary:logistic\",\n                eval_metric=\"logloss\",\n                n_jobs=-1,\n                tree_method=\"hist\"\n            ),\n        }\n\n        for name, model in models.items():\n            model.fit(X_train_flat_perm, y_train)\n            y_pred_m = model.predict(X_test_flat_perm)\n            acc_m = accuracy_score(y_test, y_pred_m)\n            prec_m = precision_score(y_test, y_pred_m, zero_division=0)\n            rec_m = recall_score(y_test, y_pred_m, zero_division=0)\n            f1_m = f1_score(y_test, y_pred_m, zero_division=0)\n            results_models[name].append([acc_m, prec_m, rec_m, f1_m])\n            print(f\"{name} (permuted channels) -> acc: {acc_m:.4f}, prec: {prec_m:.4f}, rec: {rec_m:.4f}, f1: {f1_m:.4f}\")\n\n        X_train_seq_perm = np.transpose(X_train_raw_perm, (0, 2, 1))\n        X_val_seq_perm = np.transpose(X_val_raw_perm, (0, 2, 1))\n        X_test_seq_perm = np.transpose(X_test_raw_perm, (0, 2, 1))\n\n        timesteps = X_train_seq_perm.shape[1]\n        n_features = X_train_seq_perm.shape[2]\n\n        with tf.device(device_name):\n            tf.keras.backend.clear_session()\n            tf.random.set_seed(RANDOM_STATE)\n            model_lstm = build_lstm_model(timesteps, n_features)\n            model_lstm.fit(\n                X_train_seq_perm,\n                y_train,\n                epochs=EPOCHS,\n                batch_size=BATCH_SIZE,\n                validation_data=(X_val_seq_perm, y_val),\n                verbose=1\n            )\n            y_pred_prob = model_lstm.predict(X_test_seq_perm)\n\n        y_pred_lstm = (y_pred_prob >= 0.5).astype(int).ravel()\n\n        acc = accuracy_score(y_test, y_pred_lstm)\n        prec = precision_score(y_test, y_pred_lstm, zero_division=0)\n        rec = recall_score(y_test, y_pred_lstm, zero_division=0)\n        f1 = f1_score(y_test, y_pred_lstm, zero_division=0)\n\n        results_lstm.append([acc, prec, rec, f1])\n\n        print(f\"LSTM (permuted channels) -> acc: {acc:.4f}, prec: {prec:.4f}, rec: {rec:.4f}, f1: {f1:.4f}\")\n\n    results_lstm = np.array(results_lstm)\n    print(\"\\n\" + \"=\" * 60)\n    print(\"Average metrics over subject-wise folds:\")\n    print(\"LSTM (RF-based channel ordering):\")\n    print(f\"Accuracy : {results_lstm[:,0].mean():.4f}\")\n    print(f\"Precision: {results_lstm[:,1].mean():.4f}\")\n    print(f\"Recall   : {results_lstm[:,2].mean():.4f}\")\n    print(f\"F1-score : {results_lstm[:,3].mean():.4f}\")\n\n    for name, vals in results_models.items():\n        arr = np.array(vals)\n        print(f\"\\n{name} (RF-based channel ordering):\")\n        print(f\"Accuracy : {arr[:,0].mean():.4f}\")\n        print(f\"Precision: {arr[:,1].mean():.4f}\")\n        print(f\"Recall   : {arr[:,2].mean():.4f}\")\n        print(f\"F1-score : {arr[:,3].mean():.4f}\")\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T06:18:58.564519Z","iopub.execute_input":"2025-12-11T06:18:58.565108Z"}},"outputs":[{"name":"stdout","text":"Loading Subject00_2.edf for Subject00...\nLoading Subject01_2.edf for Subject01...\nLoading Subject02_2.edf for Subject02...\nLoading Subject03_2.edf for Subject03...\nLoading Subject04_2.edf for Subject04...\nLoading Subject05_2.edf for Subject05...\nLoading Subject06_2.edf for Subject06...\nLoading Subject07_2.edf for Subject07...\nLoading Subject08_2.edf for Subject08...\nLoading Subject09_2.edf for Subject09...\nLoading Subject10_2.edf for Subject10...\nLoading Subject11_2.edf for Subject11...\nLoading Subject12_2.edf for Subject12...\nLoading Subject13_2.edf for Subject13...\nLoading Subject14_2.edf for Subject14...\nLoading Subject15_2.edf for Subject15...\nLoading Subject16_2.edf for Subject16...\nLoading Subject17_2.edf for Subject17...\nLoading Subject18_2.edf for Subject18...\nLoading Subject19_2.edf for Subject19...\nLoading Subject20_2.edf for Subject20...\nLoading Subject21_2.edf for Subject21...\nLoading Subject22_2.edf for Subject22...\nLoading Subject23_2.edf for Subject23...\nLoading Subject24_2.edf for Subject24...\nLoading Subject25_2.edf for Subject25...\nLoading Subject26_2.edf for Subject26...\nLoading Subject27_2.edf for Subject27...\nLoading Subject28_2.edf for Subject28...\nLoading Subject29_2.edf for Subject29...\nLoading Subject30_2.edf for Subject30...\nLoading Subject31_2.edf for Subject31...\nLoading Subject32_2.edf for Subject32...\nLoading Subject33_2.edf for Subject33...\nLoading Subject34_2.edf for Subject34...\nLoading Subject35_2.edf for Subject35...\nFinal tensor X shape (N, C, T): (36, 21, 7936)\nLabels y shape: (36,)\n\nUsing GPU\n\n============================================================\nFold 1/36\nTrain subjects: ['Subject02' 'Subject03' 'Subject04' 'Subject05' 'Subject06' 'Subject07'\n 'Subject08' 'Subject09' 'Subject10' 'Subject11' 'Subject12' 'Subject13'\n 'Subject14' 'Subject15' 'Subject16' 'Subject17' 'Subject18' 'Subject19'\n 'Subject20' 'Subject21' 'Subject22' 'Subject23' 'Subject24' 'Subject25'\n 'Subject26' 'Subject27' 'Subject28' 'Subject29' 'Subject30' 'Subject31'\n 'Subject32' 'Subject33' 'Subject34' 'Subject35']\nVal subject   : ['Subject01']\nTest subject  : ['Subject00']\n\nChannel importance (sorted):\nRank 1: Channel 8 -> importance 0.000009\nRank 2: Channel 4 -> importance 0.000008\nRank 3: Channel 16 -> importance 0.000008\nRank 4: Channel 20 -> importance 0.000007\nRank 5: Channel 2 -> importance 0.000007\nRank 6: Channel 13 -> importance 0.000007\nRank 7: Channel 17 -> importance 0.000007\nRank 8: Channel 12 -> importance 0.000007\nRank 9: Channel 6 -> importance 0.000006\nRank 10: Channel 18 -> importance 0.000006\nRank 11: Channel 7 -> importance 0.000006\nRank 12: Channel 3 -> importance 0.000006\nRank 13: Channel 9 -> importance 0.000006\nRank 14: Channel 5 -> importance 0.000006\nRank 15: Channel 0 -> importance 0.000005\nRank 16: Channel 11 -> importance 0.000005\nRank 17: Channel 15 -> importance 0.000004\nRank 18: Channel 14 -> importance 0.000004\nRank 19: Channel 19 -> importance 0.000004\nRank 20: Channel 10 -> importance 0.000003\nRank 21: Channel 1 -> importance 0.000003\nKNN (permuted channels) -> acc: 0.0000, prec: 0.0000, rec: 0.0000, f1: 0.0000\n","output_type":"stream"},{"name":"stderr","text":"OpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.\nOpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.\nOpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.\nOpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.\nOpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.\nOpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.\nOpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.\nOpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.\nOpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.\nOpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.\nOpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.\nOpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.\nOpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.\nOpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.\nOpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.\nOpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.\nOpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.\nOpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.\nOpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.\nOpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.\nOpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.\nOpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.\nOpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.\nOpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.\nOpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.\nOpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.\nOpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.\nOpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.\nOpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.\nOpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.\nOpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.\nOpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.\nOpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.\nOpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.\nOpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.\n","output_type":"stream"},{"name":"stdout","text":"RandomForest (permuted channels) -> acc: 0.0000, prec: 0.0000, rec: 0.0000, f1: 0.0000\nDecisionTree (permuted channels) -> acc: 0.0000, prec: 0.0000, rec: 0.0000, f1: 0.0000\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}