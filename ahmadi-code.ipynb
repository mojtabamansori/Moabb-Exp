{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14089447,"sourceType":"datasetVersion","datasetId":8971208}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install vmdpy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T18:06:45.298613Z","iopub.execute_input":"2025-12-11T18:06:45.298845Z","iopub.status.idle":"2025-12-11T18:06:50.020625Z","shell.execute_reply.started":"2025-12-11T18:06:45.298821Z","shell.execute_reply":"2025-12-11T18:06:50.019735Z"}},"outputs":[{"name":"stdout","text":"Collecting vmdpy\n  Downloading vmdpy-0.2-py2.py3-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from vmdpy) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->vmdpy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->vmdpy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->vmdpy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->vmdpy) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->vmdpy) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->vmdpy) (2.4.1)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->vmdpy) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->vmdpy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->vmdpy) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->vmdpy) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->vmdpy) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->vmdpy) (2024.2.0)\nDownloading vmdpy-0.2-py2.py3-none-any.whl (6.5 kB)\nInstalling collected packages: vmdpy\nSuccessfully installed vmdpy-0.2\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"---\ntrue implement\n---\n---","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\nimport numpy as np\nimport pandas as pd\n\nimport mne\nfrom mne.time_frequency import psd_array_welch\n\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_score\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# ======================\n# تنظیمات کلی\n# ======================\nDATA_FOLDER = \"/kaggle/input/ahmadi-dataset\"\nINFO_CSV = f\"{DATA_FOLDER}/subject-info.csv\"\n\nRESAMPLE_TO = 128        # می‌تونی 500 هم بذاری ولی کندتر می‌شه\nRANDOM_STATE = 42\n\nWINDOW_SEC = 2.0         # طول سگمنت (ثانیه)\nOVERLAP_RATIO = 0.75     # ۷۵٪ overlap  → step = 0.5s\n\nUSE_BASELINE = True      # از EEG استراحت برای baseline correction استفاده کن\n\n\n# ======================\n# لود داده + baseline correction (اختیاری)\n# ======================\ndef load_task_edf_with_baseline(folder_path, info_csv_path, resample_to=None,\n                                use_baseline=True):\n    \"\"\"\n    برای هر سوژه:\n      - SubjectXX_1.edf (استراحت) و SubjectXX_2.edf (تسک) را می‌خوانیم\n      - اگر use_baseline=True:\n            task_data_bc = task_data - mean(rest_data, axis=time)\n      - در نهایت، داده‌ی baseline-corrected حالت تسک را برمی‌گردانیم\n    \"\"\"\n    folder = Path(folder_path)\n    if not folder.is_dir():\n        raise NotADirectoryError(f\"{folder_path} is not a valid directory\")\n\n    info_df = pd.read_csv(info_csv_path)\n    label_map = dict(zip(info_df[\"Subject\"], info_df[\"Count quality\"]))\n\n    X_list = []\n    y_list = []\n    subjects = []\n    sfreq = None\n\n    for subj_row in info_df[\"Subject\"]:\n        subj_name = subj_row  # مثل Subject00, Subject01, ...\n        task_file = folder / f\"{subj_name}_2.edf\"\n        rest_file = folder / f\"{subj_name}_1.edf\"\n\n        if not task_file.is_file():\n            print(f\"Task file not found for {subj_name}, skipping.\")\n            continue\n\n        if subj_name not in label_map:\n            print(f\"Warning: {subj_name} not in subject-info, skipping.\")\n            continue\n\n        print(f\"Loading {subj_name}_1.edf (rest) and {subj_name}_2.edf (task)...\")\n\n        # --- read rest and task\n        raw_task = mne.io.read_raw_edf(task_file, preload=True, verbose=False)\n\n        if use_baseline:\n            if rest_file.is_file():\n                raw_rest = mne.io.read_raw_edf(rest_file, preload=True, verbose=False)\n            else:\n                print(f\"Rest file not found for {subj_name}, baseline skipped for this subject.\")\n                raw_rest = None\n        else:\n            raw_rest = None\n\n        # --- resample\n        if resample_to is not None:\n            raw_task.resample(resample_to)\n            if raw_rest is not None:\n                raw_rest.resample(resample_to)\n\n        if sfreq is None:\n            sfreq = raw_task.info[\"sfreq\"]\n\n        task_data = raw_task.get_data()  # (C, T_task)\n\n        if use_baseline and raw_rest is not None:\n            rest_data = raw_rest.get_data()  # (C, T_rest)\n            # مطابق توضیح Oran & Yildirim: میانگین ولتاژ resting را کم می‌کنیم\n            # Baseline Correction: average voltage in rest is subtracted from task. \n            baseline = rest_data.mean(axis=1, keepdims=True)\n            task_data = task_data - baseline\n\n        X_list.append(task_data)\n        y_list.append(int(label_map[subj_name]))\n        subjects.append(subj_name)\n\n    if not X_list:\n        raise ValueError(\"No task files loaded. Check folder path and file naming.\")\n\n    # همه سوژه‌ها را تا کوتاه‌ترین طول برش می‌دهیم\n    lengths = [d.shape[1] for d in X_list]\n    min_len = min(lengths)\n\n    X = np.stack([d[:, :min_len] for d in X_list], axis=0)  # (N, C, T)\n    y = np.array(y_list, dtype=int)\n    subjects = np.array(subjects)\n\n    print(\"Final tensor X shape (N, C, T):\", X.shape)\n    print(\"Labels y shape:\", y.shape)\n\n    return X, y, subjects, sfreq\n\n\n# ======================\n# Segment کردن با overlap درصدی\n# ======================\ndef make_segments(X, y, subjects, sfreq, window_sec=2.0, overlap_ratio=0.75):\n    \"\"\"\n    X: (N_subjects, C, T)\n    خروجی:\n      seg_X: (N_segments, C, T_seg)\n      seg_y: (N_segments,)\n      seg_subjects: (N_segments,)\n    \"\"\"\n    N, C, T = X.shape\n    win_size = int(window_sec * sfreq)\n    step_sec = window_sec * (1.0 - overlap_ratio)\n    step = max(1, int(step_sec * sfreq))\n\n    seg_X_list = []\n    seg_y_list = []\n    seg_subj_list = []\n\n    for i in range(N):\n        data = X[i]          # (C, T)\n        subj_label = y[i]\n        subj_id = subjects[i]\n\n        start = 0\n        while start + win_size <= T:\n            seg = data[:, start:start+win_size]  # (C, win_size)\n            seg_X_list.append(seg)\n            seg_y_list.append(subj_label)\n            seg_subj_list.append(subj_id)\n            start += step\n\n    seg_X = np.stack(seg_X_list, axis=0)\n    seg_y = np.array(seg_y_list, dtype=int)\n    seg_subjects = np.array(seg_subj_list)\n\n    print(\"Segmented data shape (N_segments, C, T_seg):\", seg_X.shape)\n    return seg_X, seg_y, seg_subjects\n\n\n# ======================\n# log-bandpower (absolute + relative) مثل قبل\n# ======================\ndef compute_bandpower_features(seg_X, sfreq):\n    \"\"\"\n    seg_X: (N_segments, C, T_seg)\n    خروجی:\n      feats: (N_segments, C * n_bands * 2)\n    \"\"\"\n    band_defs = {\n        \"delta\": (0.5, 4),\n        \"theta\": (4, 8),\n        \"alpha\": (8, 13),\n        \"beta\":  (13, 30),\n    }\n\n    n_segments, C, T_seg = seg_X.shape\n    feats_list = []\n\n    for i in range(n_segments):\n        data = seg_X[i]  # (C, T_seg)\n\n        psd, freqs = psd_array_welch(\n            data[np.newaxis, :, :],\n            sfreq=sfreq,\n            fmin=0.5,\n            fmax=40,\n            n_fft=T_seg,\n            verbose=False\n        )  # (1, C, n_freqs)\n\n        psd = psd[0]  # (C, n_freqs)\n        total_power = psd.sum(axis=1, keepdims=True) + 1e-12\n\n        feat_abs = []\n        feat_rel = []\n\n        for (fmin, fmax) in band_defs.values():\n            band_mask = (freqs >= fmin) & (freqs < fmax)\n            band_power = psd[:, band_mask].mean(axis=1)        # absolute\n            band_power_rel = band_power[:, None] / total_power  # relative\n\n            feat_abs.append(band_power)\n            feat_rel.append(band_power_rel[:, 0])\n\n        feat_abs = np.concatenate(feat_abs, axis=0)  # (C * n_bands,)\n        feat_rel = np.concatenate(feat_rel, axis=0)\n\n        feat_seg = np.concatenate(\n            [np.log10(feat_abs + 1e-12),\n             np.log10(feat_rel + 1e-12)],\n            axis=0\n        )\n        feats_list.append(feat_seg)\n\n    feats = np.stack(feats_list, axis=0)\n    print(\"Feature matrix shape:\", feats.shape)\n    return feats\n\n\n# ======================\n# ارزیابی ۱: LOSO روی سوژه‌ها (واقعی)\n# ======================\ndef evaluate_loso(feats_all, seg_y, seg_subjects):\n    unique_subjects = np.unique(seg_subjects)\n    print(\"Unique subjects:\", unique_subjects)\n\n    seg_metrics = []\n    subj_metrics = []\n\n    for test_subj in unique_subjects:\n        print(\"\\n\" + \"=\" * 60)\n        print(f\"Test subject: {test_subj}\")\n\n        test_mask = (seg_subjects == test_subj)\n        train_mask = ~test_mask\n\n        X_train = feats_all[train_mask]\n        y_train = seg_y[train_mask]\n        X_test = feats_all[test_mask]\n        y_test = seg_y[test_mask]\n\n        print(\"Train segments:\", X_train.shape[0], \" | Test segments:\", X_test.shape[0])\n        print(\"Train label distribution:\", np.bincount(y_train))\n\n        pipe = Pipeline([\n            (\"scaler\", StandardScaler()),\n            (\"clf\", SVC(kernel=\"rbf\",\n                        class_weight=\"balanced\",\n                        probability=False,\n                        random_state=RANDOM_STATE))\n        ])\n\n        param_grid = {\n            \"clf__C\": [1, 10],\n            \"clf__gamma\": [0.01, 0.1]\n        }\n\n        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n\n        grid = GridSearchCV(\n            pipe,\n            param_grid=param_grid,\n            scoring=\"f1\",\n            cv=cv,\n            n_jobs=-1,\n            verbose=0\n        )\n        grid.fit(X_train, y_train)\n        print(\"Best params:\", grid.best_params_, \"| best f1 (cv):\", grid.best_score_)\n\n        best_model = grid.best_estimator_\n\n        # segment-level\n        y_pred_seg = best_model.predict(X_test)\n        acc_s = accuracy_score(y_test, y_pred_seg)\n        prec_s = precision_score(y_test, y_pred_seg, zero_division=0)\n        rec_s = recall_score(y_test, y_pred_seg, zero_division=0)\n        f1_s = f1_score(y_test, y_pred_seg, zero_division=0)\n        print(f\"Segment-level -> acc: {acc_s:.4f}, prec: {prec_s:.4f}, rec: {rec_s:.4f}, f1: {f1_s:.4f}\")\n        seg_metrics.append([acc_s, prec_s, rec_s, f1_s])\n\n        # subject-level (majority vote)\n        true_label = y_test[0]\n        pred_counts = np.bincount(y_pred_seg)\n        pred_label = np.argmax(pred_counts)\n\n        acc_subj = 1.0 if pred_label == true_label else 0.0\n        print(f\"Subject-level -> true: {true_label}, pred: {pred_label}, acc: {acc_subj:.4f}\")\n\n        subj_metrics.append([acc_subj, acc_subj, acc_subj, acc_subj])\n\n    seg_metrics = np.array(seg_metrics)\n    subj_metrics = np.array(subj_metrics)\n\n    print(\"\\n\" + \"=\" * 60)\n    print(\"Average SEGMENT-level (LOSO):\")\n    print(f\"Accuracy : {seg_metrics[:,0].mean():.4f}\")\n    print(f\"Precision: {seg_metrics[:,1].mean():.4f}\")\n    print(f\"Recall   : {seg_metrics[:,2].mean():.4f}\")\n    print(f\"F1-score : {seg_metrics[:,3].mean():.4f}\")\n\n    print(\"\\n\" + \"=\" * 60)\n    print(\"Average SUBJECT-level (LOSO, majority vote):\")\n    print(f\"Accuracy : {subj_metrics[:,0].mean():.4f}\")\n    print(f\"Precision: {subj_metrics[:,1].mean():.4f}\")\n    print(f\"Recall   : {subj_metrics[:,2].mean():.4f}\")\n    print(f\"F1-score : {subj_metrics[:,3].mean():.4f}\")\n\n\n# ======================\n# ارزیابی ۲: 10-fold CV روی همه‌ی سگمنت‌ها (مثل مقاله‌ها – subject-dependent)\n# ======================\ndef evaluate_segment_cv(feats_all, seg_y):\n    pipe = Pipeline([\n        (\"scaler\", StandardScaler()),\n        (\"clf\", SVC(kernel=\"rbf\",\n                    class_weight=\"balanced\",\n                    probability=False,\n                    random_state=RANDOM_STATE))\n    ])\n\n    param_grid = {\n        \"clf__C\": [1, 10],\n        \"clf__gamma\": [0.01, 0.1]\n    }\n\n    cv_inner = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n    grid = GridSearchCV(\n        pipe,\n        param_grid=param_grid,\n        scoring=\"f1\",\n        cv=cv_inner,\n        n_jobs=-1,\n        verbose=0\n    )\n    cv_outer = StratifiedKFold(n_splits=10, shuffle=True, random_state=RANDOM_STATE)\n\n    scores_acc = []\n    scores_f1 = []\n\n    print(\"\\n\" + \"=\" * 60)\n    print(\"10-fold CV over all segments (subject-dependent, شبیه اکثر مقاله‌ها)\")\n\n    for fold_idx, (train_idx, test_idx) in enumerate(cv_outer.split(feats_all, seg_y), start=1):\n        X_train, X_test = feats_all[train_idx], feats_all[test_idx]\n        y_train, y_test = seg_y[train_idx], seg_y[test_idx]\n\n        grid.fit(X_train, y_train)\n        best_model = grid.best_estimator_\n        y_pred = best_model.predict(X_test)\n\n        acc = accuracy_score(y_test, y_pred)\n        f1 = f1_score(y_test, y_pred, zero_division=0)\n\n        print(f\"Fold {fold_idx}: acc={acc:.4f}, f1={f1:.4f}\")\n        scores_acc.append(acc)\n        scores_f1.append(f1)\n\n    print(\"\\nAverage (10-fold CV, segment-level):\")\n    print(f\"Accuracy: {np.mean(scores_acc):.4f} ± {np.std(scores_acc):.4f}\")\n    print(f\"F1-score: {np.mean(scores_f1):.4f} ± {np.std(scores_f1):.4f}\")\n\n\n# ======================\n# main\n# ======================\ndef main():\n    # 1) لود داده + baseline correction\n    X, y, subjects, sfreq = load_task_edf_with_baseline(\n        DATA_FOLDER,\n        INFO_CSV,\n        resample_to=RESAMPLE_TO,\n        use_baseline=USE_BASELINE\n    )\n\n    # 2) سگمنت‌ها\n    seg_X, seg_y, seg_subjects = make_segments(\n        X, y, subjects,\n        sfreq=sfreq,\n        window_sec=WINDOW_SEC,\n        overlap_ratio=OVERLAP_RATIO\n    )\n\n    # 3) فیچرها\n    feats_all = compute_bandpower_features(seg_X, sfreq)\n\n    # 4) ارزیابی واقعی (LOSO)\n    evaluate_loso(feats_all, seg_y, seg_subjects)\n\n    # 5) ارزیابی شبیه مقاله‌ها (10-fold CV روی سگمنت‌ها)\n    evaluate_segment_cv(feats_all, seg_y)\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T18:28:05.417973Z","iopub.execute_input":"2025-12-11T18:28:05.418663Z","iopub.status.idle":"2025-12-11T18:36:39.607074Z","shell.execute_reply.started":"2025-12-11T18:28:05.418635Z","shell.execute_reply":"2025-12-11T18:36:39.606379Z"}},"outputs":[{"name":"stdout","text":"Loading Subject00_1.edf (rest) and Subject00_2.edf (task)...\nLoading Subject01_1.edf (rest) and Subject01_2.edf (task)...\nLoading Subject02_1.edf (rest) and Subject02_2.edf (task)...\nLoading Subject03_1.edf (rest) and Subject03_2.edf (task)...\nLoading Subject04_1.edf (rest) and Subject04_2.edf (task)...\nLoading Subject05_1.edf (rest) and Subject05_2.edf (task)...\nLoading Subject06_1.edf (rest) and Subject06_2.edf (task)...\nLoading Subject07_1.edf (rest) and Subject07_2.edf (task)...\nLoading Subject08_1.edf (rest) and Subject08_2.edf (task)...\nLoading Subject09_1.edf (rest) and Subject09_2.edf (task)...\nLoading Subject10_1.edf (rest) and Subject10_2.edf (task)...\nLoading Subject11_1.edf (rest) and Subject11_2.edf (task)...\nLoading Subject12_1.edf (rest) and Subject12_2.edf (task)...\nLoading Subject13_1.edf (rest) and Subject13_2.edf (task)...\nLoading Subject14_1.edf (rest) and Subject14_2.edf (task)...\nLoading Subject15_1.edf (rest) and Subject15_2.edf (task)...\nLoading Subject16_1.edf (rest) and Subject16_2.edf (task)...\nLoading Subject17_1.edf (rest) and Subject17_2.edf (task)...\nLoading Subject18_1.edf (rest) and Subject18_2.edf (task)...\nLoading Subject19_1.edf (rest) and Subject19_2.edf (task)...\nLoading Subject20_1.edf (rest) and Subject20_2.edf (task)...\nLoading Subject21_1.edf (rest) and Subject21_2.edf (task)...\nLoading Subject22_1.edf (rest) and Subject22_2.edf (task)...\nLoading Subject23_1.edf (rest) and Subject23_2.edf (task)...\nLoading Subject24_1.edf (rest) and Subject24_2.edf (task)...\nLoading Subject25_1.edf (rest) and Subject25_2.edf (task)...\nLoading Subject26_1.edf (rest) and Subject26_2.edf (task)...\nLoading Subject27_1.edf (rest) and Subject27_2.edf (task)...\nLoading Subject28_1.edf (rest) and Subject28_2.edf (task)...\nLoading Subject29_1.edf (rest) and Subject29_2.edf (task)...\nLoading Subject30_1.edf (rest) and Subject30_2.edf (task)...\nLoading Subject31_1.edf (rest) and Subject31_2.edf (task)...\nLoading Subject32_1.edf (rest) and Subject32_2.edf (task)...\nLoading Subject33_1.edf (rest) and Subject33_2.edf (task)...\nLoading Subject34_1.edf (rest) and Subject34_2.edf (task)...\nLoading Subject35_1.edf (rest) and Subject35_2.edf (task)...\nFinal tensor X shape (N, C, T): (36, 21, 7936)\nLabels y shape: (36,)\nSegmented data shape (N_segments, C, T_seg): (4356, 21, 256)\nFeature matrix shape: (4356, 168)\nUnique subjects: ['Subject00' 'Subject01' 'Subject02' 'Subject03' 'Subject04' 'Subject05'\n 'Subject06' 'Subject07' 'Subject08' 'Subject09' 'Subject10' 'Subject11'\n 'Subject12' 'Subject13' 'Subject14' 'Subject15' 'Subject16' 'Subject17'\n 'Subject18' 'Subject19' 'Subject20' 'Subject21' 'Subject22' 'Subject23'\n 'Subject24' 'Subject25' 'Subject26' 'Subject27' 'Subject28' 'Subject29'\n 'Subject30' 'Subject31' 'Subject32' 'Subject33' 'Subject34' 'Subject35']\n\n============================================================\nTest subject: Subject00\nTrain segments: 4235  | Test segments: 121\nTrain label distribution: [1089 3146]\nBest params: {'clf__C': 10, 'clf__gamma': 0.01} | best f1 (cv): 0.994623242647681\nSegment-level -> acc: 0.0165, prec: 0.0000, rec: 0.0000, f1: 0.0000\nSubject-level -> true: 0, pred: 1, acc: 0.0000\n\n============================================================\nTest subject: Subject01\nTrain segments: 4235  | Test segments: 121\nTrain label distribution: [1210 3025]\nBest params: {'clf__C': 10, 'clf__gamma': 0.01} | best f1 (cv): 0.9940870328781368\nSegment-level -> acc: 0.9669, prec: 1.0000, rec: 0.9669, f1: 0.9832\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n============================================================\nTest subject: Subject02\nTrain segments: 4235  | Test segments: 121\nTrain label distribution: [1210 3025]\nBest params: {'clf__C': 10, 'clf__gamma': 0.01} | best f1 (cv): 0.9940870328781368\nSegment-level -> acc: 0.9917, prec: 1.0000, rec: 0.9917, f1: 0.9959\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n============================================================\nTest subject: Subject03\nTrain segments: 4235  | Test segments: 121\nTrain label distribution: [1210 3025]\nBest params: {'clf__C': 10, 'clf__gamma': 0.01} | best f1 (cv): 0.9942494879945316\nSegment-level -> acc: 1.0000, prec: 1.0000, rec: 1.0000, f1: 1.0000\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n============================================================\nTest subject: Subject04\nTrain segments: 4235  | Test segments: 121\nTrain label distribution: [1089 3146]\nBest params: {'clf__C': 10, 'clf__gamma': 0.01} | best f1 (cv): 0.9949424218474336\nSegment-level -> acc: 0.0000, prec: 0.0000, rec: 0.0000, f1: 0.0000\nSubject-level -> true: 0, pred: 1, acc: 0.0000\n\n============================================================\nTest subject: Subject05\nTrain segments: 4235  | Test segments: 121\nTrain label distribution: [1210 3025]\nBest params: {'clf__C': 10, 'clf__gamma': 0.01} | best f1 (cv): 0.9940824467365559\nSegment-level -> acc: 0.6364, prec: 1.0000, rec: 0.6364, f1: 0.7778\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n============================================================\nTest subject: Subject06\nTrain segments: 4235  | Test segments: 121\nTrain label distribution: [1089 3146]\nBest params: {'clf__C': 10, 'clf__gamma': 0.01} | best f1 (cv): 0.9954142471172862\nSegment-level -> acc: 0.0248, prec: 0.0000, rec: 0.0000, f1: 0.0000\nSubject-level -> true: 0, pred: 1, acc: 0.0000\n\n============================================================\nTest subject: Subject07\nTrain segments: 4235  | Test segments: 121\nTrain label distribution: [1210 3025]\nBest params: {'clf__C': 10, 'clf__gamma': 0.01} | best f1 (cv): 0.9944122098680962\nSegment-level -> acc: 0.9008, prec: 1.0000, rec: 0.9008, f1: 0.9478\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n============================================================\nTest subject: Subject08\nTrain segments: 4235  | Test segments: 121\nTrain label distribution: [1210 3025]\nBest params: {'clf__C': 10, 'clf__gamma': 0.01} | best f1 (cv): 0.9945776278047436\nSegment-level -> acc: 0.8843, prec: 1.0000, rec: 0.8843, f1: 0.9386\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n============================================================\nTest subject: Subject09\nTrain segments: 4235  | Test segments: 121\nTrain label distribution: [1089 3146]\nBest params: {'clf__C': 10, 'clf__gamma': 0.01} | best f1 (cv): 0.9951045322261104\nSegment-level -> acc: 0.0661, prec: 0.0000, rec: 0.0000, f1: 0.0000\nSubject-level -> true: 0, pred: 1, acc: 0.0000\n\n============================================================\nTest subject: Subject10\nTrain segments: 4235  | Test segments: 121\nTrain label distribution: [1089 3146]\nBest params: {'clf__C': 10, 'clf__gamma': 0.01} | best f1 (cv): 0.9958890587233771\nSegment-level -> acc: 0.1240, prec: 0.0000, rec: 0.0000, f1: 0.0000\nSubject-level -> true: 0, pred: 1, acc: 0.0000\n\n============================================================\nTest subject: Subject11\nTrain segments: 4235  | Test segments: 121\nTrain label distribution: [1210 3025]\nBest params: {'clf__C': 10, 'clf__gamma': 0.01} | best f1 (cv): 0.9940808278645236\nSegment-level -> acc: 0.9669, prec: 1.0000, rec: 0.9669, f1: 0.9832\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n============================================================\nTest subject: Subject12\nTrain segments: 4235  | Test segments: 121\nTrain label distribution: [1210 3025]\nBest params: {'clf__C': 10, 'clf__gamma': 0.01} | best f1 (cv): 0.9942494879945316\nSegment-level -> acc: 0.7851, prec: 1.0000, rec: 0.7851, f1: 0.8796\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n============================================================\nTest subject: Subject13\nTrain segments: 4235  | Test segments: 121\nTrain label distribution: [1210 3025]\nBest params: {'clf__C': 10, 'clf__gamma': 0.01} | best f1 (cv): 0.9939186447622582\nSegment-level -> acc: 0.9835, prec: 1.0000, rec: 0.9835, f1: 0.9917\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n============================================================\nTest subject: Subject14\nTrain segments: 4235  | Test segments: 121\nTrain label distribution: [1089 3146]\nBest params: {'clf__C': 10, 'clf__gamma': 0.01} | best f1 (cv): 0.9946299355050258\nSegment-level -> acc: 0.1983, prec: 0.0000, rec: 0.0000, f1: 0.0000\nSubject-level -> true: 0, pred: 1, acc: 0.0000\n\n============================================================\nTest subject: Subject15\nTrain segments: 4235  | Test segments: 121\nTrain label distribution: [1210 3025]\nBest params: {'clf__C': 10, 'clf__gamma': 0.01} | best f1 (cv): 0.9939183726932834\nSegment-level -> acc: 0.5785, prec: 1.0000, rec: 0.5785, f1: 0.7330\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n============================================================\nTest subject: Subject16\nTrain segments: 4235  | Test segments: 121\nTrain label distribution: [1210 3025]\nBest params: {'clf__C': 10, 'clf__gamma': 0.01} | best f1 (cv): 0.9939156719595437\nSegment-level -> acc: 0.9917, prec: 1.0000, rec: 0.9917, f1: 0.9959\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n============================================================\nTest subject: Subject17\nTrain segments: 4235  | Test segments: 121\nTrain label distribution: [1210 3025]\nBest params: {'clf__C': 10, 'clf__gamma': 0.01} | best f1 (cv): 0.9942467845724702\nSegment-level -> acc: 0.9917, prec: 1.0000, rec: 0.9917, f1: 0.9959\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n============================================================\nTest subject: Subject18\nTrain segments: 4235  | Test segments: 121\nTrain label distribution: [1210 3025]\nBest params: {'clf__C': 10, 'clf__gamma': 0.01} | best f1 (cv): 0.9935872513601123\nSegment-level -> acc: 0.9421, prec: 1.0000, rec: 0.9421, f1: 0.9702\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n============================================================\nTest subject: Subject19\nTrain segments: 4235  | Test segments: 121\nTrain label distribution: [1089 3146]\nBest params: {'clf__C': 10, 'clf__gamma': 0.01} | best f1 (cv): 0.9947858050850575\nSegment-level -> acc: 0.0000, prec: 0.0000, rec: 0.0000, f1: 0.0000\nSubject-level -> true: 0, pred: 1, acc: 0.0000\n\n============================================================\nTest subject: Subject20\nTrain segments: 4235  | Test segments: 121\nTrain label distribution: [1210 3025]\nBest params: {'clf__C': 10, 'clf__gamma': 0.01} | best f1 (cv): 0.9939156632392988\nSegment-level -> acc: 1.0000, prec: 1.0000, rec: 1.0000, f1: 1.0000\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n============================================================\nTest subject: Subject21\nTrain segments: 4235  | Test segments: 121\nTrain label distribution: [1089 3146]\nBest params: {'clf__C': 1, 'clf__gamma': 0.01} | best f1 (cv): 0.9944710715966261\nSegment-level -> acc: 0.0744, prec: 0.0000, rec: 0.0000, f1: 0.0000\nSubject-level -> true: 0, pred: 1, acc: 0.0000\n\n============================================================\nTest subject: Subject22\nTrain segments: 4235  | Test segments: 121\nTrain label distribution: [1089 3146]\nBest params: {'clf__C': 10, 'clf__gamma': 0.01} | best f1 (cv): 0.9954144818695776\nSegment-level -> acc: 0.6033, prec: 0.0000, rec: 0.0000, f1: 0.0000\nSubject-level -> true: 0, pred: 0, acc: 1.0000\n\n============================================================\nTest subject: Subject23\nTrain segments: 4235  | Test segments: 121\nTrain label distribution: [1210 3025]\nBest params: {'clf__C': 10, 'clf__gamma': 0.01} | best f1 (cv): 0.9939159204568002\nSegment-level -> acc: 0.8347, prec: 1.0000, rec: 0.8347, f1: 0.9099\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n============================================================\nTest subject: Subject24\nTrain segments: 4235  | Test segments: 121\nTrain label distribution: [1210 3025]\nBest params: {'clf__C': 10, 'clf__gamma': 0.01} | best f1 (cv): 0.9939148472548187\nSegment-level -> acc: 0.6364, prec: 1.0000, rec: 0.6364, f1: 0.7778\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n============================================================\nTest subject: Subject25\nTrain segments: 4235  | Test segments: 121\nTrain label distribution: [1210 3025]\nBest params: {'clf__C': 10, 'clf__gamma': 0.01} | best f1 (cv): 0.9937494330946091\nSegment-level -> acc: 0.9504, prec: 1.0000, rec: 0.9504, f1: 0.9746\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n============================================================\nTest subject: Subject26\nTrain segments: 4235  | Test segments: 121\nTrain label distribution: [1210 3025]\nBest params: {'clf__C': 10, 'clf__gamma': 0.01} | best f1 (cv): 0.993915659920965\nSegment-level -> acc: 0.5620, prec: 1.0000, rec: 0.5620, f1: 0.7196\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n============================================================\nTest subject: Subject27\nTrain segments: 4235  | Test segments: 121\nTrain label distribution: [1210 3025]\nBest params: {'clf__C': 10, 'clf__gamma': 0.01} | best f1 (cv): 0.9932577738416013\nSegment-level -> acc: 0.5124, prec: 1.0000, rec: 0.5124, f1: 0.6776\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n============================================================\nTest subject: Subject28\nTrain segments: 4235  | Test segments: 121\nTrain label distribution: [1210 3025]\nBest params: {'clf__C': 10, 'clf__gamma': 0.01} | best f1 (cv): 0.9937524032288417\nSegment-level -> acc: 0.9917, prec: 1.0000, rec: 0.9917, f1: 0.9959\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n============================================================\nTest subject: Subject29\nTrain segments: 4235  | Test segments: 121\nTrain label distribution: [1210 3025]\nBest params: {'clf__C': 10, 'clf__gamma': 0.01} | best f1 (cv): 0.9940816213358377\nSegment-level -> acc: 0.9587, prec: 1.0000, rec: 0.9587, f1: 0.9789\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n============================================================\nTest subject: Subject30\nTrain segments: 4235  | Test segments: 121\nTrain label distribution: [1089 3146]\nBest params: {'clf__C': 10, 'clf__gamma': 0.01} | best f1 (cv): 0.9952563848037922\nSegment-level -> acc: 0.0331, prec: 0.0000, rec: 0.0000, f1: 0.0000\nSubject-level -> true: 0, pred: 1, acc: 0.0000\n\n============================================================\nTest subject: Subject31\nTrain segments: 4235  | Test segments: 121\nTrain label distribution: [1210 3025]\nBest params: {'clf__C': 10, 'clf__gamma': 0.01} | best f1 (cv): 0.9940805463716288\nSegment-level -> acc: 0.9008, prec: 1.0000, rec: 0.9008, f1: 0.9478\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n============================================================\nTest subject: Subject32\nTrain segments: 4235  | Test segments: 121\nTrain label distribution: [1210 3025]\nBest params: {'clf__C': 10, 'clf__gamma': 0.01} | best f1 (cv): 0.9937532141108786\nSegment-level -> acc: 0.9008, prec: 1.0000, rec: 0.9008, f1: 0.9478\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n============================================================\nTest subject: Subject33\nTrain segments: 4235  | Test segments: 121\nTrain label distribution: [1210 3025]\nBest params: {'clf__C': 10, 'clf__gamma': 0.01} | best f1 (cv): 0.9939162033991902\nSegment-level -> acc: 1.0000, prec: 1.0000, rec: 1.0000, f1: 1.0000\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n============================================================\nTest subject: Subject34\nTrain segments: 4235  | Test segments: 121\nTrain label distribution: [1210 3025]\nBest params: {'clf__C': 10, 'clf__gamma': 0.01} | best f1 (cv): 0.9939180877034808\nSegment-level -> acc: 0.8347, prec: 1.0000, rec: 0.8347, f1: 0.9099\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n============================================================\nTest subject: Subject35\nTrain segments: 4235  | Test segments: 121\nTrain label distribution: [1210 3025]\nBest params: {'clf__C': 10, 'clf__gamma': 0.01} | best f1 (cv): 0.9939178244980642\nSegment-level -> acc: 1.0000, prec: 1.0000, rec: 1.0000, f1: 1.0000\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n============================================================\nAverage SEGMENT-level (LOSO):\nAccuracy : 0.6623\nPrecision: 0.7222\nRecall   : 0.6306\nF1-score : 0.6676\n\n============================================================\nAverage SUBJECT-level (LOSO, majority vote):\nAccuracy : 0.7500\nPrecision: 0.7500\nRecall   : 0.7500\nF1-score : 0.7500\n\n============================================================\n10-fold CV over all segments (subject-dependent, شبیه اکثر مقاله‌ها)\nFold 1: acc=0.9931, f1=0.9953\nFold 2: acc=0.9977, f1=0.9984\nFold 3: acc=0.9931, f1=0.9953\nFold 4: acc=0.9885, f1=0.9921\nFold 5: acc=0.9954, f1=0.9968\nFold 6: acc=0.9977, f1=0.9984\nFold 7: acc=0.9931, f1=0.9952\nFold 8: acc=0.9885, f1=0.9921\nFold 9: acc=0.9908, f1=0.9937\nFold 10: acc=0.9908, f1=0.9937\n\nAverage (10-fold CV, segment-level):\nAccuracy: 0.9929 ± 0.0032\nF1-score: 0.9951 ± 0.0022\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# eegmat_data.py\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport mne\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nDATA_FOLDER = \"/kaggle/input/ahmadi-dataset\"\nINFO_CSV = f\"{DATA_FOLDER}/subject-info.csv\"\n\nRESAMPLE_TO = 128\nWINDOW_SEC = 2.0\nOVERLAP_RATIO = 0.75  # 75%\n\ndef load_task_edf_with_baseline(folder_path=DATA_FOLDER,\n                                info_csv_path=INFO_CSV,\n                                resample_to=RESAMPLE_TO,\n                                use_baseline=True):\n    folder = Path(folder_path)\n    info_df = pd.read_csv(info_csv_path)\n    label_map = dict(zip(info_df[\"Subject\"], info_df[\"Count quality\"]))\n\n    X_list, y_list, subjects = [], [], []\n    sfreq = None\n\n    for subj_name in info_df[\"Subject\"]:\n        task_file = folder / f\"{subj_name}_2.edf\"\n        rest_file = folder / f\"{subj_name}_1.edf\"\n\n        if not task_file.is_file():\n            print(f\"Task file not found for {subj_name}, skipping.\")\n            continue\n\n        if subj_name not in label_map:\n            print(f\"{subj_name} not in subject-info, skipping.\")\n            continue\n\n        print(f\"Loading {subj_name}_1.edf (rest) and {subj_name}_2.edf (task)...\")\n        raw_task = mne.io.read_raw_edf(task_file, preload=True, verbose=False)\n\n        raw_rest = None\n        if use_baseline and rest_file.is_file():\n            raw_rest = mne.io.read_raw_edf(rest_file, preload=True, verbose=False)\n\n        if resample_to is not None:\n            raw_task.resample(resample_to)\n            if raw_rest is not None:\n                raw_rest.resample(resample_to)\n\n        if sfreq is None:\n            sfreq = raw_task.info[\"sfreq\"]\n\n        task_data = raw_task.get_data()  # (C, T)\n        if use_baseline and raw_rest is not None:\n            rest_data = raw_rest.get_data()\n            baseline = rest_data.mean(axis=1, keepdims=True)\n            task_data = task_data - baseline\n\n        X_list.append(task_data)\n        y_list.append(int(label_map[subj_name]))\n        subjects.append(subj_name)\n\n    if not X_list:\n        raise ValueError(\"No task files loaded.\")\n\n    lengths = [d.shape[1] for d in X_list]\n    min_len = min(lengths)\n    X = np.stack([d[:, :min_len] for d in X_list], axis=0)  # (N, C, T)\n    y = np.array(y_list, dtype=int)\n    subjects = np.array(subjects)\n\n    print(\"Final tensor X shape (N, C, T):\", X.shape)\n    print(\"Labels y shape:\", y.shape)\n\n    return X, y, subjects, sfreq\n\n\ndef make_segments(X, y, subjects, sfreq, window_sec=WINDOW_SEC, overlap_ratio=OVERLAP_RATIO):\n    N, C, T = X.shape\n    win_size = int(window_sec * sfreq)\n    step_sec = window_sec * (1.0 - overlap_ratio)\n    step = max(1, int(step_sec * sfreq))\n\n    seg_X_list, seg_y_list, seg_subj_list = [], [], []\n\n    for i in range(N):\n        data = X[i]\n        subj_label = y[i]\n        subj_id = subjects[i]\n\n        start = 0\n        while start + win_size <= T:\n            seg = data[:, start:start+win_size]\n            seg_X_list.append(seg)\n            seg_y_list.append(subj_label)\n            seg_subj_list.append(subj_id)\n            start += step\n\n    seg_X = np.stack(seg_X_list, axis=0)\n    seg_y = np.array(seg_y_list, dtype=int)\n    seg_subjects = np.array(seg_subj_list)\n\n    print(\"Segmented data shape:\", seg_X.shape)\n    return seg_X, seg_y, seg_subjects\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T18:51:36.789877Z","iopub.execute_input":"2025-12-11T18:51:36.790641Z","iopub.status.idle":"2025-12-11T18:51:36.802506Z","shell.execute_reply.started":"2025-12-11T18:51:36.790613Z","shell.execute_reply":"2025-12-11T18:51:36.801769Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"pip install pyriemann\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T18:48:08.500031Z","iopub.execute_input":"2025-12-11T18:48:08.500301Z","iopub.status.idle":"2025-12-11T18:48:15.885899Z","shell.execute_reply.started":"2025-12-11T18:48:08.500281Z","shell.execute_reply":"2025-12-11T18:48:15.885099Z"}},"outputs":[{"name":"stdout","text":"Collecting pyriemann\n  Downloading pyriemann-0.9-py2.py3-none-any.whl.metadata (9.3 kB)\nCollecting numpy>=2.0.0 (from pyriemann)\n  Downloading numpy-2.3.5-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from pyriemann) (1.15.3)\nRequirement already satisfied: scikit-learn>=0.24 in /usr/local/lib/python3.11/dist-packages (from pyriemann) (1.2.2)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from pyriemann) (1.5.2)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from pyriemann) (3.7.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.24->pyriemann) (3.6.0)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pyriemann) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pyriemann) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pyriemann) (4.59.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pyriemann) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pyriemann) (25.0)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pyriemann) (11.3.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pyriemann) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pyriemann) (2.9.0.post0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->pyriemann) (1.17.0)\nDownloading pyriemann-0.9-py2.py3-none-any.whl (127 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.7/127.7 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading numpy-2.3.5-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m101.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: numpy, pyriemann\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.26.4\n    Uninstalling numpy-1.26.4:\n      Successfully uninstalled numpy-1.26.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nmkl-umath 0.1.1 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.3.5 which is incompatible.\nmkl-random 1.2.4 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.3.5 which is incompatible.\nmkl-fft 1.3.8 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.3.5 which is incompatible.\nnumba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.5 which is incompatible.\ndatasets 4.4.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nydata-profiling 4.17.0 requires numpy<2.2,>=1.16.0, but you have numpy 2.3.5 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nopencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\nopencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\nopencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\ntensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.3.5 which is incompatible.\ntensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\numap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.2.2 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed numpy-2.3.5 pyriemann-0.9\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# riemann_loso.py\nimport numpy as np\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.linear_model import LogisticRegression\n\nfrom pyriemann.estimation import Covariances\nfrom pyriemann.tangentspace import TangentSpace\nfrom pyriemann.utils.mean import mean_riemann\n\n# from eegmat_data import load_task_edf_with_baseline, make_segments\n\n\nRANDOM_STATE = 42\n\n\ndef compute_covariances(seg_X):\n    \"\"\"\n    seg_X: (N_segments, C, T)\n    returns: covs (N_segments, C, C)\n    \"\"\"\n    cov_est = Covariances(estimator='oas')\n    covs = cov_est.fit_transform(seg_X)\n    return covs\n\n\ndef align_per_subject(covs, seg_subjects):\n    \"\"\"\n    Riemannian centering per subject:\n    cov_aligned = M_s^{-1/2} * C * M_s^{-1/2}\n    \"\"\"\n    from scipy.linalg import fractional_matrix_power\n\n    covs_aligned = np.zeros_like(covs)\n    unique_subj = np.unique(seg_subjects)\n\n    for subj in unique_subj:\n        mask = (seg_subjects == subj)\n        cov_subj = covs[mask]  # (N_subj, C, C)\n        M = mean_riemann(cov_subj)  # (C, C)\n        Minv_half = fractional_matrix_power(M, -0.5)\n        for i_idx, c in zip(np.where(mask)[0], cov_subj):\n            covs_aligned[i_idx] = Minv_half @ c @ Minv_half.T\n\n    return covs_aligned\n\n\ndef evaluate_riemann_loso(use_alignment=False, clf_type=\"svm\"):\n    X, y, subjects, sfreq = load_task_edf_with_baseline()\n    seg_X, seg_y, seg_subjects = make_segments(X, y, subjects, sfreq)\n\n    covs = compute_covariances(seg_X)\n    if use_alignment:\n        print(\"Applying per-subject Riemannian alignment...\")\n        covs = align_per_subject(covs, seg_subjects)\n\n    ts = TangentSpace()\n    feats_all = ts.fit_transform(covs)  # (N_segments, n_features)\n\n    unique_subjects = np.unique(seg_subjects)\n    seg_metrics = []\n    subj_metrics = []\n\n    for test_subj in unique_subjects:\n        print(\"\\n\" + \"=\" * 50)\n        print(\"Test subject:\", test_subj)\n\n        test_mask = (seg_subjects == test_subj)\n        train_mask = ~test_mask\n\n        X_train = feats_all[train_mask]\n        y_train = seg_y[train_mask]\n        X_test = feats_all[test_mask]\n        y_test = seg_y[test_mask]\n\n        print(\"Train segments:\", X_train.shape[0], \"| Test segments:\", X_test.shape[0])\n\n        if clf_type == \"svm\":\n            clf = SVC(kernel=\"rbf\", class_weight=\"balanced\", probability=False, random_state=RANDOM_STATE)\n        else:\n            clf = LogisticRegression(max_iter=2000, class_weight=\"balanced\", random_state=RANDOM_STATE)\n\n        clf.fit(X_train, y_train)\n        y_pred_seg = clf.predict(X_test)\n\n        acc_s = accuracy_score(y_test, y_pred_seg)\n        prec_s = precision_score(y_test, y_pred_seg, zero_division=0)\n        rec_s = recall_score(y_test, y_pred_seg, zero_division=0)\n        f1_s = f1_score(y_test, y_pred_seg, zero_division=0)\n        print(f\"Segment-level -> acc: {acc_s:.4f}, prec: {prec_s:.4f}, rec: {rec_s:.4f}, f1: {f1_s:.4f}\")\n        seg_metrics.append([acc_s, prec_s, rec_s, f1_s])\n\n        # subject-level majority vote\n        true_label = y_test[0]\n        counts = np.bincount(y_pred_seg)\n        pred_label = np.argmax(counts)\n        acc_subj = 1.0 if pred_label == true_label else 0.0\n        print(f\"Subject-level -> true: {true_label}, pred: {pred_label}, acc: {acc_subj:.4f}\")\n        subj_metrics.append([acc_subj, acc_subj, acc_subj, acc_subj])\n\n    seg_metrics = np.array(seg_metrics)\n    subj_metrics = np.array(subj_metrics)\n\n    print(\"\\n\" + \"=\" * 50)\n    print(\"Average SEGMENT-level (LOSO):\")\n    print(\"Accuracy :\", seg_metrics[:, 0].mean())\n    print(\"Precision:\", seg_metrics[:, 1].mean())\n    print(\"Recall   :\", seg_metrics[:, 2].mean())\n    print(\"F1-score :\", seg_metrics[:, 3].mean())\n\n    print(\"\\nAverage SUBJECT-level (LOSO, majority vote):\")\n    print(\"Accuracy :\", subj_metrics[:, 0].mean())\n\n\nif __name__ == \"__main__\":\n    print(\"=== Riemannian (no alignment) ===\")\n    evaluate_riemann_loso(use_alignment=False, clf_type=\"svm\")\n\n    print(\"\\n\\n=== Riemannian + per-subject alignment ===\")\n    evaluate_riemann_loso(use_alignment=True, clf_type=\"svm\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T18:51:46.549589Z","iopub.execute_input":"2025-12-11T18:51:46.550098Z","iopub.status.idle":"2025-12-11T18:53:27.613043Z","shell.execute_reply.started":"2025-12-11T18:51:46.550074Z","shell.execute_reply":"2025-12-11T18:53:27.612398Z"}},"outputs":[{"name":"stdout","text":"=== Riemannian (no alignment) ===\nLoading Subject00_1.edf (rest) and Subject00_2.edf (task)...\nLoading Subject01_1.edf (rest) and Subject01_2.edf (task)...\nLoading Subject02_1.edf (rest) and Subject02_2.edf (task)...\nLoading Subject03_1.edf (rest) and Subject03_2.edf (task)...\nLoading Subject04_1.edf (rest) and Subject04_2.edf (task)...\nLoading Subject05_1.edf (rest) and Subject05_2.edf (task)...\nLoading Subject06_1.edf (rest) and Subject06_2.edf (task)...\nLoading Subject07_1.edf (rest) and Subject07_2.edf (task)...\nLoading Subject08_1.edf (rest) and Subject08_2.edf (task)...\nLoading Subject09_1.edf (rest) and Subject09_2.edf (task)...\nLoading Subject10_1.edf (rest) and Subject10_2.edf (task)...\nLoading Subject11_1.edf (rest) and Subject11_2.edf (task)...\nLoading Subject12_1.edf (rest) and Subject12_2.edf (task)...\nLoading Subject13_1.edf (rest) and Subject13_2.edf (task)...\nLoading Subject14_1.edf (rest) and Subject14_2.edf (task)...\nLoading Subject15_1.edf (rest) and Subject15_2.edf (task)...\nLoading Subject16_1.edf (rest) and Subject16_2.edf (task)...\nLoading Subject17_1.edf (rest) and Subject17_2.edf (task)...\nLoading Subject18_1.edf (rest) and Subject18_2.edf (task)...\nLoading Subject19_1.edf (rest) and Subject19_2.edf (task)...\nLoading Subject20_1.edf (rest) and Subject20_2.edf (task)...\nLoading Subject21_1.edf (rest) and Subject21_2.edf (task)...\nLoading Subject22_1.edf (rest) and Subject22_2.edf (task)...\nLoading Subject23_1.edf (rest) and Subject23_2.edf (task)...\nLoading Subject24_1.edf (rest) and Subject24_2.edf (task)...\nLoading Subject25_1.edf (rest) and Subject25_2.edf (task)...\nLoading Subject26_1.edf (rest) and Subject26_2.edf (task)...\nLoading Subject27_1.edf (rest) and Subject27_2.edf (task)...\nLoading Subject28_1.edf (rest) and Subject28_2.edf (task)...\nLoading Subject29_1.edf (rest) and Subject29_2.edf (task)...\nLoading Subject30_1.edf (rest) and Subject30_2.edf (task)...\nLoading Subject31_1.edf (rest) and Subject31_2.edf (task)...\nLoading Subject32_1.edf (rest) and Subject32_2.edf (task)...\nLoading Subject33_1.edf (rest) and Subject33_2.edf (task)...\nLoading Subject34_1.edf (rest) and Subject34_2.edf (task)...\nLoading Subject35_1.edf (rest) and Subject35_2.edf (task)...\nFinal tensor X shape (N, C, T): (36, 21, 7936)\nLabels y shape: (36,)\nSegmented data shape: (4356, 21, 256)\n\n==================================================\nTest subject: Subject00\nTrain segments: 4235 | Test segments: 121\nSegment-level -> acc: 0.0000, prec: 0.0000, rec: 0.0000, f1: 0.0000\nSubject-level -> true: 0, pred: 1, acc: 0.0000\n\n==================================================\nTest subject: Subject01\nTrain segments: 4235 | Test segments: 121\nSegment-level -> acc: 0.9587, prec: 1.0000, rec: 0.9587, f1: 0.9789\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n==================================================\nTest subject: Subject02\nTrain segments: 4235 | Test segments: 121\nSegment-level -> acc: 1.0000, prec: 1.0000, rec: 1.0000, f1: 1.0000\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n==================================================\nTest subject: Subject03\nTrain segments: 4235 | Test segments: 121\nSegment-level -> acc: 1.0000, prec: 1.0000, rec: 1.0000, f1: 1.0000\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n==================================================\nTest subject: Subject04\nTrain segments: 4235 | Test segments: 121\nSegment-level -> acc: 0.0331, prec: 0.0000, rec: 0.0000, f1: 0.0000\nSubject-level -> true: 0, pred: 1, acc: 0.0000\n\n==================================================\nTest subject: Subject05\nTrain segments: 4235 | Test segments: 121\nSegment-level -> acc: 0.9917, prec: 1.0000, rec: 0.9917, f1: 0.9959\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n==================================================\nTest subject: Subject06\nTrain segments: 4235 | Test segments: 121\nSegment-level -> acc: 0.0000, prec: 0.0000, rec: 0.0000, f1: 0.0000\nSubject-level -> true: 0, pred: 1, acc: 0.0000\n\n==================================================\nTest subject: Subject07\nTrain segments: 4235 | Test segments: 121\nSegment-level -> acc: 0.9587, prec: 1.0000, rec: 0.9587, f1: 0.9789\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n==================================================\nTest subject: Subject08\nTrain segments: 4235 | Test segments: 121\nSegment-level -> acc: 0.8843, prec: 1.0000, rec: 0.8843, f1: 0.9386\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n==================================================\nTest subject: Subject09\nTrain segments: 4235 | Test segments: 121\nSegment-level -> acc: 0.0496, prec: 0.0000, rec: 0.0000, f1: 0.0000\nSubject-level -> true: 0, pred: 1, acc: 0.0000\n\n==================================================\nTest subject: Subject10\nTrain segments: 4235 | Test segments: 121\nSegment-level -> acc: 0.0000, prec: 0.0000, rec: 0.0000, f1: 0.0000\nSubject-level -> true: 0, pred: 1, acc: 0.0000\n\n==================================================\nTest subject: Subject11\nTrain segments: 4235 | Test segments: 121\nSegment-level -> acc: 1.0000, prec: 1.0000, rec: 1.0000, f1: 1.0000\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n==================================================\nTest subject: Subject12\nTrain segments: 4235 | Test segments: 121\nSegment-level -> acc: 0.2314, prec: 1.0000, rec: 0.2314, f1: 0.3758\nSubject-level -> true: 1, pred: 0, acc: 0.0000\n\n==================================================\nTest subject: Subject13\nTrain segments: 4235 | Test segments: 121\nSegment-level -> acc: 0.8182, prec: 1.0000, rec: 0.8182, f1: 0.9000\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n==================================================\nTest subject: Subject14\nTrain segments: 4235 | Test segments: 121\nSegment-level -> acc: 0.2893, prec: 0.0000, rec: 0.0000, f1: 0.0000\nSubject-level -> true: 0, pred: 1, acc: 0.0000\n\n==================================================\nTest subject: Subject15\nTrain segments: 4235 | Test segments: 121\nSegment-level -> acc: 0.8595, prec: 1.0000, rec: 0.8595, f1: 0.9244\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n==================================================\nTest subject: Subject16\nTrain segments: 4235 | Test segments: 121\nSegment-level -> acc: 0.6033, prec: 1.0000, rec: 0.6033, f1: 0.7526\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n==================================================\nTest subject: Subject17\nTrain segments: 4235 | Test segments: 121\nSegment-level -> acc: 1.0000, prec: 1.0000, rec: 1.0000, f1: 1.0000\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n==================================================\nTest subject: Subject18\nTrain segments: 4235 | Test segments: 121\nSegment-level -> acc: 0.2397, prec: 1.0000, rec: 0.2397, f1: 0.3867\nSubject-level -> true: 1, pred: 0, acc: 0.0000\n\n==================================================\nTest subject: Subject19\nTrain segments: 4235 | Test segments: 121\nSegment-level -> acc: 0.0000, prec: 0.0000, rec: 0.0000, f1: 0.0000\nSubject-level -> true: 0, pred: 1, acc: 0.0000\n\n==================================================\nTest subject: Subject20\nTrain segments: 4235 | Test segments: 121\nSegment-level -> acc: 1.0000, prec: 1.0000, rec: 1.0000, f1: 1.0000\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n==================================================\nTest subject: Subject21\nTrain segments: 4235 | Test segments: 121\nSegment-level -> acc: 0.6860, prec: 0.0000, rec: 0.0000, f1: 0.0000\nSubject-level -> true: 0, pred: 0, acc: 1.0000\n\n==================================================\nTest subject: Subject22\nTrain segments: 4235 | Test segments: 121\nSegment-level -> acc: 0.0000, prec: 0.0000, rec: 0.0000, f1: 0.0000\nSubject-level -> true: 0, pred: 1, acc: 0.0000\n\n==================================================\nTest subject: Subject23\nTrain segments: 4235 | Test segments: 121\nSegment-level -> acc: 0.9835, prec: 1.0000, rec: 0.9835, f1: 0.9917\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n==================================================\nTest subject: Subject24\nTrain segments: 4235 | Test segments: 121\nSegment-level -> acc: 0.9669, prec: 1.0000, rec: 0.9669, f1: 0.9832\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n==================================================\nTest subject: Subject25\nTrain segments: 4235 | Test segments: 121\nSegment-level -> acc: 0.8430, prec: 1.0000, rec: 0.8430, f1: 0.9148\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n==================================================\nTest subject: Subject26\nTrain segments: 4235 | Test segments: 121\nSegment-level -> acc: 0.8512, prec: 1.0000, rec: 0.8512, f1: 0.9196\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n==================================================\nTest subject: Subject27\nTrain segments: 4235 | Test segments: 121\nSegment-level -> acc: 0.0909, prec: 1.0000, rec: 0.0909, f1: 0.1667\nSubject-level -> true: 1, pred: 0, acc: 0.0000\n\n==================================================\nTest subject: Subject28\nTrain segments: 4235 | Test segments: 121\nSegment-level -> acc: 1.0000, prec: 1.0000, rec: 1.0000, f1: 1.0000\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n==================================================\nTest subject: Subject29\nTrain segments: 4235 | Test segments: 121\nSegment-level -> acc: 0.9421, prec: 1.0000, rec: 0.9421, f1: 0.9702\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n==================================================\nTest subject: Subject30\nTrain segments: 4235 | Test segments: 121\nSegment-level -> acc: 0.0000, prec: 0.0000, rec: 0.0000, f1: 0.0000\nSubject-level -> true: 0, pred: 1, acc: 0.0000\n\n==================================================\nTest subject: Subject31\nTrain segments: 4235 | Test segments: 121\nSegment-level -> acc: 0.4463, prec: 1.0000, rec: 0.4463, f1: 0.6171\nSubject-level -> true: 1, pred: 0, acc: 0.0000\n\n==================================================\nTest subject: Subject32\nTrain segments: 4235 | Test segments: 121\nSegment-level -> acc: 0.8430, prec: 1.0000, rec: 0.8430, f1: 0.9148\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n==================================================\nTest subject: Subject33\nTrain segments: 4235 | Test segments: 121\nSegment-level -> acc: 1.0000, prec: 1.0000, rec: 1.0000, f1: 1.0000\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n==================================================\nTest subject: Subject34\nTrain segments: 4235 | Test segments: 121\nSegment-level -> acc: 0.9587, prec: 1.0000, rec: 0.9587, f1: 0.9789\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n==================================================\nTest subject: Subject35\nTrain segments: 4235 | Test segments: 121\nSegment-level -> acc: 1.0000, prec: 1.0000, rec: 1.0000, f1: 1.0000\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n==================================================\nAverage SEGMENT-level (LOSO):\nAccuracy : 0.625803489439853\nPrecision: 0.7222222222222222\nRecall   : 0.5964187327823691\nF1-score : 0.6302445787419698\n\nAverage SUBJECT-level (LOSO, majority vote):\nAccuracy : 0.6388888888888888\n\n\n=== Riemannian + per-subject alignment ===\nLoading Subject00_1.edf (rest) and Subject00_2.edf (task)...\nLoading Subject01_1.edf (rest) and Subject01_2.edf (task)...\nLoading Subject02_1.edf (rest) and Subject02_2.edf (task)...\nLoading Subject03_1.edf (rest) and Subject03_2.edf (task)...\nLoading Subject04_1.edf (rest) and Subject04_2.edf (task)...\nLoading Subject05_1.edf (rest) and Subject05_2.edf (task)...\nLoading Subject06_1.edf (rest) and Subject06_2.edf (task)...\nLoading Subject07_1.edf (rest) and Subject07_2.edf (task)...\nLoading Subject08_1.edf (rest) and Subject08_2.edf (task)...\nLoading Subject09_1.edf (rest) and Subject09_2.edf (task)...\nLoading Subject10_1.edf (rest) and Subject10_2.edf (task)...\nLoading Subject11_1.edf (rest) and Subject11_2.edf (task)...\nLoading Subject12_1.edf (rest) and Subject12_2.edf (task)...\nLoading Subject13_1.edf (rest) and Subject13_2.edf (task)...\nLoading Subject14_1.edf (rest) and Subject14_2.edf (task)...\nLoading Subject15_1.edf (rest) and Subject15_2.edf (task)...\nLoading Subject16_1.edf (rest) and Subject16_2.edf (task)...\nLoading Subject17_1.edf (rest) and Subject17_2.edf (task)...\nLoading Subject18_1.edf (rest) and Subject18_2.edf (task)...\nLoading Subject19_1.edf (rest) and Subject19_2.edf (task)...\nLoading Subject20_1.edf (rest) and Subject20_2.edf (task)...\nLoading Subject21_1.edf (rest) and Subject21_2.edf (task)...\nLoading Subject22_1.edf (rest) and Subject22_2.edf (task)...\nLoading Subject23_1.edf (rest) and Subject23_2.edf (task)...\nLoading Subject24_1.edf (rest) and Subject24_2.edf (task)...\nLoading Subject25_1.edf (rest) and Subject25_2.edf (task)...\nLoading Subject26_1.edf (rest) and Subject26_2.edf (task)...\nLoading Subject27_1.edf (rest) and Subject27_2.edf (task)...\nLoading Subject28_1.edf (rest) and Subject28_2.edf (task)...\nLoading Subject29_1.edf (rest) and Subject29_2.edf (task)...\nLoading Subject30_1.edf (rest) and Subject30_2.edf (task)...\nLoading Subject31_1.edf (rest) and Subject31_2.edf (task)...\nLoading Subject32_1.edf (rest) and Subject32_2.edf (task)...\nLoading Subject33_1.edf (rest) and Subject33_2.edf (task)...\nLoading Subject34_1.edf (rest) and Subject34_2.edf (task)...\nLoading Subject35_1.edf (rest) and Subject35_2.edf (task)...\nFinal tensor X shape (N, C, T): (36, 21, 7936)\nLabels y shape: (36,)\nSegmented data shape: (4356, 21, 256)\nApplying per-subject Riemannian alignment...\n\n==================================================\nTest subject: Subject00\nTrain segments: 4235 | Test segments: 121\nSegment-level -> acc: 0.0826, prec: 0.0000, rec: 0.0000, f1: 0.0000\nSubject-level -> true: 0, pred: 1, acc: 0.0000\n\n==================================================\nTest subject: Subject01\nTrain segments: 4235 | Test segments: 121\nSegment-level -> acc: 0.9752, prec: 1.0000, rec: 0.9752, f1: 0.9874\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n==================================================\nTest subject: Subject02\nTrain segments: 4235 | Test segments: 121\nSegment-level -> acc: 0.9091, prec: 1.0000, rec: 0.9091, f1: 0.9524\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n==================================================\nTest subject: Subject03\nTrain segments: 4235 | Test segments: 121\nSegment-level -> acc: 0.9174, prec: 1.0000, rec: 0.9174, f1: 0.9569\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n==================================================\nTest subject: Subject04\nTrain segments: 4235 | Test segments: 121\nSegment-level -> acc: 0.0083, prec: 0.0000, rec: 0.0000, f1: 0.0000\nSubject-level -> true: 0, pred: 1, acc: 0.0000\n\n==================================================\nTest subject: Subject05\nTrain segments: 4235 | Test segments: 121\nSegment-level -> acc: 0.8347, prec: 1.0000, rec: 0.8347, f1: 0.9099\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n==================================================\nTest subject: Subject06\nTrain segments: 4235 | Test segments: 121\nSegment-level -> acc: 0.1570, prec: 0.0000, rec: 0.0000, f1: 0.0000\nSubject-level -> true: 0, pred: 1, acc: 0.0000\n\n==================================================\nTest subject: Subject07\nTrain segments: 4235 | Test segments: 121\nSegment-level -> acc: 0.9256, prec: 1.0000, rec: 0.9256, f1: 0.9614\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n==================================================\nTest subject: Subject08\nTrain segments: 4235 | Test segments: 121\nSegment-level -> acc: 0.7438, prec: 1.0000, rec: 0.7438, f1: 0.8531\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n==================================================\nTest subject: Subject09\nTrain segments: 4235 | Test segments: 121\nSegment-level -> acc: 0.0992, prec: 0.0000, rec: 0.0000, f1: 0.0000\nSubject-level -> true: 0, pred: 1, acc: 0.0000\n\n==================================================\nTest subject: Subject10\nTrain segments: 4235 | Test segments: 121\nSegment-level -> acc: 0.2066, prec: 0.0000, rec: 0.0000, f1: 0.0000\nSubject-level -> true: 0, pred: 1, acc: 0.0000\n\n==================================================\nTest subject: Subject11\nTrain segments: 4235 | Test segments: 121\nSegment-level -> acc: 0.7603, prec: 1.0000, rec: 0.7603, f1: 0.8638\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n==================================================\nTest subject: Subject12\nTrain segments: 4235 | Test segments: 121\nSegment-level -> acc: 0.8264, prec: 1.0000, rec: 0.8264, f1: 0.9050\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n==================================================\nTest subject: Subject13\nTrain segments: 4235 | Test segments: 121\nSegment-level -> acc: 1.0000, prec: 1.0000, rec: 1.0000, f1: 1.0000\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n==================================================\nTest subject: Subject14\nTrain segments: 4235 | Test segments: 121\nSegment-level -> acc: 0.0992, prec: 0.0000, rec: 0.0000, f1: 0.0000\nSubject-level -> true: 0, pred: 1, acc: 0.0000\n\n==================================================\nTest subject: Subject15\nTrain segments: 4235 | Test segments: 121\nSegment-level -> acc: 0.7851, prec: 1.0000, rec: 0.7851, f1: 0.8796\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n==================================================\nTest subject: Subject16\nTrain segments: 4235 | Test segments: 121\nSegment-level -> acc: 0.8430, prec: 1.0000, rec: 0.8430, f1: 0.9148\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n==================================================\nTest subject: Subject17\nTrain segments: 4235 | Test segments: 121\nSegment-level -> acc: 0.7438, prec: 1.0000, rec: 0.7438, f1: 0.8531\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n==================================================\nTest subject: Subject18\nTrain segments: 4235 | Test segments: 121\nSegment-level -> acc: 0.4215, prec: 1.0000, rec: 0.4215, f1: 0.5930\nSubject-level -> true: 1, pred: 0, acc: 0.0000\n\n==================================================\nTest subject: Subject19\nTrain segments: 4235 | Test segments: 121\nSegment-level -> acc: 0.0165, prec: 0.0000, rec: 0.0000, f1: 0.0000\nSubject-level -> true: 0, pred: 1, acc: 0.0000\n\n==================================================\nTest subject: Subject20\nTrain segments: 4235 | Test segments: 121\nSegment-level -> acc: 1.0000, prec: 1.0000, rec: 1.0000, f1: 1.0000\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n==================================================\nTest subject: Subject21\nTrain segments: 4235 | Test segments: 121\nSegment-level -> acc: 0.1818, prec: 0.0000, rec: 0.0000, f1: 0.0000\nSubject-level -> true: 0, pred: 1, acc: 0.0000\n\n==================================================\nTest subject: Subject22\nTrain segments: 4235 | Test segments: 121\nSegment-level -> acc: 0.4050, prec: 0.0000, rec: 0.0000, f1: 0.0000\nSubject-level -> true: 0, pred: 1, acc: 0.0000\n\n==================================================\nTest subject: Subject23\nTrain segments: 4235 | Test segments: 121\nSegment-level -> acc: 0.8843, prec: 1.0000, rec: 0.8843, f1: 0.9386\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n==================================================\nTest subject: Subject24\nTrain segments: 4235 | Test segments: 121\nSegment-level -> acc: 0.9835, prec: 1.0000, rec: 0.9835, f1: 0.9917\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n==================================================\nTest subject: Subject25\nTrain segments: 4235 | Test segments: 121\nSegment-level -> acc: 0.8926, prec: 1.0000, rec: 0.8926, f1: 0.9432\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n==================================================\nTest subject: Subject26\nTrain segments: 4235 | Test segments: 121\nSegment-level -> acc: 0.6364, prec: 1.0000, rec: 0.6364, f1: 0.7778\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n==================================================\nTest subject: Subject27\nTrain segments: 4235 | Test segments: 121\nSegment-level -> acc: 0.6281, prec: 1.0000, rec: 0.6281, f1: 0.7716\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n==================================================\nTest subject: Subject28\nTrain segments: 4235 | Test segments: 121\nSegment-level -> acc: 0.9091, prec: 1.0000, rec: 0.9091, f1: 0.9524\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n==================================================\nTest subject: Subject29\nTrain segments: 4235 | Test segments: 121\nSegment-level -> acc: 0.8182, prec: 1.0000, rec: 0.8182, f1: 0.9000\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n==================================================\nTest subject: Subject30\nTrain segments: 4235 | Test segments: 121\nSegment-level -> acc: 0.1570, prec: 0.0000, rec: 0.0000, f1: 0.0000\nSubject-level -> true: 0, pred: 1, acc: 0.0000\n\n==================================================\nTest subject: Subject31\nTrain segments: 4235 | Test segments: 121\nSegment-level -> acc: 0.9339, prec: 1.0000, rec: 0.9339, f1: 0.9658\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n==================================================\nTest subject: Subject32\nTrain segments: 4235 | Test segments: 121\nSegment-level -> acc: 0.6446, prec: 1.0000, rec: 0.6446, f1: 0.7839\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n==================================================\nTest subject: Subject33\nTrain segments: 4235 | Test segments: 121\nSegment-level -> acc: 0.7686, prec: 1.0000, rec: 0.7686, f1: 0.8692\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n==================================================\nTest subject: Subject34\nTrain segments: 4235 | Test segments: 121\nSegment-level -> acc: 0.8512, prec: 1.0000, rec: 0.8512, f1: 0.9196\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n==================================================\nTest subject: Subject35\nTrain segments: 4235 | Test segments: 121\nSegment-level -> acc: 0.9917, prec: 1.0000, rec: 0.9917, f1: 0.9959\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n==================================================\nAverage SEGMENT-level (LOSO):\nAccuracy : 0.6400367309458218\nPrecision: 0.7222222222222222\nRecall   : 0.6007805325987143\nF1-score : 0.6511127424534001\n\nAverage SUBJECT-level (LOSO, majority vote):\nAccuracy : 0.6944444444444444\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"pip install torch torchvision\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T18:57:02.234158Z","iopub.execute_input":"2025-12-11T18:57:02.234441Z","iopub.status.idle":"2025-12-11T18:57:05.482724Z","shell.execute_reply.started":"2025-12-11T18:57:02.234420Z","shell.execute_reply":"2025-12-11T18:57:05.481586Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.20.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.15.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.10.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.3.5)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.3)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# dann_eegmat.py\nimport numpy as np\n# from eegmat_data import load_task_edf_with_baseline, make_segments\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\n\ntorch.backends.cudnn.enabled = False\ntorch.backends.cudnn.benchmark = False\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", DEVICE)\n\nRANDOM_STATE = 42\ntorch.manual_seed(RANDOM_STATE)\nnp.random.seed(RANDOM_STATE)\n\n\n# ---------- Dataset ----------\nclass EEGSegmentDataset(Dataset):\n    def __init__(self, X, y, subj_ids):\n        # X: (N, C, T)\n        self.X = torch.from_numpy(X).float()\n        self.y = torch.from_numpy(y).long()\n        # subj_ids: mapping string -> int باید قبلش انجام بشه\n        self.subj_ids = torch.from_numpy(subj_ids).long()\n\n    def __len__(self):\n        return self.X.shape[0]\n\n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx], self.subj_ids[idx]\n\n\n# ---------- Gradient Reversal Layer ----------\nclass GradReverse(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x, lambd):\n        ctx.lambd = lambd\n        return x.view_as(x)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        return -ctx.lambd * grad_output, None\n\n\ndef grad_reverse(x, lambd=1.0):\n    return GradReverse.apply(x, lambd)\n\n\n# ---------- Model ----------\nclass DANN_EEG(nn.Module):\n    def __init__(self, n_channels, input_len, n_domains, feat_dim=128):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv1d(n_channels, 32, kernel_size=7, stride=1, padding=3),\n            nn.BatchNorm1d(32),\n            nn.ReLU(),\n            nn.MaxPool1d(2),\n\n            nn.Conv1d(32, 64, kernel_size=5, stride=1, padding=2),\n            nn.BatchNorm1d(64),\n            nn.ReLU(),\n            nn.MaxPool1d(2),\n\n            nn.Conv1d(64, 128, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm1d(128),\n            nn.ReLU(),\n            nn.AdaptiveAvgPool1d(1),  # -> (B, 128, 1)\n        )\n        self.feat = nn.Linear(128, feat_dim)\n\n        self.label_head = nn.Sequential(\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(feat_dim, 1)\n        )\n\n        self.domain_head = nn.Sequential(\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(feat_dim, n_domains)\n        )\n\n    def forward(self, x, lambd=0.0):\n        # x: (B, C, T)\n        h = self.conv(x)          # (B, 128, 1)\n        h = h.squeeze(-1)         # (B, 128)\n        h = self.feat(h)          # (B, feat_dim)\n\n        # label prediction\n        logits_label = self.label_head(h).squeeze(-1)  # (B,)\n\n        # domain prediction with GRL\n        h_rev = grad_reverse(h, lambd)\n        logits_domain = self.domain_head(h_rev)        # (B, n_domains)\n\n        return logits_label, logits_domain\n\n\n# ---------- Helper: subject mapping ----------\ndef encode_subjects(subj_array, train_subjects):\n    \"\"\"\n    subj_array: (N_segments,) with string IDs\n    train_subjects: unique strings used in this fold\n    returns: domain_ids (int) for each segment, domain_label_map dict\n    \"\"\"\n    subj_to_idx = {s: i for i, s in enumerate(train_subjects)}\n    domain_ids = np.array([subj_to_idx[s] for s in subj_array])\n    return domain_ids, subj_to_idx\n\n\n# ---------- Training Loop ----------\ndef train_dann_for_fold(X_train, y_train, subj_train, X_val, y_val, subj_val,\n                        num_epochs=15, batch_size=64):\n    # Map subject strings to domain ids (only train subjects)\n    unique_train_subj = np.unique(subj_train)\n    train_domain_ids, subj_to_idx = encode_subjects(subj_train, unique_train_subj)\n\n    # For validation segments, if subj not in train (shouldn't happen here for LOSO),\n    # we skip domain loss; ساده‌ترین حالت: domain id = 0 (ignored).\n    val_domain_ids = np.array([subj_to_idx.get(s, 0) for s in subj_val])\n\n    train_ds = EEGSegmentDataset(X_train, y_train, train_domain_ids)\n    val_ds = EEGSegmentDataset(X_val, y_val, val_domain_ids)\n\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n\n    n_channels = X_train.shape[1]\n    input_len = X_train.shape[2]\n    n_domains = len(unique_train_subj)\n\n    model = DANN_EEG(n_channels, input_len, n_domains).to(DEVICE)\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    bce = nn.BCEWithLogitsLoss()\n    ce = nn.CrossEntropyLoss()\n\n    total_steps = num_epochs * len(train_loader)\n\n    def calc_lambda(p):\n        # p in [0,1], schedule از مقاله DANN\n        return 2.0 / (1.0 + np.exp(-10 * p)) - 1.0\n\n    step = 0\n    for epoch in range(num_epochs):\n        model.train()\n        for xb, yb, db in train_loader:\n            xb = xb.to(DEVICE)\n            yb = yb.float().to(DEVICE)\n            db = db.to(DEVICE)\n\n            p = step / total_steps\n            lambd = calc_lambda(p)\n\n            logits_label, logits_domain = model(xb, lambd=lambd)\n\n            loss_cls = bce(logits_label, yb)\n            loss_dom = ce(logits_domain, db)\n\n            loss = loss_cls + 0.1 * loss_dom  # وزن domain\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            step += 1\n\n        # ساده: فقط یک پاس validation برای چاپ\n        model.eval()\n        with torch.no_grad():\n            all_logits = []\n            all_y = []\n            for xb, yb, db in val_loader:\n                xb = xb.to(DEVICE)\n                yb = yb.float().to(DEVICE)\n                lg, _ = model(xb, lambd=0.0)\n                all_logits.append(lg.cpu())\n                all_y.append(yb.cpu())\n            if len(all_logits) > 0:\n                logits = torch.cat(all_logits)\n                ys = torch.cat(all_y)\n                preds = (torch.sigmoid(logits) >= 0.5).long()\n                acc = (preds == ys.long()).float().mean().item()\n                print(f\"Epoch {epoch+1}/{num_epochs}, val acc={acc:.4f}\")\n\n    return model\n\n\ndef evaluate_dann_loso(num_epochs=15):\n    X, y, subjects, sfreq = load_task_edf_with_baseline()\n    seg_X, seg_y, seg_subjects = make_segments(X, y, subjects, sfreq)\n\n    unique_subjects = np.unique(seg_subjects)\n    subj_metrics = []\n\n    for test_subj in unique_subjects:\n        print(\"\\n\" + \"=\" * 60)\n        print(\"Test subject:\", test_subj)\n\n        test_mask = (seg_subjects == test_subj)\n        train_mask = ~test_mask\n\n        X_train = seg_X[train_mask]\n        y_train = seg_y[train_mask]\n        subj_train = seg_subjects[train_mask]\n\n        X_test = seg_X[test_mask]\n        y_test = seg_y[test_mask]\n        subj_test = seg_subjects[test_mask]\n\n        # split train -> train/val (مثلاً 90/10)\n        n_train = X_train.shape[0]\n        idx = np.arange(n_train)\n        np.random.shuffle(idx)\n        split = int(0.9 * n_train)\n        tr_idx, val_idx = idx[:split], idx[split:]\n\n        X_tr, y_tr, subj_tr = X_train[tr_idx], y_train[tr_idx], subj_train[tr_idx]\n        X_val, y_val, subj_val = X_train[val_idx], y_train[val_idx], subj_train[val_idx]\n\n        model = train_dann_for_fold(X_tr, y_tr, subj_tr, X_val, y_val, subj_val,\n                                    num_epochs=num_epochs, batch_size=64)\n\n        # test\n        model.eval()\n        ds_test = EEGSegmentDataset(X_test, y_test,\n                                    np.zeros_like(y_test))  # domain ids irrelevant\n        dl_test = DataLoader(ds_test, batch_size=64, shuffle=False)\n\n        all_preds = []\n        all_true = []\n        with torch.no_grad():\n            for xb, yb, db in dl_test:\n                xb = xb.to(DEVICE)\n                yb = yb.to(DEVICE)\n                logits, _ = model(xb, lambd=0.0)\n                preds = (torch.sigmoid(logits) >= 0.5).long()\n                all_preds.append(preds.cpu().numpy())\n                all_true.append(yb.cpu().numpy())\n\n        y_pred_seg = np.concatenate(all_preds)\n        y_true_seg = np.concatenate(all_true)\n\n        # majority vote subject-level\n        true_label = y_true_seg[0]\n        counts = np.bincount(y_pred_seg)\n        pred_label = np.argmax(counts)\n        acc_subj = 1.0 if pred_label == true_label else 0.0\n        print(f\"Subject-level -> true: {true_label}, pred: {pred_label}, acc: {acc_subj:.4f}\")\n        subj_metrics.append(acc_subj)\n\n    subj_metrics = np.array(subj_metrics)\n    print(\"\\n\" + \"=\" * 60)\n    print(\"Average SUBJECT-level (LOSO, DANN):\", subj_metrics.mean())\n\n\nif __name__ == \"__main__\":\n    evaluate_dann_loso(num_epochs=10)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T18:58:21.898651Z","iopub.execute_input":"2025-12-11T18:58:21.899335Z","iopub.status.idle":"2025-12-11T19:02:22.531880Z","shell.execute_reply.started":"2025-12-11T18:58:21.899313Z","shell.execute_reply":"2025-12-11T19:02:22.531073Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nLoading Subject00_1.edf (rest) and Subject00_2.edf (task)...\nLoading Subject01_1.edf (rest) and Subject01_2.edf (task)...\nLoading Subject02_1.edf (rest) and Subject02_2.edf (task)...\nLoading Subject03_1.edf (rest) and Subject03_2.edf (task)...\nLoading Subject04_1.edf (rest) and Subject04_2.edf (task)...\nLoading Subject05_1.edf (rest) and Subject05_2.edf (task)...\nLoading Subject06_1.edf (rest) and Subject06_2.edf (task)...\nLoading Subject07_1.edf (rest) and Subject07_2.edf (task)...\nLoading Subject08_1.edf (rest) and Subject08_2.edf (task)...\nLoading Subject09_1.edf (rest) and Subject09_2.edf (task)...\nLoading Subject10_1.edf (rest) and Subject10_2.edf (task)...\nLoading Subject11_1.edf (rest) and Subject11_2.edf (task)...\nLoading Subject12_1.edf (rest) and Subject12_2.edf (task)...\nLoading Subject13_1.edf (rest) and Subject13_2.edf (task)...\nLoading Subject14_1.edf (rest) and Subject14_2.edf (task)...\nLoading Subject15_1.edf (rest) and Subject15_2.edf (task)...\nLoading Subject16_1.edf (rest) and Subject16_2.edf (task)...\nLoading Subject17_1.edf (rest) and Subject17_2.edf (task)...\nLoading Subject18_1.edf (rest) and Subject18_2.edf (task)...\nLoading Subject19_1.edf (rest) and Subject19_2.edf (task)...\nLoading Subject20_1.edf (rest) and Subject20_2.edf (task)...\nLoading Subject21_1.edf (rest) and Subject21_2.edf (task)...\nLoading Subject22_1.edf (rest) and Subject22_2.edf (task)...\nLoading Subject23_1.edf (rest) and Subject23_2.edf (task)...\nLoading Subject24_1.edf (rest) and Subject24_2.edf (task)...\nLoading Subject25_1.edf (rest) and Subject25_2.edf (task)...\nLoading Subject26_1.edf (rest) and Subject26_2.edf (task)...\nLoading Subject27_1.edf (rest) and Subject27_2.edf (task)...\nLoading Subject28_1.edf (rest) and Subject28_2.edf (task)...\nLoading Subject29_1.edf (rest) and Subject29_2.edf (task)...\nLoading Subject30_1.edf (rest) and Subject30_2.edf (task)...\nLoading Subject31_1.edf (rest) and Subject31_2.edf (task)...\nLoading Subject32_1.edf (rest) and Subject32_2.edf (task)...\nLoading Subject33_1.edf (rest) and Subject33_2.edf (task)...\nLoading Subject34_1.edf (rest) and Subject34_2.edf (task)...\nLoading Subject35_1.edf (rest) and Subject35_2.edf (task)...\nFinal tensor X shape (N, C, T): (36, 21, 7936)\nLabels y shape: (36,)\nSegmented data shape: (4356, 21, 256)\n\n============================================================\nTest subject: Subject00\nEpoch 1/10, val acc=0.7335\nEpoch 2/10, val acc=0.7335\nEpoch 3/10, val acc=0.2665\nEpoch 4/10, val acc=0.7335\nEpoch 5/10, val acc=0.7335\nEpoch 6/10, val acc=0.2665\nEpoch 7/10, val acc=0.7335\nEpoch 8/10, val acc=0.7335\nEpoch 9/10, val acc=0.7335\nEpoch 10/10, val acc=0.2665\nSubject-level -> true: 0, pred: 0, acc: 1.0000\n\n============================================================\nTest subject: Subject01\nEpoch 1/10, val acc=0.2759\nEpoch 2/10, val acc=0.2759\nEpoch 3/10, val acc=0.2759\nEpoch 4/10, val acc=0.2759\nEpoch 5/10, val acc=0.2759\nEpoch 6/10, val acc=0.2759\nEpoch 7/10, val acc=0.2759\nEpoch 8/10, val acc=0.2759\nEpoch 9/10, val acc=0.7241\nEpoch 10/10, val acc=0.7241\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n============================================================\nTest subject: Subject02\nEpoch 1/10, val acc=0.7335\nEpoch 2/10, val acc=0.2665\nEpoch 3/10, val acc=0.7335\nEpoch 4/10, val acc=0.7335\nEpoch 5/10, val acc=0.7335\nEpoch 6/10, val acc=0.2665\nEpoch 7/10, val acc=0.7358\nEpoch 8/10, val acc=0.7335\nEpoch 9/10, val acc=0.2665\nEpoch 10/10, val acc=0.7335\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n============================================================\nTest subject: Subject03\nEpoch 1/10, val acc=0.6981\nEpoch 2/10, val acc=0.3019\nEpoch 3/10, val acc=0.6981\nEpoch 4/10, val acc=0.6981\nEpoch 5/10, val acc=0.3019\nEpoch 6/10, val acc=0.6981\nEpoch 7/10, val acc=0.6981\nEpoch 8/10, val acc=0.6981\nEpoch 9/10, val acc=0.6981\nEpoch 10/10, val acc=0.6981\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n============================================================\nTest subject: Subject04\nEpoch 1/10, val acc=0.7358\nEpoch 2/10, val acc=0.7358\nEpoch 3/10, val acc=0.2642\nEpoch 4/10, val acc=0.7358\nEpoch 5/10, val acc=0.2642\nEpoch 6/10, val acc=0.7358\nEpoch 7/10, val acc=0.7358\nEpoch 8/10, val acc=0.7358\nEpoch 9/10, val acc=0.7358\nEpoch 10/10, val acc=0.7358\nSubject-level -> true: 0, pred: 1, acc: 0.0000\n\n============================================================\nTest subject: Subject05\nEpoch 1/10, val acc=0.7005\nEpoch 2/10, val acc=0.7005\nEpoch 3/10, val acc=0.7005\nEpoch 4/10, val acc=0.7005\nEpoch 5/10, val acc=0.2995\nEpoch 6/10, val acc=0.2995\nEpoch 7/10, val acc=0.7005\nEpoch 8/10, val acc=0.2995\nEpoch 9/10, val acc=0.7005\nEpoch 10/10, val acc=0.7005\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n============================================================\nTest subject: Subject06\nEpoch 1/10, val acc=0.7476\nEpoch 2/10, val acc=0.7476\nEpoch 3/10, val acc=0.2524\nEpoch 4/10, val acc=0.2524\nEpoch 5/10, val acc=0.2524\nEpoch 6/10, val acc=0.2524\nEpoch 7/10, val acc=0.2524\nEpoch 8/10, val acc=0.7476\nEpoch 9/10, val acc=0.2524\nEpoch 10/10, val acc=0.7476\nSubject-level -> true: 0, pred: 1, acc: 0.0000\n\n============================================================\nTest subject: Subject07\nEpoch 1/10, val acc=0.6840\nEpoch 2/10, val acc=0.3231\nEpoch 3/10, val acc=0.3231\nEpoch 4/10, val acc=0.6769\nEpoch 5/10, val acc=0.3231\nEpoch 6/10, val acc=0.6769\nEpoch 7/10, val acc=0.6769\nEpoch 8/10, val acc=0.6769\nEpoch 9/10, val acc=0.3231\nEpoch 10/10, val acc=0.6769\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n============================================================\nTest subject: Subject08\nEpoch 1/10, val acc=0.6863\nEpoch 2/10, val acc=0.6863\nEpoch 3/10, val acc=0.6863\nEpoch 4/10, val acc=0.3137\nEpoch 5/10, val acc=0.6863\nEpoch 6/10, val acc=0.6863\nEpoch 7/10, val acc=0.3137\nEpoch 8/10, val acc=0.6863\nEpoch 9/10, val acc=0.3137\nEpoch 10/10, val acc=0.6863\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n============================================================\nTest subject: Subject09\nEpoch 1/10, val acc=0.7335\nEpoch 2/10, val acc=0.2665\nEpoch 3/10, val acc=0.2665\nEpoch 4/10, val acc=0.7335\nEpoch 5/10, val acc=0.7335\nEpoch 6/10, val acc=0.7335\nEpoch 7/10, val acc=0.7335\nEpoch 8/10, val acc=0.7335\nEpoch 9/10, val acc=0.7335\nEpoch 10/10, val acc=0.7335\nSubject-level -> true: 0, pred: 1, acc: 0.0000\n\n============================================================\nTest subject: Subject10\nEpoch 1/10, val acc=0.2476\nEpoch 2/10, val acc=0.7524\nEpoch 3/10, val acc=0.7524\nEpoch 4/10, val acc=0.2476\nEpoch 5/10, val acc=0.7524\nEpoch 6/10, val acc=0.7524\nEpoch 7/10, val acc=0.2476\nEpoch 8/10, val acc=0.7524\nEpoch 9/10, val acc=0.2476\nEpoch 10/10, val acc=0.7524\nSubject-level -> true: 0, pred: 1, acc: 0.0000\n\n============================================================\nTest subject: Subject11\nEpoch 1/10, val acc=0.7311\nEpoch 2/10, val acc=0.7311\nEpoch 3/10, val acc=0.2689\nEpoch 4/10, val acc=0.7311\nEpoch 5/10, val acc=0.2689\nEpoch 6/10, val acc=0.7311\nEpoch 7/10, val acc=0.7311\nEpoch 8/10, val acc=0.7311\nEpoch 9/10, val acc=0.7311\nEpoch 10/10, val acc=0.7311\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n============================================================\nTest subject: Subject12\nEpoch 1/10, val acc=0.3184\nEpoch 2/10, val acc=0.6816\nEpoch 3/10, val acc=0.6816\nEpoch 4/10, val acc=0.6816\nEpoch 5/10, val acc=0.6816\nEpoch 6/10, val acc=0.6816\nEpoch 7/10, val acc=0.6816\nEpoch 8/10, val acc=0.6816\nEpoch 9/10, val acc=0.3184\nEpoch 10/10, val acc=0.6816\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n============================================================\nTest subject: Subject13\nEpoch 1/10, val acc=0.3160\nEpoch 2/10, val acc=0.6840\nEpoch 3/10, val acc=0.3514\nEpoch 4/10, val acc=0.6840\nEpoch 5/10, val acc=0.3160\nEpoch 6/10, val acc=0.3160\nEpoch 7/10, val acc=0.6840\nEpoch 8/10, val acc=0.6840\nEpoch 9/10, val acc=0.6840\nEpoch 10/10, val acc=0.6840\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n============================================================\nTest subject: Subject14\nEpoch 1/10, val acc=0.7429\nEpoch 2/10, val acc=0.2571\nEpoch 3/10, val acc=0.7429\nEpoch 4/10, val acc=0.7429\nEpoch 5/10, val acc=0.7429\nEpoch 6/10, val acc=0.7429\nEpoch 7/10, val acc=0.7429\nEpoch 8/10, val acc=0.7429\nEpoch 9/10, val acc=0.7429\nEpoch 10/10, val acc=0.2571\nSubject-level -> true: 0, pred: 0, acc: 1.0000\n\n============================================================\nTest subject: Subject15\nEpoch 1/10, val acc=0.7241\nEpoch 2/10, val acc=0.7241\nEpoch 3/10, val acc=0.7241\nEpoch 4/10, val acc=0.7241\nEpoch 5/10, val acc=0.7241\nEpoch 6/10, val acc=0.2759\nEpoch 7/10, val acc=0.2759\nEpoch 8/10, val acc=0.7241\nEpoch 9/10, val acc=0.7241\nEpoch 10/10, val acc=0.7241\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n============================================================\nTest subject: Subject16\nEpoch 1/10, val acc=0.7028\nEpoch 2/10, val acc=0.7028\nEpoch 3/10, val acc=0.7028\nEpoch 4/10, val acc=0.7028\nEpoch 5/10, val acc=0.7028\nEpoch 6/10, val acc=0.7028\nEpoch 7/10, val acc=0.7028\nEpoch 8/10, val acc=0.7028\nEpoch 9/10, val acc=0.7028\nEpoch 10/10, val acc=0.2972\nSubject-level -> true: 1, pred: 0, acc: 0.0000\n\n============================================================\nTest subject: Subject17\nEpoch 1/10, val acc=0.7170\nEpoch 2/10, val acc=0.7170\nEpoch 3/10, val acc=0.7170\nEpoch 4/10, val acc=0.2830\nEpoch 5/10, val acc=0.7170\nEpoch 6/10, val acc=0.2830\nEpoch 7/10, val acc=0.7170\nEpoch 8/10, val acc=0.2830\nEpoch 9/10, val acc=0.7170\nEpoch 10/10, val acc=0.7170\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n============================================================\nTest subject: Subject18\nEpoch 1/10, val acc=0.6769\nEpoch 2/10, val acc=0.3231\nEpoch 3/10, val acc=0.3231\nEpoch 4/10, val acc=0.3231\nEpoch 5/10, val acc=0.6769\nEpoch 6/10, val acc=0.6769\nEpoch 7/10, val acc=0.6769\nEpoch 8/10, val acc=0.3231\nEpoch 9/10, val acc=0.6769\nEpoch 10/10, val acc=0.3231\nSubject-level -> true: 1, pred: 0, acc: 0.0000\n\n============================================================\nTest subject: Subject19\nEpoch 1/10, val acc=0.7382\nEpoch 2/10, val acc=0.7382\nEpoch 3/10, val acc=0.7382\nEpoch 4/10, val acc=0.7382\nEpoch 5/10, val acc=0.7382\nEpoch 6/10, val acc=0.7382\nEpoch 7/10, val acc=0.2618\nEpoch 8/10, val acc=0.2618\nEpoch 9/10, val acc=0.7382\nEpoch 10/10, val acc=0.7382\nSubject-level -> true: 0, pred: 1, acc: 0.0000\n\n============================================================\nTest subject: Subject20\nEpoch 1/10, val acc=0.7075\nEpoch 2/10, val acc=0.7075\nEpoch 3/10, val acc=0.7075\nEpoch 4/10, val acc=0.2925\nEpoch 5/10, val acc=0.2925\nEpoch 6/10, val acc=0.7075\nEpoch 7/10, val acc=0.7075\nEpoch 8/10, val acc=0.7075\nEpoch 9/10, val acc=0.2925\nEpoch 10/10, val acc=0.7075\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n============================================================\nTest subject: Subject21\nEpoch 1/10, val acc=0.7335\nEpoch 2/10, val acc=0.7335\nEpoch 3/10, val acc=0.7335\nEpoch 4/10, val acc=0.7335\nEpoch 5/10, val acc=0.7335\nEpoch 6/10, val acc=0.7335\nEpoch 7/10, val acc=0.7335\nEpoch 8/10, val acc=0.2665\nEpoch 9/10, val acc=0.7335\nEpoch 10/10, val acc=0.7335\nSubject-level -> true: 0, pred: 1, acc: 0.0000\n\n============================================================\nTest subject: Subject22\nEpoch 1/10, val acc=0.7665\nEpoch 2/10, val acc=0.7665\nEpoch 3/10, val acc=0.2335\nEpoch 4/10, val acc=0.7665\nEpoch 5/10, val acc=0.7665\nEpoch 6/10, val acc=0.7665\nEpoch 7/10, val acc=0.2335\nEpoch 8/10, val acc=0.7665\nEpoch 9/10, val acc=0.7665\nEpoch 10/10, val acc=0.2335\nSubject-level -> true: 0, pred: 0, acc: 1.0000\n\n============================================================\nTest subject: Subject23\nEpoch 1/10, val acc=0.3019\nEpoch 2/10, val acc=0.6981\nEpoch 3/10, val acc=0.3019\nEpoch 4/10, val acc=0.3019\nEpoch 5/10, val acc=0.3019\nEpoch 6/10, val acc=0.6981\nEpoch 7/10, val acc=0.3019\nEpoch 8/10, val acc=0.3019\nEpoch 9/10, val acc=0.6981\nEpoch 10/10, val acc=0.6981\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n============================================================\nTest subject: Subject24\nEpoch 1/10, val acc=0.6698\nEpoch 2/10, val acc=0.6698\nEpoch 3/10, val acc=0.3302\nEpoch 4/10, val acc=0.6698\nEpoch 5/10, val acc=0.6698\nEpoch 6/10, val acc=0.6698\nEpoch 7/10, val acc=0.6698\nEpoch 8/10, val acc=0.6698\nEpoch 9/10, val acc=0.6698\nEpoch 10/10, val acc=0.6698\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n============================================================\nTest subject: Subject25\nEpoch 1/10, val acc=0.7217\nEpoch 2/10, val acc=0.7217\nEpoch 3/10, val acc=0.7217\nEpoch 4/10, val acc=0.2783\nEpoch 5/10, val acc=0.7217\nEpoch 6/10, val acc=0.2783\nEpoch 7/10, val acc=0.7217\nEpoch 8/10, val acc=0.7217\nEpoch 9/10, val acc=0.7217\nEpoch 10/10, val acc=0.7217\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n============================================================\nTest subject: Subject26\nEpoch 1/10, val acc=0.7075\nEpoch 2/10, val acc=0.7075\nEpoch 3/10, val acc=0.7075\nEpoch 4/10, val acc=0.7075\nEpoch 5/10, val acc=0.2925\nEpoch 6/10, val acc=0.7075\nEpoch 7/10, val acc=0.7075\nEpoch 8/10, val acc=0.7075\nEpoch 9/10, val acc=0.7075\nEpoch 10/10, val acc=0.7075\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n============================================================\nTest subject: Subject27\nEpoch 1/10, val acc=0.6698\nEpoch 2/10, val acc=0.6698\nEpoch 3/10, val acc=0.3302\nEpoch 4/10, val acc=0.3302\nEpoch 5/10, val acc=0.6698\nEpoch 6/10, val acc=0.3302\nEpoch 7/10, val acc=0.3302\nEpoch 8/10, val acc=0.3302\nEpoch 9/10, val acc=0.3302\nEpoch 10/10, val acc=0.6698\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n============================================================\nTest subject: Subject28\nEpoch 1/10, val acc=0.2948\nEpoch 2/10, val acc=0.7052\nEpoch 3/10, val acc=0.2948\nEpoch 4/10, val acc=0.3066\nEpoch 5/10, val acc=0.7052\nEpoch 6/10, val acc=0.2948\nEpoch 7/10, val acc=0.7052\nEpoch 8/10, val acc=0.7052\nEpoch 9/10, val acc=0.2948\nEpoch 10/10, val acc=0.2948\nSubject-level -> true: 1, pred: 0, acc: 0.0000\n\n============================================================\nTest subject: Subject29\nEpoch 1/10, val acc=0.6887\nEpoch 2/10, val acc=0.6887\nEpoch 3/10, val acc=0.3113\nEpoch 4/10, val acc=0.6887\nEpoch 5/10, val acc=0.6887\nEpoch 6/10, val acc=0.6887\nEpoch 7/10, val acc=0.3113\nEpoch 8/10, val acc=0.3113\nEpoch 9/10, val acc=0.6887\nEpoch 10/10, val acc=0.3113\nSubject-level -> true: 1, pred: 0, acc: 0.0000\n\n============================================================\nTest subject: Subject30\nEpoch 1/10, val acc=0.2830\nEpoch 2/10, val acc=0.2830\nEpoch 3/10, val acc=0.7170\nEpoch 4/10, val acc=0.2830\nEpoch 5/10, val acc=0.7170\nEpoch 6/10, val acc=0.7170\nEpoch 7/10, val acc=0.2830\nEpoch 8/10, val acc=0.2830\nEpoch 9/10, val acc=0.2830\nEpoch 10/10, val acc=0.2830\nSubject-level -> true: 0, pred: 0, acc: 1.0000\n\n============================================================\nTest subject: Subject31\nEpoch 1/10, val acc=0.2618\nEpoch 2/10, val acc=0.2618\nEpoch 3/10, val acc=0.2618\nEpoch 4/10, val acc=0.7382\nEpoch 5/10, val acc=0.2618\nEpoch 6/10, val acc=0.7382\nEpoch 7/10, val acc=0.7382\nEpoch 8/10, val acc=0.7382\nEpoch 9/10, val acc=0.2618\nEpoch 10/10, val acc=0.7382\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n============================================================\nTest subject: Subject32\nEpoch 1/10, val acc=0.7288\nEpoch 2/10, val acc=0.2712\nEpoch 3/10, val acc=0.7288\nEpoch 4/10, val acc=0.2712\nEpoch 5/10, val acc=0.7288\nEpoch 6/10, val acc=0.7288\nEpoch 7/10, val acc=0.7288\nEpoch 8/10, val acc=0.7288\nEpoch 9/10, val acc=0.2712\nEpoch 10/10, val acc=0.7288\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n============================================================\nTest subject: Subject33\nEpoch 1/10, val acc=0.7052\nEpoch 2/10, val acc=0.7052\nEpoch 3/10, val acc=0.2948\nEpoch 4/10, val acc=0.7052\nEpoch 5/10, val acc=0.2948\nEpoch 6/10, val acc=0.2948\nEpoch 7/10, val acc=0.7052\nEpoch 8/10, val acc=0.2948\nEpoch 9/10, val acc=0.2948\nEpoch 10/10, val acc=0.4623\nSubject-level -> true: 1, pred: 0, acc: 0.0000\n\n============================================================\nTest subject: Subject34\nEpoch 1/10, val acc=0.7311\nEpoch 2/10, val acc=0.7311\nEpoch 3/10, val acc=0.7311\nEpoch 4/10, val acc=0.7311\nEpoch 5/10, val acc=0.2689\nEpoch 6/10, val acc=0.7311\nEpoch 7/10, val acc=0.7311\nEpoch 8/10, val acc=0.7311\nEpoch 9/10, val acc=0.2689\nEpoch 10/10, val acc=0.2689\nSubject-level -> true: 1, pred: 0, acc: 0.0000\n\n============================================================\nTest subject: Subject35\nEpoch 1/10, val acc=0.7075\nEpoch 2/10, val acc=0.2925\nEpoch 3/10, val acc=0.2925\nEpoch 4/10, val acc=0.7075\nEpoch 5/10, val acc=0.7075\nEpoch 6/10, val acc=0.2925\nEpoch 7/10, val acc=0.7075\nEpoch 8/10, val acc=0.2925\nEpoch 9/10, val acc=0.2925\nEpoch 10/10, val acc=0.7075\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n============================================================\nAverage SUBJECT-level (LOSO, DANN): 0.6666666666666666\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# ================================\n#   SimCLR-style SSL + LOSO + SVM\n#       روی EEGMAT - GPU if avail\n# ================================\n\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport mne\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# --------------------------------\n# تنظیمات دیتاست\n# --------------------------------\nDATA_FOLDER = \"/kaggle/input/ahmadi-dataset\"   # اگر مسیرت فرق دارد، این را عوض کن\nINFO_CSV = f\"{DATA_FOLDER}/subject-info.csv\"\n\nRESAMPLE_TO = 128\nWINDOW_SEC = 2.0\nOVERLAP_RATIO = 0.75  # 75% overlap\n\nRANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE)\n\n# --------------------------------\n# ۱) لود دیتاست + baseline از rest\n# --------------------------------\ndef load_task_edf_with_baseline(folder_path=DATA_FOLDER,\n                                info_csv_path=INFO_CSV,\n                                resample_to=RESAMPLE_TO,\n                                use_baseline=True):\n    folder = Path(folder_path)\n    info_df = pd.read_csv(info_csv_path)\n    label_map = dict(zip(info_df[\"Subject\"], info_df[\"Count quality\"]))\n\n    X_list, y_list, subjects = [], [], []\n    sfreq = None\n\n    for subj_name in info_df[\"Subject\"]:\n        task_file = folder / f\"{subj_name}_2.edf\"\n        rest_file = folder / f\"{subj_name}_1.edf\"\n\n        if not task_file.is_file():\n            print(f\"Task file not found for {subj_name}, skipping.\")\n            continue\n\n        if subj_name not in label_map:\n            print(f\"{subj_name} not in subject-info, skipping.\")\n            continue\n\n        print(f\"Loading {subj_name}_1.edf (rest) and {subj_name}_2.edf (task)...\")\n        raw_task = mne.io.read_raw_edf(task_file, preload=True, verbose=False)\n\n        raw_rest = None\n        if use_baseline and rest_file.is_file():\n            raw_rest = mne.io.read_raw_edf(rest_file, preload=True, verbose=False)\n\n        # resample\n        if resample_to is not None:\n            raw_task.resample(resample_to)\n            if raw_rest is not None:\n                raw_rest.resample(resample_to)\n\n        if sfreq is None:\n            sfreq = raw_task.info[\"sfreq\"]\n\n        task_data = raw_task.get_data()  # (C, T)\n\n        if use_baseline and raw_rest is not None:\n            rest_data = raw_rest.get_data()\n            baseline = rest_data.mean(axis=1, keepdims=True)\n            task_data = task_data - baseline\n\n        X_list.append(task_data)\n        y_list.append(int(label_map[subj_name]))\n        subjects.append(subj_name)\n\n    if not X_list:\n        raise ValueError(\"No task files loaded.\")\n\n    lengths = [d.shape[1] for d in X_list]\n    min_len = min(lengths)\n    X = np.stack([d[:, :min_len] for d in X_list], axis=0)  # (N, C, T)\n    y = np.array(y_list, dtype=int)\n    subjects = np.array(subjects)\n\n    print(\"Final tensor X shape (N, C, T):\", X.shape)\n    print(\"Labels y shape:\", y.shape)\n\n    return X, y, subjects, sfreq\n\n\n# --------------------------------\n# ۲) سگمنت‌کردن سیگنال‌ها\n# --------------------------------\ndef make_segments(X, y, subjects, sfreq,\n                  window_sec=WINDOW_SEC,\n                  overlap_ratio=OVERLAP_RATIO):\n    N, C, T = X.shape\n    win_size = int(window_sec * sfreq)\n    step_sec = window_sec * (1.0 - overlap_ratio)\n    step = max(1, int(step_sec * sfreq))\n\n    seg_X_list, seg_y_list, seg_subj_list = [], [], []\n\n    for i in range(N):\n        data = X[i]\n        subj_label = y[i]\n        subj_id = subjects[i]\n\n        start = 0\n        while start + win_size <= T:\n            seg = data[:, start:start+win_size]\n            seg_X_list.append(seg)\n            seg_y_list.append(subj_label)\n            seg_subj_list.append(subj_id)\n            start += step\n\n    seg_X = np.stack(seg_X_list, axis=0)\n    seg_y = np.array(seg_y_list, dtype=int)\n    seg_subjects = np.array(seg_subj_list)\n\n    print(\"Segmented data shape:\", seg_X.shape)\n    return seg_X, seg_y, seg_subjects\n\n\n# --------------------------------\n# ۳) بخش PyTorch (SSL + Encoder)\n# --------------------------------\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n# cuDNN را فعال می‌گذاریم (برای سرعت)، فقط TF32 را خاموش می‌کنیم برای پایداری\ntorch.backends.cudnn.enabled = True\ntorch.backends.cudnn.benchmark = True\ntorch.backends.cuda.matmul.allow_tf32 = False\ntorch.backends.cudnn.allow_tf32 = False\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", DEVICE)\n\ntorch.manual_seed(RANDOM_STATE)\n\n\n# --------- Augmentations برای EEG (ملایم) ---------\nclass EEGAugment:\n    def __init__(self, jitter_std=0.005, time_shift=0.05, dropout_p=0.05):\n        self.jitter_std = jitter_std\n        self.time_shift = time_shift\n        self.dropout_p = dropout_p\n\n    def __call__(self, x):\n        # x: (C, T) tensor\n        x = self.jitter(x)\n        x = self.time_shift_aug(x)\n        x = self.channel_dropout(x)\n        return x\n\n    def jitter(self, x):\n        noise = torch.randn_like(x) * self.jitter_std\n        return x + noise\n\n    def time_shift_aug(self, x):\n        T = x.shape[1]\n        if T <= 1:\n            return x\n        shift = int(self.time_shift * T)\n        if shift == 0:\n            return x\n        k = np.random.randint(-shift, shift + 1)\n        return torch.roll(x, shifts=k, dims=1)\n\n    def channel_dropout(self, x):\n        # dropout روی کانال‌ها\n        mask = (torch.rand(x.shape[0], 1, device=x.device) > self.dropout_p).float()\n        return x * mask\n\n\n# --------- Dataset برای SSL ---------\nclass EEGSSL_Dataset(Dataset):\n    def __init__(self, X, augment):\n        # X: (N, C, T)\n        self.X = torch.from_numpy(X).float()\n        self.augment = augment\n\n    def __len__(self):\n        return self.X.shape[0]\n\n    def __getitem__(self, idx):\n        x = self.X[idx]  # (C, T)\n        v1 = self.augment(x)\n        v2 = self.augment(x)\n        return v1, v2\n\n\n# --------- Encoder CNN ---------\nclass EEGEncoder(nn.Module):\n    def __init__(self, n_channels, input_len, feat_dim=128):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv1d(n_channels, 32, kernel_size=7, padding=3),\n            nn.BatchNorm1d(32),\n            nn.ReLU(),\n            nn.MaxPool1d(2),\n\n            nn.Conv1d(32, 64, kernel_size=5, padding=2),\n            nn.BatchNorm1d(64),\n            nn.ReLU(),\n            nn.MaxPool1d(2),\n\n            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n            nn.BatchNorm1d(128),\n            nn.ReLU(),\n            nn.AdaptiveAvgPool1d(1),\n        )\n        self.fc = nn.Linear(128, feat_dim)\n\n    def forward(self, x):\n        # x: (B, C, T)\n        h = self.conv(x).squeeze(-1)  # (B, 128)\n        z = self.fc(h)                # (B, feat_dim)\n        return F.normalize(z, dim=1)  # نرمال‌سازی روی بردار embedding\n\n\n# --------- NT-Xent (SimCLR استاندارد) ---------\ndef nt_xent_loss(z1, z2, temperature=0.1):\n    \"\"\"\n    z1, z2: (B, D), normalized embeddings\n    SimCLR NT-Xent: هر نمونه در view1 پارتنرش در view2 است و برعکس.\n    \"\"\"\n    B = z1.size(0)\n\n    # ۲B تا embedding پشت سر هم\n    z = torch.cat([z1, z2], dim=0)         # (2B, D)\n\n    # cosine similarity بین همه‌ی جفت‌ها: (2B, 2B)\n    sim = F.cosine_similarity(\n        z.unsqueeze(1),  # (2B, 1, D)\n        z.unsqueeze(0),  # (1, 2B, D)\n        dim=2\n    )  # -> (2B, 2B)\n\n    # positive index:\n    # view1[i] ↔ view2[i]  → برای i در [0..B-1]\n    labels = torch.arange(B, device=z.device)\n    labels = torch.cat([labels + B, labels], dim=0)   # (2B,)\n\n    # self-sim رو حذف می‌کنیم (قطر)\n    mask = torch.eye(2 * B, dtype=torch.bool, device=z.device)\n    sim = sim.masked_fill(mask, -1e9)\n\n    # scale by temperature و cross-entropy\n    sim = sim / temperature\n    loss = F.cross_entropy(sim, labels)\n    return loss\n\n\n# --------- Pretraining SimCLR ---------\ndef pretrain_simclr(seg_X, num_epochs=20, batch_size=128, feat_dim=128):\n    augment = EEGAugment()\n    ds_ssl = EEGSSL_Dataset(seg_X, augment)\n    dl_ssl = DataLoader(ds_ssl, batch_size=batch_size,\n                        shuffle=True, drop_last=True)\n\n    n_channels = seg_X.shape[1]\n    input_len = seg_X.shape[2]\n    encoder = EEGEncoder(n_channels, input_len,\n                         feat_dim=feat_dim).to(DEVICE)\n\n    optimizer = torch.optim.Adam(encoder.parameters(),\n                                 lr=1e-3, weight_decay=1e-4)\n\n    for epoch in range(num_epochs):\n        encoder.train()\n        total_loss = 0.0\n        for v1, v2 in dl_ssl:\n            v1 = v1.to(DEVICE)\n            v2 = v2.to(DEVICE)\n\n            z1 = encoder(v1)\n            z2 = encoder(v2)\n\n            loss = nt_xent_loss(z1, z2, temperature=0.1)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n\n        avg_loss = total_loss / len(dl_ssl)\n        print(f\"[SSL] Epoch {epoch+1}/{num_epochs}, loss={avg_loss:.4f}\")\n\n    return encoder\n\n\n# --------- استخراج embedding ---------\ndef extract_embeddings(encoder, seg_X, batch_size=256):\n    encoder.eval()\n    ds = torch.from_numpy(seg_X).float()\n    all_emb = []\n    with torch.no_grad():\n        for i in range(0, ds.shape[0], batch_size):\n            xb = ds[i:i+batch_size].to(DEVICE)\n            z = encoder(xb)\n            all_emb.append(z.cpu().numpy())\n    return np.concatenate(all_emb, axis=0)\n\n\n# --------- LOSO Evaluation با SVM + StandardScaler ---------\ndef evaluate_ssl_loso(encoder, seg_X, seg_y, seg_subjects):\n    feats_all = extract_embeddings(encoder, seg_X)\n    unique_subjects = np.unique(seg_subjects)\n    subj_accs = []\n\n    for test_subj in unique_subjects:\n        print(\"\\n\" + \"=\" * 50)\n        print(\"Test subject:\", test_subj)\n\n        test_mask = (seg_subjects == test_subj)\n        train_mask = ~test_mask\n\n        X_train = feats_all[train_mask]\n        y_train = seg_y[train_mask]\n        X_test = feats_all[test_mask]\n        y_test = seg_y[test_mask]\n\n        print(\"Train segments:\", X_train.shape[0],\n              \"| Test segments:\", X_test.shape[0])\n\n        clf = make_pipeline(\n            StandardScaler(),\n            SVC(kernel=\"rbf\", class_weight=\"balanced\",\n                random_state=RANDOM_STATE)\n        )\n        clf.fit(X_train, y_train)\n        y_pred_seg = clf.predict(X_test)\n\n        # subject-level majority vote\n        true_label = y_test[0]\n        counts = np.bincount(y_pred_seg)\n        pred_label = np.argmax(counts)\n        acc_subj = 1.0 if pred_label == true_label else 0.0\n        print(f\"Subject-level -> true: {true_label}, \"\n              f\"pred: {pred_label}, acc: {acc_subj:.4f}\")\n        subj_accs.append(acc_subj)\n\n    subj_accs = np.array(subj_accs)\n    print(\"\\n\" + \"=\" * 50)\n    print(\"Average SUBJECT-level (LOSO, SSL+SVM):\",\n          subj_accs.mean())\n\n\n# --------------------------------\n# main\n# --------------------------------\ndef main():\n    X, y, subjects, sfreq = load_task_edf_with_baseline()\n    seg_X, seg_y, seg_subjects = make_segments(X, y, subjects, sfreq)\n\n    print(\"Pretraining SimCLR-style SSL on all segments...\")\n    encoder = pretrain_simclr(seg_X,\n                              num_epochs=20,   # برای تست سریع می‌تونی 10 بذاری\n                              batch_size=128,\n                              feat_dim=128)\n\n    print(\"Evaluating LOSO using frozen encoder + SVM...\")\n    evaluate_ssl_loso(encoder, seg_X, seg_y, seg_subjects)\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T19:21:21.176342Z","iopub.execute_input":"2025-12-11T19:21:21.177011Z","iopub.status.idle":"2025-12-11T19:22:28.414044Z","shell.execute_reply.started":"2025-12-11T19:21:21.176983Z","shell.execute_reply":"2025-12-11T19:22:28.413371Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nLoading Subject00_1.edf (rest) and Subject00_2.edf (task)...\nLoading Subject01_1.edf (rest) and Subject01_2.edf (task)...\nLoading Subject02_1.edf (rest) and Subject02_2.edf (task)...\nLoading Subject03_1.edf (rest) and Subject03_2.edf (task)...\nLoading Subject04_1.edf (rest) and Subject04_2.edf (task)...\nLoading Subject05_1.edf (rest) and Subject05_2.edf (task)...\nLoading Subject06_1.edf (rest) and Subject06_2.edf (task)...\nLoading Subject07_1.edf (rest) and Subject07_2.edf (task)...\nLoading Subject08_1.edf (rest) and Subject08_2.edf (task)...\nLoading Subject09_1.edf (rest) and Subject09_2.edf (task)...\nLoading Subject10_1.edf (rest) and Subject10_2.edf (task)...\nLoading Subject11_1.edf (rest) and Subject11_2.edf (task)...\nLoading Subject12_1.edf (rest) and Subject12_2.edf (task)...\nLoading Subject13_1.edf (rest) and Subject13_2.edf (task)...\nLoading Subject14_1.edf (rest) and Subject14_2.edf (task)...\nLoading Subject15_1.edf (rest) and Subject15_2.edf (task)...\nLoading Subject16_1.edf (rest) and Subject16_2.edf (task)...\nLoading Subject17_1.edf (rest) and Subject17_2.edf (task)...\nLoading Subject18_1.edf (rest) and Subject18_2.edf (task)...\nLoading Subject19_1.edf (rest) and Subject19_2.edf (task)...\nLoading Subject20_1.edf (rest) and Subject20_2.edf (task)...\nLoading Subject21_1.edf (rest) and Subject21_2.edf (task)...\nLoading Subject22_1.edf (rest) and Subject22_2.edf (task)...\nLoading Subject23_1.edf (rest) and Subject23_2.edf (task)...\nLoading Subject24_1.edf (rest) and Subject24_2.edf (task)...\nLoading Subject25_1.edf (rest) and Subject25_2.edf (task)...\nLoading Subject26_1.edf (rest) and Subject26_2.edf (task)...\nLoading Subject27_1.edf (rest) and Subject27_2.edf (task)...\nLoading Subject28_1.edf (rest) and Subject28_2.edf (task)...\nLoading Subject29_1.edf (rest) and Subject29_2.edf (task)...\nLoading Subject30_1.edf (rest) and Subject30_2.edf (task)...\nLoading Subject31_1.edf (rest) and Subject31_2.edf (task)...\nLoading Subject32_1.edf (rest) and Subject32_2.edf (task)...\nLoading Subject33_1.edf (rest) and Subject33_2.edf (task)...\nLoading Subject34_1.edf (rest) and Subject34_2.edf (task)...\nLoading Subject35_1.edf (rest) and Subject35_2.edf (task)...\nFinal tensor X shape (N, C, T): (36, 21, 7936)\nLabels y shape: (36,)\nSegmented data shape: (4356, 21, 256)\nPretraining SimCLR-style SSL on all segments...\n[SSL] Epoch 1/20, loss=5.5421\n[SSL] Epoch 2/20, loss=5.5415\n[SSL] Epoch 3/20, loss=5.5417\n[SSL] Epoch 4/20, loss=5.5412\n[SSL] Epoch 5/20, loss=5.5413\n[SSL] Epoch 6/20, loss=5.5413\n[SSL] Epoch 7/20, loss=5.5410\n[SSL] Epoch 8/20, loss=5.5415\n[SSL] Epoch 9/20, loss=5.5414\n[SSL] Epoch 10/20, loss=5.5412\n[SSL] Epoch 11/20, loss=5.5416\n[SSL] Epoch 12/20, loss=5.5411\n[SSL] Epoch 13/20, loss=5.5414\n[SSL] Epoch 14/20, loss=5.5412\n[SSL] Epoch 15/20, loss=5.5414\n[SSL] Epoch 16/20, loss=5.5413\n[SSL] Epoch 17/20, loss=5.5412\n[SSL] Epoch 18/20, loss=5.5413\n[SSL] Epoch 19/20, loss=5.5415\n[SSL] Epoch 20/20, loss=5.5412\nEvaluating LOSO using frozen encoder + SVM...\n\n==================================================\nTest subject: Subject00\nTrain segments: 4235 | Test segments: 121\nSubject-level -> true: 0, pred: 1, acc: 0.0000\n\n==================================================\nTest subject: Subject01\nTrain segments: 4235 | Test segments: 121\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n==================================================\nTest subject: Subject02\nTrain segments: 4235 | Test segments: 121\nSubject-level -> true: 1, pred: 0, acc: 0.0000\n\n==================================================\nTest subject: Subject03\nTrain segments: 4235 | Test segments: 121\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n==================================================\nTest subject: Subject04\nTrain segments: 4235 | Test segments: 121\nSubject-level -> true: 0, pred: 1, acc: 0.0000\n\n==================================================\nTest subject: Subject05\nTrain segments: 4235 | Test segments: 121\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n==================================================\nTest subject: Subject06\nTrain segments: 4235 | Test segments: 121\nSubject-level -> true: 0, pred: 1, acc: 0.0000\n\n==================================================\nTest subject: Subject07\nTrain segments: 4235 | Test segments: 121\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n==================================================\nTest subject: Subject08\nTrain segments: 4235 | Test segments: 121\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n==================================================\nTest subject: Subject09\nTrain segments: 4235 | Test segments: 121\nSubject-level -> true: 0, pred: 1, acc: 0.0000\n\n==================================================\nTest subject: Subject10\nTrain segments: 4235 | Test segments: 121\nSubject-level -> true: 0, pred: 1, acc: 0.0000\n\n==================================================\nTest subject: Subject11\nTrain segments: 4235 | Test segments: 121\nSubject-level -> true: 1, pred: 0, acc: 0.0000\n\n==================================================\nTest subject: Subject12\nTrain segments: 4235 | Test segments: 121\nSubject-level -> true: 1, pred: 0, acc: 0.0000\n\n==================================================\nTest subject: Subject13\nTrain segments: 4235 | Test segments: 121\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n==================================================\nTest subject: Subject14\nTrain segments: 4235 | Test segments: 121\nSubject-level -> true: 0, pred: 1, acc: 0.0000\n\n==================================================\nTest subject: Subject15\nTrain segments: 4235 | Test segments: 121\nSubject-level -> true: 1, pred: 0, acc: 0.0000\n\n==================================================\nTest subject: Subject16\nTrain segments: 4235 | Test segments: 121\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n==================================================\nTest subject: Subject17\nTrain segments: 4235 | Test segments: 121\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n==================================================\nTest subject: Subject18\nTrain segments: 4235 | Test segments: 121\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n==================================================\nTest subject: Subject19\nTrain segments: 4235 | Test segments: 121\nSubject-level -> true: 0, pred: 1, acc: 0.0000\n\n==================================================\nTest subject: Subject20\nTrain segments: 4235 | Test segments: 121\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n==================================================\nTest subject: Subject21\nTrain segments: 4235 | Test segments: 121\nSubject-level -> true: 0, pred: 0, acc: 1.0000\n\n==================================================\nTest subject: Subject22\nTrain segments: 4235 | Test segments: 121\nSubject-level -> true: 0, pred: 0, acc: 1.0000\n\n==================================================\nTest subject: Subject23\nTrain segments: 4235 | Test segments: 121\nSubject-level -> true: 1, pred: 0, acc: 0.0000\n\n==================================================\nTest subject: Subject24\nTrain segments: 4235 | Test segments: 121\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n==================================================\nTest subject: Subject25\nTrain segments: 4235 | Test segments: 121\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n==================================================\nTest subject: Subject26\nTrain segments: 4235 | Test segments: 121\nSubject-level -> true: 1, pred: 0, acc: 0.0000\n\n==================================================\nTest subject: Subject27\nTrain segments: 4235 | Test segments: 121\nSubject-level -> true: 1, pred: 0, acc: 0.0000\n\n==================================================\nTest subject: Subject28\nTrain segments: 4235 | Test segments: 121\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n==================================================\nTest subject: Subject29\nTrain segments: 4235 | Test segments: 121\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n==================================================\nTest subject: Subject30\nTrain segments: 4235 | Test segments: 121\nSubject-level -> true: 0, pred: 0, acc: 1.0000\n\n==================================================\nTest subject: Subject31\nTrain segments: 4235 | Test segments: 121\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n==================================================\nTest subject: Subject32\nTrain segments: 4235 | Test segments: 121\nSubject-level -> true: 1, pred: 0, acc: 0.0000\n\n==================================================\nTest subject: Subject33\nTrain segments: 4235 | Test segments: 121\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n==================================================\nTest subject: Subject34\nTrain segments: 4235 | Test segments: 121\nSubject-level -> true: 1, pred: 0, acc: 0.0000\n\n==================================================\nTest subject: Subject35\nTrain segments: 4235 | Test segments: 121\nSubject-level -> true: 1, pred: 1, acc: 1.0000\n\n==================================================\nAverage SUBJECT-level (LOSO, SSL+SVM): 0.5555555555555556\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"---\nthst\n---\n---","metadata":{}},{"cell_type":"code","source":"# ============================================\n#   Dual-Path CNN (Temporal + Channel Branch)\n#   LOSO روی EEGMAT با class-weight و val سوژه‌ای\n# ============================================\n\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport mne\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n# -----------------------------\n# تنظیمات دیتاست و مدل\n# -----------------------------\nDATA_FOLDER = \"/kaggle/input/ahmadi-dataset\"    # مسیر دیتاست EEGMAT\nINFO_CSV = f\"{DATA_FOLDER}/subject-info.csv\"\n\nRESAMPLE_TO = 128         # Hz\nWINDOW_SEC = 2.0          # طول سگمنت (ثانیه)\nOVERLAP_RATIO = 0.75      # درصد overlap بین سگمنت‌ها\n\nBATCH_SIZE = 64\nNUM_EPOCHS = 10           # می‌تونی بعداً بیشترش کنی (مثلاً 20)\nLR = 1e-3\nWEIGHT_DECAY = 1e-4\nRANDOM_STATE = 42\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", DEVICE)\nif DEVICE.type == \"cuda\":\n    print(\"CUDA version:\", torch.version.cuda)\n    print(\"GPU name:\", torch.cuda.get_device_name(0))\n\ntorch.manual_seed(RANDOM_STATE)\nnp.random.seed(RANDOM_STATE)\n\n# -----------------------------\n# ۱) لود EEG + baseline از rest\n# -----------------------------\ndef load_task_edf_with_baseline(folder_path=DATA_FOLDER,\n                                info_csv_path=INFO_CSV,\n                                resample_to=RESAMPLE_TO,\n                                use_baseline=True):\n    folder = Path(folder_path)\n    info_df = pd.read_csv(info_csv_path)\n    label_map = dict(zip(info_df[\"Subject\"], info_df[\"Count quality\"]))\n\n    X_list, y_list, subjects = [], [], []\n    sfreq = None\n\n    for subj_name in info_df[\"Subject\"]:\n        task_file = folder / f\"{subj_name}_2.edf\"\n        rest_file = folder / f\"{subj_name}_1.edf\"\n\n        if not task_file.is_file():\n            print(f\"Task file not found for {subj_name}, skipping.\")\n            continue\n\n        if subj_name not in label_map:\n            print(f\"{subj_name} not in subject-info, skipping.\")\n            continue\n\n        print(f\"Loading {subj_name}_1.edf (rest) and {subj_name}_2.edf (task)...\")\n        raw_task = mne.io.read_raw_edf(task_file, preload=True, verbose=False)\n\n        raw_rest = None\n        if use_baseline and rest_file.is_file():\n            raw_rest = mne.io.read_raw_edf(rest_file, preload=True, verbose=False)\n\n        if resample_to is not None:\n            raw_task.resample(resample_to)\n            if raw_rest is not None:\n                raw_rest.resample(resample_to)\n\n        if sfreq is None:\n            sfreq = raw_task.info[\"sfreq\"]\n\n        task_data = raw_task.get_data()  # (C, T)\n\n        if use_baseline and raw_rest is not None:\n            rest_data = raw_rest.get_data()\n            baseline = rest_data.mean(axis=1, keepdims=True)  # (C,1)\n            task_data = task_data - baseline\n\n        X_list.append(task_data)\n        y_list.append(int(label_map[subj_name]))\n        subjects.append(subj_name)\n\n    if not X_list:\n        raise ValueError(\"No task files loaded.\")\n\n    lengths = [d.shape[1] for d in X_list]\n    min_len = min(lengths)\n    X = np.stack([d[:, :min_len] for d in X_list], axis=0)  # (N, C, T)\n    y = np.array(y_list, dtype=int)\n    subjects = np.array(subjects)\n\n    print(\"Final tensor X shape (N, C, T):\", X.shape)\n    print(\"Labels y shape:\", y.shape)\n\n    return X, y, subjects, sfreq\n\n\n# -----------------------------\n# ۲) سگمنت کردن سیگنال‌ها\n# -----------------------------\ndef make_segments(X, y, subjects, sfreq,\n                  window_sec=WINDOW_SEC,\n                  overlap_ratio=OVERLAP_RATIO):\n    N, C, T = X.shape\n    win_size = int(window_sec * sfreq)\n    step_sec = window_sec * (1.0 - overlap_ratio)\n    step = max(1, int(step_sec * sfreq))\n\n    seg_X_list, seg_y_list, seg_subj_list = [], [], []\n\n    for i in range(N):\n        data = X[i]\n        subj_label = y[i]\n        subj_id = subjects[i]\n\n        start = 0\n        while start + win_size <= T:\n            seg = data[:, start:start+win_size]     # (C, win_size)\n            seg_X_list.append(seg)\n            seg_y_list.append(subj_label)\n            seg_subj_list.append(subj_id)\n            start += step\n\n    seg_X = np.stack(seg_X_list, axis=0)       # (N_seg, C, win_size)\n    seg_y = np.array(seg_y_list, dtype=int)\n    seg_subj = np.array(seg_subj_list)\n\n    print(\"Segmented data shape:\", seg_X.shape)\n    return seg_X, seg_y, seg_subj\n\n\n# -----------------------------\n# ۳) نرمال‌سازی بر اساس train\n# -----------------------------\ndef standardize_train_val_test(X_train, X_val, X_test):\n    mean = X_train.mean()\n    std = X_train.std()\n    if std == 0:\n        std = 1.0\n    X_train_norm = (X_train - mean) / std\n    X_val_norm = (X_val - mean) / std\n    X_test_norm = (X_test - mean) / std\n    return (X_train_norm.astype(\"float32\"),\n            X_val_norm.astype(\"float32\"),\n            X_test_norm.astype(\"float32\"))\n\n\n# -----------------------------\n# ۴) Dataset برای PyTorch\n# -----------------------------\nclass EEGSegDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.from_numpy(X).float()\n        self.y = torch.from_numpy(y).long()\n\n    def __len__(self):\n        return self.X.shape[0]\n\n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]\n\n\n# -----------------------------\n# ۵) Dual-Path CNN مدل\n# -----------------------------\nclass DualPathEEGNet(nn.Module):\n    \"\"\"\n    شاخه اول: روی (C, T) → فیچرهای زمانی (Temporal)\n    شاخه دوم: روی (T, C) → Conv روی Channel → فیچرهای فضایی/چنلی\n    \"\"\"\n    def __init__(self, n_channels, n_timepoints,\n                 temporal_feat_dim=128, spatial_feat_dim=128,\n                 hidden_dim=128, n_classes=2):\n        super().__init__()\n\n        # --- Temporal Branch (B, C, T) ---\n        self.temporal_branch = nn.Sequential(\n            nn.Conv1d(n_channels, 64, kernel_size=7, padding=3),\n            nn.BatchNorm1d(64),\n            nn.ReLU(),\n            nn.MaxPool1d(2),\n\n            nn.Conv1d(64, 128, kernel_size=5, padding=2),\n            nn.BatchNorm1d(128),\n            nn.ReLU(),\n            nn.MaxPool1d(2),\n\n            nn.Conv1d(128, temporal_feat_dim, kernel_size=3, padding=1),\n            nn.BatchNorm1d(temporal_feat_dim),\n            nn.ReLU(),\n            nn.AdaptiveAvgPool1d(1)  # → (B, temporal_feat_dim, 1)\n        )\n\n        # --- Spatial/Channel Branch ---\n        # x: (B, C, T) → (B, T, C) → (B*T, 1, C)\n        self.spatial_conv = nn.Sequential(\n            nn.Conv1d(1, 32, kernel_size=3, padding=1),\n            nn.BatchNorm1d(32),\n            nn.ReLU(),\n            nn.MaxPool1d(2),\n\n            nn.Conv1d(32, 64, kernel_size=3, padding=1),\n            nn.BatchNorm1d(64),\n            nn.ReLU(),\n            nn.MaxPool1d(2),\n\n            nn.Conv1d(64, spatial_feat_dim, kernel_size=3, padding=1),\n            nn.BatchNorm1d(spatial_feat_dim),\n            nn.ReLU(),\n            nn.AdaptiveAvgPool1d(1)  # → (B*T, spatial_feat_dim, 1)\n        )\n\n        fused_dim = temporal_feat_dim + spatial_feat_dim\n        self.fusion_dropout = nn.Dropout(0.5)\n        self.classifier = nn.Sequential(\n            nn.Linear(fused_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(hidden_dim, n_classes)\n        )\n\n    def forward(self, x):\n        \"\"\"\n        x: (B, C, T)\n        \"\"\"\n        B, C, T = x.shape\n\n        # --- Temporal branch ---\n        t_feat = self.temporal_branch(x)          # (B, temporal_feat_dim, 1)\n        t_feat = t_feat.squeeze(-1)               # (B, temporal_feat_dim)\n\n        # --- Spatial branch ---\n        x_tc = x.transpose(1, 2)                  # (B, T, C)\n        x_tc_flat = x_tc.reshape(B * T, 1, C)     # (B*T, 1, C)\n        s_feat = self.spatial_conv(x_tc_flat)     # (B*T, spatial_feat_dim, 1)\n        s_feat = s_feat.squeeze(-1)               # (B*T, spatial_feat_dim)\n        s_feat = s_feat.reshape(B, T, -1)         # (B, T, spatial_feat_dim)\n        s_feat = s_feat.mean(dim=1)               # (B, spatial_feat_dim)\n\n        fused = torch.cat([t_feat, s_feat], dim=1)  # (B, fused_dim)\n        fused = self.fusion_dropout(fused)\n        logits = self.classifier(fused)           # (B, n_classes)\n        return logits\n\n\n# -----------------------------\n# ۶) train و evaluate روی یک fold\n# -----------------------------\ndef train_one_fold(model, train_loader, val_loader,\n                   num_epochs=NUM_EPOCHS,\n                   lr=LR, weight_decay=WEIGHT_DECAY,\n                   device=DEVICE,\n                   class_weights=None):\n    model.to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr,\n                                 weight_decay=weight_decay)\n\n    if class_weights is not None:\n        criterion = nn.CrossEntropyLoss(weight=class_weights)\n    else:\n        criterion = nn.CrossEntropyLoss()\n\n    best_val_f1 = -1.0\n    best_state = None\n\n    for epoch in range(num_epochs):\n        # ----- train -----\n        model.train()\n        total_loss = 0.0\n        for xb, yb in train_loader:\n            xb = xb.to(device)\n            yb = yb.to(device)\n\n            optimizer.zero_grad()\n            logits = model(xb)\n            loss = criterion(logits, yb)\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n\n        avg_loss = total_loss / len(train_loader)\n\n        # ----- validation -----\n        model.eval()\n        all_y = []\n        all_pred = []\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                xb = xb.to(device)\n                yb = yb.to(device)\n\n                logits = model(xb)\n                preds = torch.argmax(logits, dim=1)\n\n                all_y.append(yb.cpu().numpy())\n                all_pred.append(preds.cpu().numpy())\n\n        all_y = np.concatenate(all_y)\n        all_pred = np.concatenate(all_pred)\n\n        acc = accuracy_score(all_y, all_pred)\n        prec = precision_score(all_y, all_pred, zero_division=0)\n        rec = recall_score(all_y, all_pred, zero_division=0)\n        f1 = f1_score(all_y, all_pred, zero_division=0)\n\n        print(f\"Epoch {epoch+1}/{num_epochs} | \"\n              f\"train_loss={avg_loss:.4f} | \"\n              f\"val_acc={acc:.4f}, val_f1={f1:.4f}\")\n\n        if f1 > best_val_f1:\n            best_val_f1 = f1\n            best_state = model.state_dict()\n\n    if best_state is not None:\n        model.load_state_dict(best_state)\n\n    return model\n\n\ndef evaluate_on_segments(model, loader, device=DEVICE):\n    model.eval()\n    all_y, all_pred = [], []\n    with torch.no_grad():\n        for xb, yb in loader:\n            xb = xb.to(device)\n            yb = yb.to(device)\n            logits = model(xb)\n            preds = torch.argmax(logits, dim=1)\n            all_y.append(yb.cpu().numpy())\n            all_pred.append(preds.cpu().numpy())\n\n    all_y = np.concatenate(all_y)\n    all_pred = np.concatenate(all_pred)\n\n    acc = accuracy_score(all_y, all_pred)\n    prec = precision_score(all_y, all_pred, zero_division=0)\n    rec = recall_score(all_y, all_pred, zero_division=0)\n    f1 = f1_score(all_y, all_pred, zero_division=0)\n    return acc, prec, rec, f1, all_y, all_pred\n\n\n# -----------------------------\n# ۷) LOSO evaluation با val سوژه‌ای\n# -----------------------------\ndef loso_dualpath(X_seg, y_seg, subj_seg):\n    unique_subjects = np.unique(subj_seg)\n    results_seg = []\n    results_subj = []\n\n    N_seg, C, T = X_seg.shape\n    print(\"Segmented data shape:\", X_seg.shape)\n\n    for fold_idx, test_subj in enumerate(unique_subjects):\n        print(\"\\n\" + \"=\" * 60)\n        print(\"Test subject:\", test_subj)\n\n        is_test = (subj_seg == test_subj)\n        is_train_all = ~is_test\n\n        # سوژه‌های train\n        train_subjects = np.unique(subj_seg[is_train_all])\n\n        # سوژه‌های val (20% از train subjects)\n        rng = np.random.RandomState(RANDOM_STATE + fold_idx)\n        perm_subj = rng.permutation(train_subjects)\n        n_val_subj = max(1, int(0.2 * len(train_subjects)))\n        val_subj = perm_subj[:n_val_subj]\n        real_train_subj = perm_subj[n_val_subj:]\n\n        is_val = np.isin(subj_seg, val_subj) & is_train_all\n        is_train = np.isin(subj_seg, real_train_subj)\n\n        X_train = X_seg[is_train]\n        y_train = y_seg[is_train]\n        X_val = X_seg[is_val]\n        y_val = y_seg[is_val]\n        X_test = X_seg[is_test]\n        y_test = y_seg[is_test]\n\n        print(f\"Train segments: {X_train.shape[0]} | \"\n              f\"Val segments: {X_val.shape[0]} | \"\n              f\"Test segments: {X_test.shape[0]}\")\n\n        # نرمال‌سازی\n        X_train_norm, X_val_norm, X_test_norm = standardize_train_val_test(\n            X_train, X_val, X_test\n        )\n\n        # class weights از روی y_train\n        class_counts = np.bincount(y_train, minlength=2)\n        class_counts[class_counts == 0] = 1\n        total = class_counts.sum()\n        num_classes = len(class_counts)\n        weights_np = total / (num_classes * class_counts.astype(np.float32))\n        class_weights = torch.tensor(weights_np, dtype=torch.float32).to(DEVICE)\n        print(\"Class counts (train):\", class_counts, \"-> weights:\", weights_np)\n\n        train_ds = EEGSegDataset(X_train_norm, y_train)\n        val_ds = EEGSegDataset(X_val_norm, y_val)\n        test_ds = EEGSegDataset(X_test_norm, y_test)\n\n        train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE,\n                                  shuffle=True, drop_last=False)\n        val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE,\n                                shuffle=False, drop_last=False)\n        test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE,\n                                 shuffle=False, drop_last=False)\n\n        model = DualPathEEGNet(\n            n_channels=C,\n            n_timepoints=T,\n            temporal_feat_dim=128,\n            spatial_feat_dim=128,\n            hidden_dim=128,\n            n_classes=2\n        )\n\n        model = train_one_fold(\n            model,\n            train_loader,\n            val_loader,\n            num_epochs=NUM_EPOCHS,\n            lr=LR,\n            weight_decay=WEIGHT_DECAY,\n            device=DEVICE,\n            class_weights=class_weights\n        )\n\n        acc_s, prec_s, rec_s, f1_s, y_true_seg, y_pred_seg = evaluate_on_segments(\n            model, test_loader, device=DEVICE\n        )\n\n        print(f\"Segment-level metrics -> acc: {acc_s:.4f}, \"\n              f\"prec: {prec_s:.4f}, rec: {rec_s:.4f}, f1: {f1_s:.4f}\")\n        results_seg.append([acc_s, prec_s, rec_s, f1_s])\n\n        # subject-level majority vote\n        true_label = int(y_test[0])\n        counts = np.bincount(y_pred_seg)\n        pred_label = int(np.argmax(counts))\n        acc_subj = 1.0 if pred_label == true_label else 0.0\n        print(f\"Subject-level prediction -> true: {true_label}, \"\n              f\"pred: {pred_label}, acc: {acc_subj:.4f}\")\n        results_subj.append(acc_subj)\n\n    results_seg = np.array(results_seg)\n    subj_accs = np.array(results_subj)\n\n    print(\"\\n\" + \"=\" * 60)\n    print(\"Average SEGMENT-level (LOSO):\")\n    print(f\"Accuracy : {results_seg[:,0].mean():.4f}\")\n    print(f\"Precision: {results_seg[:,1].mean():.4f}\")\n    print(f\"Recall   : {results_seg[:,2].mean():.4f}\")\n    print(f\"F1-score : {results_seg[:,3].mean():.4f}\")\n\n    print(\"\\nAverage SUBJECT-level (LOSO, majority vote):\")\n    print(f\"Accuracy : {subj_accs.mean():.4f}\")\n\n\n# -----------------------------\n# ۸) main\n# -----------------------------\ndef main():\n    X, y, subjects, sfreq = load_task_edf_with_baseline()\n    X_seg, y_seg, subj_seg = make_segments(X, y, subjects, sfreq)\n    loso_dualpath(X_seg, y_seg, subj_seg)\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T19:38:32.211497Z","iopub.execute_input":"2025-12-11T19:38:32.211770Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nCUDA version: 12.4\nGPU name: Tesla P100-PCIE-16GB\nLoading Subject00_1.edf (rest) and Subject00_2.edf (task)...\nLoading Subject01_1.edf (rest) and Subject01_2.edf (task)...\nLoading Subject02_1.edf (rest) and Subject02_2.edf (task)...\nLoading Subject03_1.edf (rest) and Subject03_2.edf (task)...\nLoading Subject04_1.edf (rest) and Subject04_2.edf (task)...\nLoading Subject05_1.edf (rest) and Subject05_2.edf (task)...\nLoading Subject06_1.edf (rest) and Subject06_2.edf (task)...\nLoading Subject07_1.edf (rest) and Subject07_2.edf (task)...\nLoading Subject08_1.edf (rest) and Subject08_2.edf (task)...\nLoading Subject09_1.edf (rest) and Subject09_2.edf (task)...\nLoading Subject10_1.edf (rest) and Subject10_2.edf (task)...\nLoading Subject11_1.edf (rest) and Subject11_2.edf (task)...\nLoading Subject12_1.edf (rest) and Subject12_2.edf (task)...\nLoading Subject13_1.edf (rest) and Subject13_2.edf (task)...\nLoading Subject14_1.edf (rest) and Subject14_2.edf (task)...\nLoading Subject15_1.edf (rest) and Subject15_2.edf (task)...\nLoading Subject16_1.edf (rest) and Subject16_2.edf (task)...\nLoading Subject17_1.edf (rest) and Subject17_2.edf (task)...\nLoading Subject18_1.edf (rest) and Subject18_2.edf (task)...\nLoading Subject19_1.edf (rest) and Subject19_2.edf (task)...\nLoading Subject20_1.edf (rest) and Subject20_2.edf (task)...\nLoading Subject21_1.edf (rest) and Subject21_2.edf (task)...\nLoading Subject22_1.edf (rest) and Subject22_2.edf (task)...\nLoading Subject23_1.edf (rest) and Subject23_2.edf (task)...\nLoading Subject24_1.edf (rest) and Subject24_2.edf (task)...\nLoading Subject25_1.edf (rest) and Subject25_2.edf (task)...\nLoading Subject26_1.edf (rest) and Subject26_2.edf (task)...\nLoading Subject27_1.edf (rest) and Subject27_2.edf (task)...\nLoading Subject28_1.edf (rest) and Subject28_2.edf (task)...\nLoading Subject29_1.edf (rest) and Subject29_2.edf (task)...\nLoading Subject30_1.edf (rest) and Subject30_2.edf (task)...\nLoading Subject31_1.edf (rest) and Subject31_2.edf (task)...\nLoading Subject32_1.edf (rest) and Subject32_2.edf (task)...\nLoading Subject33_1.edf (rest) and Subject33_2.edf (task)...\nLoading Subject34_1.edf (rest) and Subject34_2.edf (task)...\nLoading Subject35_1.edf (rest) and Subject35_2.edf (task)...\nFinal tensor X shape (N, C, T): (36, 21, 7936)\nLabels y shape: (36,)\nSegmented data shape: (4356, 21, 256)\nSegmented data shape: (4356, 21, 256)\n\n============================================================\nTest subject: Subject00\nTrain segments: 3388 | Val segments: 847 | Test segments: 121\nClass counts (train): [ 726 2662] -> weights: [2.3333333 0.6363636]\nEpoch 1/10 | train_loss=0.4584 | val_acc=0.3022, val_f1=0.4268\nEpoch 2/10 | train_loss=0.1081 | val_acc=0.3530, val_f1=0.3911\nEpoch 3/10 | train_loss=0.0425 | val_acc=0.4286, val_f1=0.3995\nEpoch 4/10 | train_loss=0.0388 | val_acc=0.4357, val_f1=0.3541\nEpoch 5/10 | train_loss=0.0464 | val_acc=0.3164, val_f1=0.4722\nEpoch 6/10 | train_loss=0.0181 | val_acc=0.3447, val_f1=0.4959\nEpoch 7/10 | train_loss=0.0167 | val_acc=0.4109, val_f1=0.4271\nEpoch 8/10 | train_loss=0.0131 | val_acc=0.3282, val_f1=0.4677\nEpoch 9/10 | train_loss=0.0068 | val_acc=0.3920, val_f1=0.4408\nEpoch 10/10 | train_loss=0.0048 | val_acc=0.3684, val_f1=0.4557\nSegment-level metrics -> acc: 0.0000, prec: 0.0000, rec: 0.0000, f1: 0.0000\nSubject-level prediction -> true: 0, pred: 1, acc: 0.0000\n\n============================================================\nTest subject: Subject01\nTrain segments: 3388 | Val segments: 847 | Test segments: 121\nClass counts (train): [ 968 2420] -> weights: [1.75 0.7 ]\nEpoch 1/10 | train_loss=0.5557 | val_acc=0.6387, val_f1=0.7766\nEpoch 2/10 | train_loss=0.1525 | val_acc=0.3259, val_f1=0.3459\nEpoch 3/10 | train_loss=0.0671 | val_acc=0.3140, val_f1=0.3891\nEpoch 4/10 | train_loss=0.0501 | val_acc=0.5277, val_f1=0.6855\nEpoch 5/10 | train_loss=0.0375 | val_acc=0.7013, val_f1=0.8244\nEpoch 6/10 | train_loss=0.0238 | val_acc=0.7084, val_f1=0.8293\nEpoch 7/10 | train_loss=0.0242 | val_acc=0.6659, val_f1=0.7974\nEpoch 8/10 | train_loss=0.0249 | val_acc=0.4970, val_f1=0.6468\nEpoch 9/10 | train_loss=0.0174 | val_acc=0.6316, val_f1=0.7733\nEpoch 10/10 | train_loss=0.0163 | val_acc=0.7131, val_f1=0.8325\nSegment-level metrics -> acc: 1.0000, prec: 1.0000, rec: 1.0000, f1: 1.0000\nSubject-level prediction -> true: 1, pred: 1, acc: 1.0000\n\n============================================================\nTest subject: Subject02\nTrain segments: 3388 | Val segments: 847 | Test segments: 121\nClass counts (train): [ 847 2541] -> weights: [2.        0.6666667]\nEpoch 1/10 | train_loss=0.5692 | val_acc=0.5407, val_f1=0.6777\nEpoch 2/10 | train_loss=0.1677 | val_acc=0.5714, val_f1=0.7210\nEpoch 3/10 | train_loss=0.0817 | val_acc=0.5643, val_f1=0.7207\nEpoch 4/10 | train_loss=0.0400 | val_acc=0.5549, val_f1=0.6952\nEpoch 5/10 | train_loss=0.0401 | val_acc=0.5620, val_f1=0.7174\nEpoch 6/10 | train_loss=0.0202 | val_acc=0.5112, val_f1=0.6618\nEpoch 7/10 | train_loss=0.0485 | val_acc=0.4557, val_f1=0.4838\nEpoch 8/10 | train_loss=0.0183 | val_acc=0.5714, val_f1=0.7269\nEpoch 9/10 | train_loss=0.0161 | val_acc=0.5431, val_f1=0.6689\nEpoch 10/10 | train_loss=0.0094 | val_acc=0.5726, val_f1=0.7241\nSegment-level metrics -> acc: 1.0000, prec: 1.0000, rec: 1.0000, f1: 1.0000\nSubject-level prediction -> true: 1, pred: 1, acc: 1.0000\n\n============================================================\nTest subject: Subject03\nTrain segments: 3388 | Val segments: 847 | Test segments: 121\nClass counts (train): [ 847 2541] -> weights: [2.        0.6666667]\nEpoch 1/10 | train_loss=0.5331 | val_acc=0.5608, val_f1=0.7160\nEpoch 2/10 | train_loss=0.1192 | val_acc=0.5313, val_f1=0.6477\nEpoch 3/10 | train_loss=0.0652 | val_acc=0.5466, val_f1=0.6957\nEpoch 4/10 | train_loss=0.0421 | val_acc=0.5490, val_f1=0.7030\nEpoch 5/10 | train_loss=0.0414 | val_acc=0.4829, val_f1=0.5723\nEpoch 6/10 | train_loss=0.0322 | val_acc=0.5136, val_f1=0.6617\nEpoch 7/10 | train_loss=0.0173 | val_acc=0.5006, val_f1=0.6597\nEpoch 8/10 | train_loss=0.0227 | val_acc=0.5655, val_f1=0.7212\nEpoch 9/10 | train_loss=0.0521 | val_acc=0.5407, val_f1=0.6761\nEpoch 10/10 | train_loss=0.0294 | val_acc=0.4852, val_f1=0.6175\nSegment-level metrics -> acc: 0.9752, prec: 1.0000, rec: 0.9752, f1: 0.9874\nSubject-level prediction -> true: 1, pred: 1, acc: 1.0000\n\n============================================================\nTest subject: Subject04\nTrain segments: 3388 | Val segments: 847 | Test segments: 121\nClass counts (train): [ 968 2420] -> weights: [1.75 0.7 ]\nEpoch 1/10 | train_loss=0.5278 | val_acc=0.7816, val_f1=0.8771\nEpoch 2/10 | train_loss=0.1690 | val_acc=0.5112, val_f1=0.6109\nEpoch 3/10 | train_loss=0.0689 | val_acc=0.7969, val_f1=0.8868\nEpoch 4/10 | train_loss=0.0558 | val_acc=0.7556, val_f1=0.8539\nEpoch 5/10 | train_loss=0.0329 | val_acc=0.7462, val_f1=0.8544\nEpoch 6/10 | train_loss=0.0223 | val_acc=0.8359, val_f1=0.9105\nEpoch 7/10 | train_loss=0.0354 | val_acc=0.7285, val_f1=0.8331\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"---\nbi lstm\n---\n---","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport random\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport mne\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n# ======================\n# تنظیمات کلی\n# ======================\nDATA_FOLDER = \"/kaggle/input/ahmadi-dataset\"\nINFO_CSV = f\"{DATA_FOLDER}/subject-info.csv\"\n\nRESAMPLE_TO = 128\nRANDOM_STATE = 42\n\nWINDOW_SEC = 2.0\nOVERLAP_RATIO = 0.75\n\nUSE_BASELINE = True\n\nBATCH_SIZE = 128\nEPOCHS = 40\nLR = 1e-3\nPATIENCE = 6\n\n# BiLSTM\nHIDDEN = 64\nNUM_LAYERS = 2\nDROPOUT = 0.3\n\n# ======================\n# Seed\n# ======================\ndef seed_everything(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\nseed_everything(RANDOM_STATE)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)\n\n\n# ======================\n# لود داده + baseline correction\n# ======================\ndef load_task_edf_with_baseline(folder_path, info_csv_path, resample_to=None,\n                                use_baseline=True):\n    folder = Path(folder_path)\n    if not folder.is_dir():\n        raise NotADirectoryError(f\"{folder_path} is not a valid directory\")\n\n    info_df = pd.read_csv(info_csv_path)\n    label_map = dict(zip(info_df[\"Subject\"], info_df[\"Count quality\"]))\n\n    X_list, y_list, subjects = [], [], []\n    sfreq = None\n\n    for subj_name in info_df[\"Subject\"]:\n        task_file = folder / f\"{subj_name}_2.edf\"\n        rest_file = folder / f\"{subj_name}_1.edf\"\n\n        if not task_file.is_file():\n            print(f\"Task file not found for {subj_name}, skipping.\")\n            continue\n\n        if subj_name not in label_map:\n            print(f\"{subj_name} not in subject-info, skipping.\")\n            continue\n\n        raw_task = mne.io.read_raw_edf(task_file, preload=True, verbose=False)\n\n        raw_rest = None\n        if use_baseline and rest_file.is_file():\n            raw_rest = mne.io.read_raw_edf(rest_file, preload=True, verbose=False)\n\n        if resample_to is not None:\n            raw_task.resample(resample_to)\n            if raw_rest is not None:\n                raw_rest.resample(resample_to)\n\n        if sfreq is None:\n            sfreq = raw_task.info[\"sfreq\"]\n\n        task_data = raw_task.get_data()  # (C, T_task)\n\n        if use_baseline and raw_rest is not None:\n            rest_data = raw_rest.get_data()  # (C, T_rest)\n            baseline = rest_data.mean(axis=1, keepdims=True)\n            task_data = task_data - baseline\n\n        X_list.append(task_data)\n        y_list.append(int(label_map[subj_name]))\n        subjects.append(subj_name)\n\n    if not X_list:\n        raise ValueError(\"No files loaded. Check paths/names.\")\n\n    lengths = [d.shape[1] for d in X_list]\n    min_len = min(lengths)\n\n    X = np.stack([d[:, :min_len] for d in X_list], axis=0)  # (N, C, T)\n    y = np.array(y_list, dtype=int)\n    subjects = np.array(subjects)\n\n    print(\"X:\", X.shape, \"y:\", y.shape, \"sfreq:\", sfreq)\n    return X, y, subjects, sfreq\n\n\n# ======================\n# Segment با overlap 75%\n# ======================\ndef make_segments(X, y, subjects, sfreq, window_sec=2.0, overlap_ratio=0.75):\n    N, C, T = X.shape\n    win_size = int(window_sec * sfreq)          # 2s * 128 = 256\n    step_sec = window_sec * (1.0 - overlap_ratio)\n    step = max(1, int(step_sec * sfreq))        # 0.5s * 128 = 64\n\n    seg_X, seg_y, seg_subj = [], [], []\n\n    for i in range(N):\n        data = X[i]  # (C, T)\n        lab = y[i]\n        sid = subjects[i]\n\n        start = 0\n        while start + win_size <= T:\n            seg = data[:, start:start+win_size]  # (C, win)\n            seg_X.append(seg)\n            seg_y.append(lab)\n            seg_subj.append(sid)\n            start += step\n\n    seg_X = np.stack(seg_X, axis=0)  # (Nseg, C, win)\n    seg_y = np.array(seg_y, dtype=int)\n    seg_subj = np.array(seg_subj)\n\n    print(\"Segments:\", seg_X.shape)\n    return seg_X, seg_y, seg_subj\n\n\n# ======================\n# Dataset + Augmentation (فقط train)\n# ======================\nclass EEGSegDataset(Dataset):\n    def __init__(self, X_seq, y, augment=False):\n        \"\"\"\n        X_seq: (N, T, C) float32\n        y: (N,) int\n        \"\"\"\n        self.X = X_seq.astype(np.float32)\n        self.y = y.astype(np.float32)\n        self.augment = augment\n\n    def __len__(self):\n        return len(self.y)\n\n    def _augment(self, x):\n        # x: (T, C)\n        # 1) noise\n        if np.random.rand() < 0.5:\n            noise = np.random.normal(0, 0.02, size=x.shape).astype(np.float32)\n            x = x + noise\n        # 2) channel-wise scale\n        if np.random.rand() < 0.5:\n            scale = (1.0 + np.random.normal(0, 0.05, size=(1, x.shape[1]))).astype(np.float32)\n            x = x * scale\n        return x\n\n    def __getitem__(self, idx):\n        x = self.X[idx]\n        if self.augment:\n            x = self._augment(x.copy())\n        y = self.y[idx]\n        return torch.from_numpy(x), torch.tensor(y)\n\n\n# ======================\n# BiLSTM Model\n# ======================\nclass BiLSTMClassifier(nn.Module):\n    def __init__(self, n_channels, hidden=64, num_layers=2, dropout=0.3):\n        super().__init__()\n        self.lstm = nn.LSTM(\n            input_size=n_channels,\n            hidden_size=hidden,\n            num_layers=num_layers,\n            batch_first=True,\n            bidirectional=True,\n            dropout=dropout if num_layers > 1 else 0.0\n        )\n        self.norm = nn.LayerNorm(hidden * 2)\n        self.fc = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Linear(hidden * 2, 64),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(64, 1)\n        )\n\n    def forward(self, x):\n        # x: (B, T, C)\n        out, _ = self.lstm(x)       # (B, T, 2H)\n        out = out.mean(dim=1)       # mean pooling over time -> (B, 2H)\n        out = self.norm(out)\n        logits = self.fc(out).squeeze(1)  # (B,)\n        return logits\n\n\n# ======================\n# Utilities: normalization without leakage\n# ======================\ndef compute_train_norm_stats(X_train_seq):\n    # X_train_seq: (N, T, C)\n    flat = X_train_seq.reshape(-1, X_train_seq.shape[-1])  # (N*T, C)\n    mean = flat.mean(axis=0, keepdims=True)\n    std = flat.std(axis=0, keepdims=True) + 1e-6\n    return mean.astype(np.float32), std.astype(np.float32)\n\ndef apply_norm(X_seq, mean, std):\n    return ((X_seq - mean) / std).astype(np.float32)\n\n\n# ======================\n# Train/Val split by SUBJECT (برای LOSO بهتر)\n# ======================\ndef make_subject_val_split(train_subjects, val_ratio=0.15):\n    uniq = np.unique(train_subjects)\n    rng = np.random.RandomState(RANDOM_STATE)\n    rng.shuffle(uniq)\n    n_val = max(1, int(len(uniq) * val_ratio))\n    val_subj = set(uniq[:n_val])\n    val_mask = np.array([s in val_subj for s in train_subjects])\n    return val_mask\n\n\n# ======================\n# Training loop with early stopping\n# ======================\ndef train_one_fold(X_train, y_train, subj_train, n_channels):\n    # val split by subject\n    val_mask = make_subject_val_split(subj_train, val_ratio=0.15)\n    tr_mask = ~val_mask\n\n    X_tr, y_tr = X_train[tr_mask], y_train[tr_mask]\n    X_val, y_val = X_train[val_mask], y_train[val_mask]\n\n    # class imbalance handling\n    n_pos = (y_tr == 1).sum()\n    n_neg = (y_tr == 0).sum()\n    pos_weight = torch.tensor([n_neg / max(1, n_pos)], device=device, dtype=torch.float32)\n\n    # sampler (optional) - کمک می‌کند batchها متعادل‌تر شوند\n    weights = np.where(y_tr == 1, n_neg / max(1, n_pos), 1.0).astype(np.float32)\n    sampler = WeightedRandomSampler(weights=weights, num_samples=len(weights), replacement=True)\n\n    ds_tr = EEGSegDataset(X_tr, y_tr, augment=True)\n    ds_val = EEGSegDataset(X_val, y_val, augment=False)\n\n    dl_tr = DataLoader(ds_tr, batch_size=BATCH_SIZE, sampler=sampler, num_workers=2, pin_memory=True)\n    dl_val = DataLoader(ds_val, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n\n    model = BiLSTMClassifier(n_channels=n_channels, hidden=HIDDEN, num_layers=NUM_LAYERS, dropout=DROPOUT).to(device)\n    opt = torch.optim.Adam(model.parameters(), lr=LR)\n    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n\n    best_f1 = -1\n    best_state = None\n    patience = 0\n\n    for epoch in range(1, EPOCHS + 1):\n        model.train()\n        tr_losses = []\n\n        for xb, yb in dl_tr:\n            xb = xb.to(device)\n            yb = yb.to(device)\n\n            opt.zero_grad()\n            logits = model(xb)\n            loss = criterion(logits, yb)\n            loss.backward()\n            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            opt.step()\n            tr_losses.append(loss.item())\n\n        # validation\n        model.eval()\n        all_pred, all_true = [], []\n        with torch.no_grad():\n            for xb, yb in dl_val:\n                xb = xb.to(device)\n                logits = model(xb)\n                prob = torch.sigmoid(logits).cpu().numpy()\n                pred = (prob >= 0.5).astype(int)\n                all_pred.append(pred)\n                all_true.append(yb.numpy().astype(int))\n\n        all_pred = np.concatenate(all_pred)\n        all_true = np.concatenate(all_true)\n\n        f1 = f1_score(all_true, all_pred, zero_division=0)\n        avg_loss = float(np.mean(tr_losses))\n\n        # print short progress\n        if epoch == 1 or epoch % 5 == 0:\n            print(f\"  Epoch {epoch:02d} | train_loss={avg_loss:.4f} | val_f1={f1:.4f}\")\n\n        if f1 > best_f1:\n            best_f1 = f1\n            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n            patience = 0\n        else:\n            patience += 1\n            if patience >= PATIENCE:\n                break\n\n    # load best\n    model.load_state_dict(best_state)\n    model.eval()\n    return model\n\n\n# ======================\n# LOSO Evaluation\n# ======================\ndef evaluate_loso_bilstm(seg_X, seg_y, seg_subjects):\n    # تبدیل به (N, T, C)\n    X_seq = np.transpose(seg_X, (0, 2, 1)).astype(np.float32)  # (Nseg, 256, 21)\n    y = seg_y.astype(int)\n    subjects = seg_subjects\n\n    uniq_subj = np.unique(subjects)\n    print(\"Unique subjects:\", len(uniq_subj))\n\n    seg_metrics = []\n    subj_metrics = []\n\n    for test_subj in uniq_subj:\n        print(\"\\n\" + \"=\"*60)\n        print(\"Test subject:\", test_subj)\n\n        test_mask = (subjects == test_subj)\n        train_mask = ~test_mask\n\n        X_train_raw = X_seq[train_mask]\n        y_train = y[train_mask]\n        subj_train = subjects[train_mask]\n\n        X_test_raw = X_seq[test_mask]\n        y_test = y[test_mask]\n\n        # normalization from TRAIN only (NO leakage)\n        mean, std = compute_train_norm_stats(X_train_raw)\n        X_train = apply_norm(X_train_raw, mean, std)\n        X_test  = apply_norm(X_test_raw,  mean, std)\n\n        print(\"Train seg:\", X_train.shape[0], \"| Test seg:\", X_test.shape[0],\n              \"| Train dist:\", np.bincount(y_train))\n\n        # train\n        model = train_one_fold(X_train, y_train, subj_train, n_channels=X_train.shape[-1])\n\n        # predict test\n        with torch.no_grad():\n            xb = torch.from_numpy(X_test).to(device)\n            logits = model(xb).cpu().numpy()\n            prob = 1.0 / (1.0 + np.exp(-logits))\n            y_pred = (prob >= 0.5).astype(int)\n\n        # segment-level metrics (برای باینری)\n        acc = accuracy_score(y_test, y_pred)\n        prec = precision_score(y_test, y_pred, zero_division=0)\n        rec = recall_score(y_test, y_pred, zero_division=0)\n        f1 = f1_score(y_test, y_pred, zero_division=0)\n        print(f\"Segment-level -> acc={acc:.4f}, prec={prec:.4f}, rec={rec:.4f}, f1={f1:.4f}\")\n        seg_metrics.append([acc, prec, rec, f1])\n\n        # subject-level majority vote\n        true_label = int(y_test[0])\n        pred_label = int(np.argmax(np.bincount(y_pred)))\n        acc_subj = 1.0 if pred_label == true_label else 0.0\n        print(f\"Subject-level -> true={true_label}, pred={pred_label}, acc={acc_subj:.4f}\")\n        subj_metrics.append([acc_subj, acc_subj, acc_subj, acc_subj])\n\n    seg_metrics = np.array(seg_metrics)\n    subj_metrics = np.array(subj_metrics)\n\n    print(\"\\n\" + \"=\"*60)\n    print(\"Average SEGMENT-level (LOSO):\")\n    print(f\"Accuracy : {seg_metrics[:,0].mean():.4f}\")\n    print(f\"Precision: {seg_metrics[:,1].mean():.4f}\")\n    print(f\"Recall   : {seg_metrics[:,2].mean():.4f}\")\n    print(f\"F1-score : {seg_metrics[:,3].mean():.4f}\")\n\n    print(\"\\n\" + \"=\"*60)\n    print(\"Average SUBJECT-level (LOSO, majority vote):\")\n    print(f\"Accuracy : {subj_metrics[:,0].mean():.4f}\")\n    print(f\"Precision: {subj_metrics[:,1].mean():.4f}\")\n    print(f\"Recall   : {subj_metrics[:,2].mean():.4f}\")\n    print(f\"F1-score : {subj_metrics[:,3].mean():.4f}\")\n\n\n# ======================\n# main\n# ======================\ndef main():\n    X, y, subjects, sfreq = load_task_edf_with_baseline(\n        DATA_FOLDER, INFO_CSV,\n        resample_to=RESAMPLE_TO,\n        use_baseline=USE_BASELINE\n    )\n\n    seg_X, seg_y, seg_subjects = make_segments(\n        X, y, subjects, sfreq,\n        window_sec=WINDOW_SEC,\n        overlap_ratio=OVERLAP_RATIO\n    )\n\n    evaluate_loso_bilstm(seg_X, seg_y, seg_subjects)\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T08:59:45.701209Z","iopub.execute_input":"2025-12-12T08:59:45.701435Z","iopub.status.idle":"2025-12-12T09:05:36.414040Z","shell.execute_reply.started":"2025-12-12T08:59:45.701409Z","shell.execute_reply":"2025-12-12T09:05:36.413180Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\nX: (36, 21, 7936) y: (36,) sfreq: 128.0\nSegments: (4356, 21, 256)\nUnique subjects: 36\n\n============================================================\nTest subject: Subject00\nTrain seg: 4235 | Test seg: 121 | Train dist: [1089 3146]\n  Epoch 01 | train_loss=0.3623 | val_f1=0.0000\n  Epoch 05 | train_loss=0.0354 | val_f1=0.4084\nSegment-level -> acc=0.0165, prec=0.0000, rec=0.0000, f1=0.0000\nSubject-level -> true=0, pred=1, acc=0.0000\n\n============================================================\nTest subject: Subject01\nTrain seg: 4235 | Test seg: 121 | Train dist: [1210 3025]\n  Epoch 01 | train_loss=0.4089 | val_f1=0.0000\n  Epoch 05 | train_loss=0.0714 | val_f1=0.2044\n  Epoch 10 | train_loss=0.0214 | val_f1=0.2188\nSegment-level -> acc=0.9008, prec=1.0000, rec=0.9008, f1=0.9478\nSubject-level -> true=1, pred=1, acc=1.0000\n\n============================================================\nTest subject: Subject02\nTrain seg: 4235 | Test seg: 121 | Train dist: [1210 3025]\n  Epoch 01 | train_loss=0.4044 | val_f1=0.0000\n  Epoch 05 | train_loss=0.1025 | val_f1=0.3007\nSegment-level -> acc=0.9835, prec=1.0000, rec=0.9835, f1=0.9917\nSubject-level -> true=1, pred=1, acc=1.0000\n\n============================================================\nTest subject: Subject03\nTrain seg: 4235 | Test seg: 121 | Train dist: [1210 3025]\n  Epoch 01 | train_loss=0.3981 | val_f1=0.0000\n  Epoch 05 | train_loss=0.1373 | val_f1=0.2461\nSegment-level -> acc=0.8926, prec=1.0000, rec=0.8926, f1=0.9432\nSubject-level -> true=1, pred=1, acc=1.0000\n\n============================================================\nTest subject: Subject04\nTrain seg: 4235 | Test seg: 121 | Train dist: [1089 3146]\n  Epoch 01 | train_loss=0.3699 | val_f1=0.0000\n  Epoch 05 | train_loss=0.0721 | val_f1=0.2878\n  Epoch 10 | train_loss=0.0178 | val_f1=0.2686\nSegment-level -> acc=0.0826, prec=0.0000, rec=0.0000, f1=0.0000\nSubject-level -> true=0, pred=1, acc=0.0000\n\n============================================================\nTest subject: Subject05\nTrain seg: 4235 | Test seg: 121 | Train dist: [1210 3025]\n  Epoch 01 | train_loss=0.4064 | val_f1=0.0000\n  Epoch 05 | train_loss=0.0781 | val_f1=0.2509\n  Epoch 10 | train_loss=0.0156 | val_f1=0.2721\nSegment-level -> acc=0.8430, prec=1.0000, rec=0.8430, f1=0.9148\nSubject-level -> true=1, pred=1, acc=1.0000\n\n============================================================\nTest subject: Subject06\nTrain seg: 4235 | Test seg: 121 | Train dist: [1089 3146]\n  Epoch 01 | train_loss=0.3569 | val_f1=0.0000\n  Epoch 05 | train_loss=0.0986 | val_f1=0.2509\n  Epoch 10 | train_loss=0.0195 | val_f1=0.2876\nSegment-level -> acc=0.4959, prec=0.0000, rec=0.0000, f1=0.0000\nSubject-level -> true=0, pred=1, acc=0.0000\n\n============================================================\nTest subject: Subject07\nTrain seg: 4235 | Test seg: 121 | Train dist: [1210 3025]\n  Epoch 01 | train_loss=0.4064 | val_f1=0.0000\n  Epoch 05 | train_loss=0.1110 | val_f1=0.2048\n  Epoch 10 | train_loss=0.0175 | val_f1=0.2469\nSegment-level -> acc=0.8430, prec=1.0000, rec=0.8430, f1=0.9148\nSubject-level -> true=1, pred=1, acc=1.0000\n\n============================================================\nTest subject: Subject08\nTrain seg: 4235 | Test seg: 121 | Train dist: [1210 3025]\n  Epoch 01 | train_loss=0.3998 | val_f1=0.0000\n  Epoch 05 | train_loss=0.1150 | val_f1=0.1328\n  Epoch 10 | train_loss=0.0158 | val_f1=0.2176\nSegment-level -> acc=0.9752, prec=1.0000, rec=0.9752, f1=0.9874\nSubject-level -> true=1, pred=1, acc=1.0000\n\n============================================================\nTest subject: Subject09\nTrain seg: 4235 | Test seg: 121 | Train dist: [1089 3146]\n  Epoch 01 | train_loss=0.3582 | val_f1=0.0000\n  Epoch 05 | train_loss=0.0827 | val_f1=0.1371\n  Epoch 10 | train_loss=0.0137 | val_f1=0.3639\nSegment-level -> acc=0.0413, prec=0.0000, rec=0.0000, f1=0.0000\nSubject-level -> true=0, pred=1, acc=0.0000\n\n============================================================\nTest subject: Subject10\nTrain seg: 4235 | Test seg: 121 | Train dist: [1089 3146]\n  Epoch 01 | train_loss=0.3591 | val_f1=0.0000\n  Epoch 05 | train_loss=0.0628 | val_f1=0.5838\n  Epoch 10 | train_loss=0.0187 | val_f1=0.6438\n  Epoch 15 | train_loss=0.0076 | val_f1=0.5839\nSegment-level -> acc=0.1901, prec=0.0000, rec=0.0000, f1=0.0000\nSubject-level -> true=0, pred=1, acc=0.0000\n\n============================================================\nTest subject: Subject11\nTrain seg: 4235 | Test seg: 121 | Train dist: [1210 3025]\n  Epoch 01 | train_loss=0.4064 | val_f1=0.0000\n  Epoch 05 | train_loss=0.0888 | val_f1=0.1910\nSegment-level -> acc=0.9835, prec=1.0000, rec=0.9835, f1=0.9917\nSubject-level -> true=1, pred=1, acc=1.0000\n\n============================================================\nTest subject: Subject12\nTrain seg: 4235 | Test seg: 121 | Train dist: [1210 3025]\n  Epoch 01 | train_loss=0.4004 | val_f1=0.0000\n  Epoch 05 | train_loss=0.0983 | val_f1=0.2877\n  Epoch 10 | train_loss=0.0324 | val_f1=0.2163\n  Epoch 15 | train_loss=0.0163 | val_f1=0.4485\n  Epoch 20 | train_loss=0.0056 | val_f1=0.3240\nSegment-level -> acc=0.8264, prec=1.0000, rec=0.8264, f1=0.9050\nSubject-level -> true=1, pred=1, acc=1.0000\n\n============================================================\nTest subject: Subject13\nTrain seg: 4235 | Test seg: 121 | Train dist: [1210 3025]\n  Epoch 01 | train_loss=0.3990 | val_f1=0.0000\n  Epoch 05 | train_loss=0.0696 | val_f1=0.2594\nSegment-level -> acc=0.3967, prec=1.0000, rec=0.3967, f1=0.5680\nSubject-level -> true=1, pred=0, acc=0.0000\n\n============================================================\nTest subject: Subject14\nTrain seg: 4235 | Test seg: 121 | Train dist: [1089 3146]\n  Epoch 01 | train_loss=0.3994 | val_f1=0.0000\n  Epoch 05 | train_loss=0.0755 | val_f1=0.3144\n  Epoch 10 | train_loss=0.0132 | val_f1=0.5301\n  Epoch 15 | train_loss=0.0163 | val_f1=0.5480\nSegment-level -> acc=0.3306, prec=0.0000, rec=0.0000, f1=0.0000\nSubject-level -> true=0, pred=1, acc=0.0000\n\n============================================================\nTest subject: Subject15\nTrain seg: 4235 | Test seg: 121 | Train dist: [1210 3025]\n  Epoch 01 | train_loss=0.4361 | val_f1=0.0000\n  Epoch 05 | train_loss=0.0941 | val_f1=0.3501\nSegment-level -> acc=0.4545, prec=1.0000, rec=0.4545, f1=0.6250\nSubject-level -> true=1, pred=0, acc=0.0000\n\n============================================================\nTest subject: Subject16\nTrain seg: 4235 | Test seg: 121 | Train dist: [1210 3025]\n  Epoch 01 | train_loss=0.4440 | val_f1=0.0000\n  Epoch 05 | train_loss=0.0963 | val_f1=0.4831\n  Epoch 10 | train_loss=0.0158 | val_f1=0.6077\nSegment-level -> acc=0.6446, prec=1.0000, rec=0.6446, f1=0.7839\nSubject-level -> true=1, pred=1, acc=1.0000\n\n============================================================\nTest subject: Subject17\nTrain seg: 4235 | Test seg: 121 | Train dist: [1210 3025]\n  Epoch 01 | train_loss=0.4226 | val_f1=0.0754\n  Epoch 05 | train_loss=0.0965 | val_f1=0.4516\n  Epoch 10 | train_loss=0.0337 | val_f1=0.5300\n  Epoch 15 | train_loss=0.0157 | val_f1=0.4564\nSegment-level -> acc=0.8264, prec=1.0000, rec=0.8264, f1=0.9050\nSubject-level -> true=1, pred=1, acc=1.0000\n\n============================================================\nTest subject: Subject18\nTrain seg: 4235 | Test seg: 121 | Train dist: [1210 3025]\n  Epoch 01 | train_loss=0.4444 | val_f1=0.0000\n  Epoch 05 | train_loss=0.1180 | val_f1=0.4455\n  Epoch 10 | train_loss=0.0231 | val_f1=0.6185\n  Epoch 15 | train_loss=0.0184 | val_f1=0.5362\nSegment-level -> acc=0.3967, prec=1.0000, rec=0.3967, f1=0.5680\nSubject-level -> true=1, pred=0, acc=0.0000\n\n============================================================\nTest subject: Subject19\nTrain seg: 4235 | Test seg: 121 | Train dist: [1089 3146]\n  Epoch 01 | train_loss=0.3959 | val_f1=0.0000\n  Epoch 05 | train_loss=0.0731 | val_f1=0.6314\n  Epoch 10 | train_loss=0.0312 | val_f1=0.6023\nSegment-level -> acc=0.0083, prec=0.0000, rec=0.0000, f1=0.0000\nSubject-level -> true=0, pred=1, acc=0.0000\n\n============================================================\nTest subject: Subject20\nTrain seg: 4235 | Test seg: 121 | Train dist: [1210 3025]\n  Epoch 01 | train_loss=0.4414 | val_f1=0.0000\n  Epoch 05 | train_loss=0.0984 | val_f1=0.4139\n  Epoch 10 | train_loss=0.0346 | val_f1=0.5700\n  Epoch 15 | train_loss=0.0127 | val_f1=0.5605\nSegment-level -> acc=0.9752, prec=1.0000, rec=0.9752, f1=0.9874\nSubject-level -> true=1, pred=1, acc=1.0000\n\n============================================================\nTest subject: Subject21\nTrain seg: 4235 | Test seg: 121 | Train dist: [1089 3146]\n  Epoch 01 | train_loss=0.4110 | val_f1=0.0000\n  Epoch 05 | train_loss=0.1109 | val_f1=0.4217\n  Epoch 10 | train_loss=0.0441 | val_f1=0.4662\n  Epoch 15 | train_loss=0.0076 | val_f1=0.5582\nSegment-level -> acc=0.5702, prec=0.0000, rec=0.0000, f1=0.0000\nSubject-level -> true=0, pred=0, acc=1.0000\n\n============================================================\nTest subject: Subject22\nTrain seg: 4235 | Test seg: 121 | Train dist: [1089 3146]\n  Epoch 01 | train_loss=0.3991 | val_f1=0.0000\n  Epoch 05 | train_loss=0.1130 | val_f1=0.5468\n  Epoch 10 | train_loss=0.0232 | val_f1=0.5790\n  Epoch 15 | train_loss=0.0124 | val_f1=0.5976\nSegment-level -> acc=0.3306, prec=0.0000, rec=0.0000, f1=0.0000\nSubject-level -> true=0, pred=1, acc=0.0000\n\n============================================================\nTest subject: Subject23\nTrain seg: 4235 | Test seg: 121 | Train dist: [1210 3025]\n  Epoch 01 | train_loss=0.4318 | val_f1=0.0000\n  Epoch 05 | train_loss=0.0983 | val_f1=0.5154\n  Epoch 10 | train_loss=0.0381 | val_f1=0.4955\nSegment-level -> acc=0.9669, prec=1.0000, rec=0.9669, f1=0.9832\nSubject-level -> true=1, pred=1, acc=1.0000\n\n============================================================\nTest subject: Subject24\nTrain seg: 4235 | Test seg: 121 | Train dist: [1210 3025]\n  Epoch 01 | train_loss=0.4462 | val_f1=0.0000\n  Epoch 05 | train_loss=0.1152 | val_f1=0.4388\nSegment-level -> acc=0.8595, prec=1.0000, rec=0.8595, f1=0.9244\nSubject-level -> true=1, pred=1, acc=1.0000\n\n============================================================\nTest subject: Subject25\nTrain seg: 4235 | Test seg: 121 | Train dist: [1210 3025]\n  Epoch 01 | train_loss=0.4291 | val_f1=0.0000\n  Epoch 05 | train_loss=0.0967 | val_f1=0.6329\n  Epoch 10 | train_loss=0.0309 | val_f1=0.6316\n  Epoch 15 | train_loss=0.0316 | val_f1=0.6690\n  Epoch 20 | train_loss=0.0096 | val_f1=0.7435\n  Epoch 25 | train_loss=0.0090 | val_f1=0.7131\nSegment-level -> acc=0.4876, prec=1.0000, rec=0.4876, f1=0.6556\nSubject-level -> true=1, pred=0, acc=0.0000\n\n============================================================\nTest subject: Subject26\nTrain seg: 4235 | Test seg: 121 | Train dist: [1210 3025]\n  Epoch 01 | train_loss=0.4373 | val_f1=0.0000\n  Epoch 05 | train_loss=0.0856 | val_f1=0.6083\n  Epoch 10 | train_loss=0.0189 | val_f1=0.5852\nSegment-level -> acc=0.8347, prec=1.0000, rec=0.8347, f1=0.9099\nSubject-level -> true=1, pred=1, acc=1.0000\n\n============================================================\nTest subject: Subject27\nTrain seg: 4235 | Test seg: 121 | Train dist: [1210 3025]\n  Epoch 01 | train_loss=0.4354 | val_f1=0.0000\n  Epoch 05 | train_loss=0.1513 | val_f1=0.7714\n  Epoch 10 | train_loss=0.0208 | val_f1=0.8065\n  Epoch 15 | train_loss=0.0190 | val_f1=0.7937\nSegment-level -> acc=0.1240, prec=1.0000, rec=0.1240, f1=0.2206\nSubject-level -> true=1, pred=0, acc=0.0000\n\n============================================================\nTest subject: Subject28\nTrain seg: 4235 | Test seg: 121 | Train dist: [1210 3025]\n  Epoch 01 | train_loss=0.4312 | val_f1=0.0041\n  Epoch 05 | train_loss=0.1204 | val_f1=0.7070\n  Epoch 10 | train_loss=0.0573 | val_f1=0.6702\n  Epoch 15 | train_loss=0.0153 | val_f1=0.7541\nSegment-level -> acc=0.7769, prec=1.0000, rec=0.7769, f1=0.8744\nSubject-level -> true=1, pred=1, acc=1.0000\n\n============================================================\nTest subject: Subject29\nTrain seg: 4235 | Test seg: 121 | Train dist: [1210 3025]\n  Epoch 01 | train_loss=0.4412 | val_f1=0.0000\n  Epoch 05 | train_loss=0.1276 | val_f1=0.7755\n  Epoch 10 | train_loss=0.0444 | val_f1=0.6576\n  Epoch 15 | train_loss=0.0140 | val_f1=0.7815\nSegment-level -> acc=0.9008, prec=1.0000, rec=0.9008, f1=0.9478\nSubject-level -> true=1, pred=1, acc=1.0000\n\n============================================================\nTest subject: Subject30\nTrain seg: 4235 | Test seg: 121 | Train dist: [1089 3146]\n  Epoch 01 | train_loss=0.3982 | val_f1=0.0000\n  Epoch 05 | train_loss=0.1131 | val_f1=0.6887\n  Epoch 10 | train_loss=0.0374 | val_f1=0.7042\n  Epoch 15 | train_loss=0.0179 | val_f1=0.8112\n  Epoch 20 | train_loss=0.0157 | val_f1=0.7779\n  Epoch 25 | train_loss=0.0078 | val_f1=0.7366\nSegment-level -> acc=0.2975, prec=0.0000, rec=0.0000, f1=0.0000\nSubject-level -> true=0, pred=1, acc=0.0000\n\n============================================================\nTest subject: Subject31\nTrain seg: 4235 | Test seg: 121 | Train dist: [1210 3025]\n  Epoch 01 | train_loss=0.4251 | val_f1=0.3488\n  Epoch 05 | train_loss=0.1097 | val_f1=0.5951\n  Epoch 10 | train_loss=0.0432 | val_f1=0.6454\n  Epoch 15 | train_loss=0.0153 | val_f1=0.7001\nSegment-level -> acc=0.7686, prec=1.0000, rec=0.7686, f1=0.8692\nSubject-level -> true=1, pred=1, acc=1.0000\n\n============================================================\nTest subject: Subject32\nTrain seg: 4235 | Test seg: 121 | Train dist: [1210 3025]\n  Epoch 01 | train_loss=0.4350 | val_f1=0.0000\n  Epoch 05 | train_loss=0.1425 | val_f1=0.6597\n  Epoch 10 | train_loss=0.0410 | val_f1=0.7541\n  Epoch 15 | train_loss=0.0123 | val_f1=0.8290\n  Epoch 20 | train_loss=0.0185 | val_f1=0.8480\n  Epoch 25 | train_loss=0.0084 | val_f1=0.8303\nSegment-level -> acc=0.8760, prec=1.0000, rec=0.8760, f1=0.9339\nSubject-level -> true=1, pred=1, acc=1.0000\n\n============================================================\nTest subject: Subject33\nTrain seg: 4235 | Test seg: 121 | Train dist: [1210 3025]\n  Epoch 01 | train_loss=0.4355 | val_f1=0.0000\n  Epoch 05 | train_loss=0.1103 | val_f1=0.6700\n  Epoch 10 | train_loss=0.0567 | val_f1=0.7001\n  Epoch 15 | train_loss=0.0152 | val_f1=0.7446\n  Epoch 20 | train_loss=0.0054 | val_f1=0.8197\n  Epoch 25 | train_loss=0.0048 | val_f1=0.7786\nSegment-level -> acc=0.9752, prec=1.0000, rec=0.9752, f1=0.9874\nSubject-level -> true=1, pred=1, acc=1.0000\n\n============================================================\nTest subject: Subject34\nTrain seg: 4235 | Test seg: 121 | Train dist: [1210 3025]\n  Epoch 01 | train_loss=0.4318 | val_f1=0.0164\n  Epoch 05 | train_loss=0.1281 | val_f1=0.6919\n  Epoch 10 | train_loss=0.0426 | val_f1=0.6836\nSegment-level -> acc=0.8430, prec=1.0000, rec=0.8430, f1=0.9148\nSubject-level -> true=1, pred=1, acc=1.0000\n\n============================================================\nTest subject: Subject35\nTrain seg: 4235 | Test seg: 121 | Train dist: [1210 3025]\n  Epoch 01 | train_loss=0.4446 | val_f1=0.0000\n  Epoch 05 | train_loss=0.1767 | val_f1=0.5743\n  Epoch 10 | train_loss=0.0625 | val_f1=0.7908\n  Epoch 15 | train_loss=0.0147 | val_f1=0.7086\n  Epoch 20 | train_loss=0.0051 | val_f1=0.8033\nSegment-level -> acc=0.8926, prec=1.0000, rec=0.8926, f1=0.9432\nSubject-level -> true=1, pred=1, acc=1.0000\n\n============================================================\nAverage SEGMENT-level (LOSO):\nAccuracy : 0.6281\nPrecision: 0.7222\nRecall   : 0.5624\nF1-score : 0.6166\n\n============================================================\nAverage SUBJECT-level (LOSO, majority vote):\nAccuracy : 0.6111\nPrecision: 0.6111\nRecall   : 0.6111\nF1-score : 0.6111\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport random\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport mne\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n# ======================\n# تنظیمات کلی\n# ======================\nDATA_FOLDER = \"/kaggle/input/ahmadi-dataset\"\nINFO_CSV = f\"{DATA_FOLDER}/subject-info.csv\"\n\nRESAMPLE_TO = 128\nRANDOM_STATE = 42\n\nWINDOW_SEC = 2.0\nOVERLAP_RATIO = 0.75\n\nUSE_BASELINE = True\n\nBATCH_SIZE = 128\nEPOCHS = 40\nLR = 1e-3\nPATIENCE = 6\n\n# BiLSTM\nHIDDEN = 64\nNUM_LAYERS = 2\nDROPOUT = 0.3\n\n# --- Validation by SUBJECT\nVAL_SUBJECT_COUNT = 2          # دقیقاً 2 سابجکت برای ولید (از trainها)\nNOISY_VALIDATION = True       # حالت 2: ولید نویزی\nVAL_NOISE_STD = 0.05           # شدت نویز برای ولید (اگر NOISY_VALIDATION=True)\nVAL_SCALE_STD = 0.08           # شدت scale-jitter برای ولید نویزی (اختیاری)\n\n# ======================\n# Seed\n# ======================\ndef seed_everything(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\nseed_everything(RANDOM_STATE)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)\n\n\n# ======================\n# لود داده + baseline correction\n# ======================\ndef load_task_edf_with_baseline(folder_path, info_csv_path, resample_to=None,\n                                use_baseline=True):\n    folder = Path(folder_path)\n    if not folder.is_dir():\n        raise NotADirectoryError(f\"{folder_path} is not a valid directory\")\n\n    info_df = pd.read_csv(info_csv_path)\n    label_map = dict(zip(info_df[\"Subject\"], info_df[\"Count quality\"]))\n\n    X_list, y_list, subjects = [], [], []\n    sfreq = None\n\n    for subj_name in info_df[\"Subject\"]:\n        task_file = folder / f\"{subj_name}_2.edf\"\n        rest_file = folder / f\"{subj_name}_1.edf\"\n\n        if not task_file.is_file():\n            print(f\"Task file not found for {subj_name}, skipping.\")\n            continue\n\n        raw_task = mne.io.read_raw_edf(task_file, preload=True, verbose=False)\n\n        raw_rest = None\n        if use_baseline and rest_file.is_file():\n            raw_rest = mne.io.read_raw_edf(rest_file, preload=True, verbose=False)\n\n        if resample_to is not None:\n            raw_task.resample(resample_to)\n            if raw_rest is not None:\n                raw_rest.resample(resample_to)\n\n        if sfreq is None:\n            sfreq = raw_task.info[\"sfreq\"]\n\n        task_data = raw_task.get_data()  # (C, T_task)\n\n        if use_baseline and raw_rest is not None:\n            rest_data = raw_rest.get_data()  # (C, T_rest)\n            baseline = rest_data.mean(axis=1, keepdims=True)\n            task_data = task_data - baseline\n\n        X_list.append(task_data)\n        y_list.append(int(label_map[subj_name]))\n        subjects.append(subj_name)\n\n    if not X_list:\n        raise ValueError(\"No files loaded. Check paths/names.\")\n\n    lengths = [d.shape[1] for d in X_list]\n    min_len = min(lengths)\n\n    X = np.stack([d[:, :min_len] for d in X_list], axis=0)  # (N, C, T)\n    y = np.array(y_list, dtype=int)\n    subjects = np.array(subjects)\n\n    print(\"X:\", X.shape, \"y:\", y.shape, \"sfreq:\", sfreq)\n    return X, y, subjects, sfreq\n\n\n# ======================\n# Segment با overlap 75%\n# ======================\ndef make_segments(X, y, subjects, sfreq, window_sec=2.0, overlap_ratio=0.75):\n    N, C, T = X.shape\n    win_size = int(window_sec * sfreq)          # 256\n    step_sec = window_sec * (1.0 - overlap_ratio)\n    step = max(1, int(step_sec * sfreq))        # 64\n\n    seg_X, seg_y, seg_subj = [], [], []\n\n    for i in range(N):\n        data = X[i]  # (C, T)\n        lab = y[i]\n        sid = subjects[i]\n\n        start = 0\n        while start + win_size <= T:\n            seg = data[:, start:start+win_size]  # (C, win)\n            seg_X.append(seg)\n            seg_y.append(lab)\n            seg_subj.append(sid)\n            start += step\n\n    seg_X = np.stack(seg_X, axis=0)  # (Nseg, C, win)\n    seg_y = np.array(seg_y, dtype=int)\n    seg_subj = np.array(seg_subj)\n\n    print(\"Segments:\", seg_X.shape)\n    return seg_X, seg_y, seg_subj\n\n\n# ======================\n# Dataset + Augmentation\n# ======================\nclass EEGSegDataset(Dataset):\n    def __init__(self, X_seq, y, do_noise=False, do_scale=False,\n                 noise_std=0.02, scale_std=0.05):\n        \"\"\"\n        X_seq: (N, T, C) float32\n        y: (N,) int/float\n        \"\"\"\n        self.X = X_seq.astype(np.float32)\n        self.y = y.astype(np.float32)\n\n        self.do_noise = do_noise\n        self.do_scale = do_scale\n        self.noise_std = float(noise_std)\n        self.scale_std = float(scale_std)\n\n    def __len__(self):\n        return len(self.y)\n\n    def _augment(self, x):\n        # x: (T, C)\n        if self.do_noise:\n            noise = np.random.normal(0, self.noise_std, size=x.shape).astype(np.float32)\n            x = x + noise\n        if self.do_scale:\n            scale = (1.0 + np.random.normal(0, self.scale_std, size=(1, x.shape[1]))).astype(np.float32)\n            x = x * scale\n        return x\n\n    def __getitem__(self, idx):\n        x = self.X[idx]\n        if self.do_noise or self.do_scale:\n            x = self._augment(x.copy())\n        y = self.y[idx]\n        return torch.from_numpy(x), torch.tensor(y)\n\n\n# ======================\n# BiLSTM Model\n# ======================\nclass BiLSTMClassifier(nn.Module):\n    def __init__(self, n_channels, hidden=64, num_layers=2, dropout=0.3):\n        super().__init__()\n        self.lstm = nn.LSTM(\n            input_size=n_channels,\n            hidden_size=hidden,\n            num_layers=num_layers,\n            batch_first=True,\n            bidirectional=True,\n            dropout=dropout if num_layers > 1 else 0.0\n        )\n        self.norm = nn.LayerNorm(hidden * 2)\n        self.fc = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Linear(hidden * 2, 64),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(64, 1)\n        )\n\n    def forward(self, x):\n        # x: (B, T, C)\n        out, _ = self.lstm(x)       # (B, T, 2H)\n        out = out.mean(dim=1)       # mean pooling over time -> (B, 2H)\n        out = self.norm(out)\n        logits = self.fc(out).squeeze(1)  # (B,)\n        return logits\n\n\n# ======================\n# Normalization without leakage\n# ======================\ndef compute_train_norm_stats(X_train_seq):\n    flat = X_train_seq.reshape(-1, X_train_seq.shape[-1])  # (N*T, C)\n    mean = flat.mean(axis=0, keepdims=True)\n    std = flat.std(axis=0, keepdims=True) + 1e-6\n    return mean.astype(np.float32), std.astype(np.float32)\n\ndef apply_norm(X_seq, mean, std):\n    return ((X_seq - mean) / std).astype(np.float32)\n\n\n# ======================\n# Val split: دقیقاً K سابجکت رندوم از train\n# ======================\ndef make_subject_val_split_exact_k(train_subjects, k=2, seed=42):\n    uniq = np.unique(train_subjects)\n    if len(uniq) <= k:\n        # اگر تعداد سابجکت‌های train کم بود، مجبوریم 1 تا ولید کنیم\n        k = max(1, len(uniq) - 1)\n\n    rng = np.random.RandomState(seed)\n    rng.shuffle(uniq)\n    val_subj = set(uniq[:k])\n    val_mask = np.array([s in val_subj for s in train_subjects])\n    return val_mask, val_subj\n\n\n# ======================\n# Train with early stopping\n# ======================\ndef train_one_fold(X_train, y_train, subj_train, n_channels, fold_seed,\n                   noisy_validation=False):\n    # validation: 2 subject random\n    val_mask, val_subj = make_subject_val_split_exact_k(\n        subj_train, k=VAL_SUBJECT_COUNT, seed=fold_seed\n    )\n    tr_mask = ~val_mask\n\n    X_tr, y_tr = X_train[tr_mask], y_train[tr_mask]\n    X_val, y_val = X_train[val_mask], y_train[val_mask]\n\n    # class imbalance\n    n_pos = int((y_tr == 1).sum())\n    n_neg = int((y_tr == 0).sum())\n    pos_weight = torch.tensor([n_neg / max(1, n_pos)], device=device, dtype=torch.float32)\n\n    # sampler (balanced batches)\n    weights = np.where(y_tr == 1, n_neg / max(1, n_pos), 1.0).astype(np.float32)\n    sampler = WeightedRandomSampler(weights=weights, num_samples=len(weights), replacement=True)\n\n    # Train augmentation (معمولاً مفید برای LOSO)\n    ds_tr = EEGSegDataset(\n        X_tr, y_tr,\n        do_noise=True, do_scale=True,\n        noise_std=0.02, scale_std=0.05\n    )\n\n    # Validation:\n    # - حالت معمول: بدون augmentation\n    # - حالت نویزی: عمداً ولید را نویزی می‌کنیم\n    if noisy_validation:\n        ds_val = EEGSegDataset(\n            X_val, y_val,\n            do_noise=True, do_scale=True,\n            noise_std=VAL_NOISE_STD, scale_std=VAL_SCALE_STD\n        )\n    else:\n        ds_val = EEGSegDataset(X_val, y_val, do_noise=False, do_scale=False)\n\n    dl_tr = DataLoader(ds_tr, batch_size=BATCH_SIZE, sampler=sampler, num_workers=2, pin_memory=True)\n    dl_val = DataLoader(ds_val, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n\n    model = BiLSTMClassifier(\n        n_channels=n_channels, hidden=HIDDEN, num_layers=NUM_LAYERS, dropout=DROPOUT\n    ).to(device)\n\n    opt = torch.optim.Adam(model.parameters(), lr=LR)\n    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n\n    best_f1 = -1\n    best_state = None\n    patience = 0\n\n    print(f\"  Validation subjects: {sorted(list(val_subj))} | noisy_val={noisy_validation}\")\n\n    for epoch in range(1, EPOCHS + 1):\n        model.train()\n        tr_losses = []\n\n        for xb, yb in dl_tr:\n            xb = xb.to(device)\n            yb = yb.to(device)\n\n            opt.zero_grad()\n            logits = model(xb)\n            loss = criterion(logits, yb)\n            loss.backward()\n            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            opt.step()\n            tr_losses.append(loss.item())\n\n        # validation\n        model.eval()\n        all_pred, all_true = [], []\n        with torch.no_grad():\n            for xb, yb in dl_val:\n                xb = xb.to(device)\n                logits = model(xb)\n                prob = torch.sigmoid(logits).cpu().numpy()\n                pred = (prob >= 0.5).astype(int)\n                all_pred.append(pred)\n                all_true.append(yb.numpy().astype(int))\n\n        all_pred = np.concatenate(all_pred)\n        all_true = np.concatenate(all_true)\n\n        f1 = f1_score(all_true, all_pred, zero_division=0)\n        avg_loss = float(np.mean(tr_losses))\n\n        if epoch == 1 or epoch % 5 == 0:\n            print(f\"  Epoch {epoch:02d} | train_loss={avg_loss:.4f} | val_f1={f1:.4f}\")\n\n        if f1 > best_f1:\n            best_f1 = f1\n            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n            patience = 0\n        else:\n            patience += 1\n            if patience >= PATIENCE:\n                break\n\n    model.load_state_dict(best_state)\n    model.eval()\n    return model\n\n\n# ======================\n# LOSO Evaluation\n# ======================\ndef evaluate_loso_bilstm(seg_X, seg_y, seg_subjects, noisy_validation=False):\n    # (N, C, T) -> (N, T, C)\n    X_seq = np.transpose(seg_X, (0, 2, 1)).astype(np.float32)  # (Nseg, 256, 21)\n    y = seg_y.astype(int)\n    subjects = seg_subjects\n\n    uniq_subj = np.unique(subjects)\n    print(\"Unique subjects:\", len(uniq_subj))\n\n    seg_metrics = []\n    subj_metrics = []\n\n    for test_subj in uniq_subj:\n        print(\"\\n\" + \"=\"*60)\n        print(\"Test subject:\", test_subj)\n\n        test_mask = (subjects == test_subj)\n        train_mask = ~test_mask\n\n        X_train_raw = X_seq[train_mask]\n        y_train = y[train_mask]\n        subj_train = subjects[train_mask]\n\n        X_test_raw = X_seq[test_mask]\n        y_test = y[test_mask]\n\n        # normalization from TRAIN only\n        mean, std = compute_train_norm_stats(X_train_raw)\n        X_train = apply_norm(X_train_raw, mean, std)\n        X_test  = apply_norm(X_test_raw,  mean, std)\n\n        print(\"Train seg:\", X_train.shape[0], \"| Test seg:\", X_test.shape[0],\n              \"| Train dist:\", np.bincount(y_train))\n\n        # fold-specific seed (برای اینکه انتخاب 2 سابجکت ولید هر fold رندوم ولی reproducible باشد)\n        # SubjectXX -> XX\n        try:\n            sid_num = int(str(test_subj).replace(\"Subject\", \"\"))\n        except:\n            sid_num = 0\n        fold_seed = RANDOM_STATE + 1000 + sid_num\n\n        model = train_one_fold(\n            X_train, y_train, subj_train,\n            n_channels=X_train.shape[-1],\n            fold_seed=fold_seed,\n            noisy_validation=noisy_validation\n        )\n\n        # predict test\n        with torch.no_grad():\n            xb = torch.from_numpy(X_test).to(device)\n            logits = model(xb).cpu().numpy()\n            prob = 1.0 / (1.0 + np.exp(-logits))\n            y_pred = (prob >= 0.5).astype(int)\n\n        # segment-level metrics\n        acc = accuracy_score(y_test, y_pred)\n        prec = precision_score(y_test, y_pred, zero_division=0)\n        rec = recall_score(y_test, y_pred, zero_division=0)\n        f1 = f1_score(y_test, y_pred, zero_division=0)\n        print(f\"Segment-level -> acc={acc:.4f}, prec={prec:.4f}, rec={rec:.4f}, f1={f1:.4f}\")\n        seg_metrics.append([acc, prec, rec, f1])\n\n        # subject-level majority vote\n        true_label = int(y_test[0])\n        pred_label = int(np.argmax(np.bincount(y_pred)))\n        acc_subj = 1.0 if pred_label == true_label else 0.0\n        print(f\"Subject-level -> true={true_label}, pred={pred_label}, acc={acc_subj:.4f}\")\n        subj_metrics.append([acc_subj, acc_subj, acc_subj, acc_subj])\n\n    seg_metrics = np.array(seg_metrics)\n    subj_metrics = np.array(subj_metrics)\n\n    print(\"\\n\" + \"=\"*60)\n    print(\"Average SEGMENT-level (LOSO):\")\n    print(f\"Accuracy : {seg_metrics[:,0].mean():.4f}\")\n    print(f\"Precision: {seg_metrics[:,1].mean():.4f}\")\n    print(f\"Recall   : {seg_metrics[:,2].mean():.4f}\")\n    print(f\"F1-score : {seg_metrics[:,3].mean():.4f}\")\n\n    print(\"\\n\" + \"=\"*60)\n    print(\"Average SUBJECT-level (LOSO, majority vote):\")\n    print(f\"Accuracy : {subj_metrics[:,0].mean():.4f}\")\n    print(f\"Precision: {subj_metrics[:,1].mean():.4f}\")\n    print(f\"Recall   : {subj_metrics[:,2].mean():.4f}\")\n    print(f\"F1-score : {subj_metrics[:,3].mean():.4f}\")\n\n\n# ======================\n# main\n# ======================\ndef main():\n    X, y, subjects, sfreq = load_task_edf_with_baseline(\n        DATA_FOLDER, INFO_CSV,\n        resample_to=RESAMPLE_TO,\n        use_baseline=USE_BASELINE\n    )\n\n    seg_X, seg_y, seg_subjects = make_segments(\n        X, y, subjects, sfreq,\n        window_sec=WINDOW_SEC,\n        overlap_ratio=OVERLAP_RATIO\n    )\n\n    evaluate_loso_bilstm(seg_X, seg_y, seg_subjects, noisy_validation=NOISY_VALIDATION)\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport random\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport mne\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n# ======================\n# تنظیمات کلی\n# ======================\nDATA_FOLDER = \"/kaggle/input/ahmadi-dataset\"\nINFO_CSV = f\"{DATA_FOLDER}/subject-info.csv\"\n\nRESAMPLE_TO = 128\nRANDOM_STATE = 42\n\nWINDOW_SEC = 2.0\nOVERLAP_RATIO = 0.75\n\nUSE_BASELINE = True\n\nBATCH_SIZE = 128\nEPOCHS = 40\nLR = 1e-3\nPATIENCE = 6\n\n# BiLSTM\nHIDDEN = 64\nNUM_LAYERS = 2\nDROPOUT = 0.3\n\n# --- Validation by SUBJECT\nVAL_SUBJECT_COUNT = 4          # دقیقاً 2 سابجکت برای ولید (از trainها)\nNOISY_VALIDATION = False       # حالت 2: ولید نویزی\nVAL_NOISE_STD = 0.05           # شدت نویز برای ولید (اگر NOISY_VALIDATION=True)\nVAL_SCALE_STD = 0.08           # شدت scale-jitter برای ولید نویزی (اختیاری)\n\n# ======================\n# Seed\n# ======================\ndef seed_everything(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\nseed_everything(RANDOM_STATE)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)\n\n\n# ======================\n# لود داده + baseline correction\n# ======================\ndef load_task_edf_with_baseline(folder_path, info_csv_path, resample_to=None,\n                                use_baseline=True):\n    folder = Path(folder_path)\n    if not folder.is_dir():\n        raise NotADirectoryError(f\"{folder_path} is not a valid directory\")\n\n    info_df = pd.read_csv(info_csv_path)\n    label_map = dict(zip(info_df[\"Subject\"], info_df[\"Count quality\"]))\n\n    X_list, y_list, subjects = [], [], []\n    sfreq = None\n\n    for subj_name in info_df[\"Subject\"]:\n        task_file = folder / f\"{subj_name}_2.edf\"\n        rest_file = folder / f\"{subj_name}_1.edf\"\n\n        if not task_file.is_file():\n            print(f\"Task file not found for {subj_name}, skipping.\")\n            continue\n\n        raw_task = mne.io.read_raw_edf(task_file, preload=True, verbose=False)\n\n        raw_rest = None\n        if use_baseline and rest_file.is_file():\n            raw_rest = mne.io.read_raw_edf(rest_file, preload=True, verbose=False)\n\n        if resample_to is not None:\n            raw_task.resample(resample_to)\n            if raw_rest is not None:\n                raw_rest.resample(resample_to)\n\n        if sfreq is None:\n            sfreq = raw_task.info[\"sfreq\"]\n\n        task_data = raw_task.get_data()  # (C, T_task)\n\n        if use_baseline and raw_rest is not None:\n            rest_data = raw_rest.get_data()  # (C, T_rest)\n            baseline = rest_data.mean(axis=1, keepdims=True)\n            task_data = task_data - baseline\n\n        X_list.append(task_data)\n        y_list.append(int(label_map[subj_name]))\n        subjects.append(subj_name)\n\n    if not X_list:\n        raise ValueError(\"No files loaded. Check paths/names.\")\n\n    lengths = [d.shape[1] for d in X_list]\n    min_len = min(lengths)\n\n    X = np.stack([d[:, :min_len] for d in X_list], axis=0)  # (N, C, T)\n    y = np.array(y_list, dtype=int)\n    subjects = np.array(subjects)\n\n    print(\"X:\", X.shape, \"y:\", y.shape, \"sfreq:\", sfreq)\n    return X, y, subjects, sfreq\n\n\n# ======================\n# Segment با overlap 75%\n# ======================\ndef make_segments(X, y, subjects, sfreq, window_sec=2.0, overlap_ratio=0.75):\n    N, C, T = X.shape\n    win_size = int(window_sec * sfreq)          # 256\n    step_sec = window_sec * (1.0 - overlap_ratio)\n    step = max(1, int(step_sec * sfreq))        # 64\n\n    seg_X, seg_y, seg_subj = [], [], []\n\n    for i in range(N):\n        data = X[i]  # (C, T)\n        lab = y[i]\n        sid = subjects[i]\n\n        start = 0\n        while start + win_size <= T:\n            seg = data[:, start:start+win_size]  # (C, win)\n            seg_X.append(seg)\n            seg_y.append(lab)\n            seg_subj.append(sid)\n            start += step\n\n    seg_X = np.stack(seg_X, axis=0)  # (Nseg, C, win)\n    seg_y = np.array(seg_y, dtype=int)\n    seg_subj = np.array(seg_subj)\n\n    print(\"Segments:\", seg_X.shape)\n    return seg_X, seg_y, seg_subj\n\n\n# ======================\n# Dataset + Augmentation\n# ======================\nclass EEGSegDataset(Dataset):\n    def __init__(self, X_seq, y, do_noise=False, do_scale=False,\n                 noise_std=0.02, scale_std=0.05):\n        \"\"\"\n        X_seq: (N, T, C) float32\n        y: (N,) int/float\n        \"\"\"\n        self.X = X_seq.astype(np.float32)\n        self.y = y.astype(np.float32)\n\n        self.do_noise = do_noise\n        self.do_scale = do_scale\n        self.noise_std = float(noise_std)\n        self.scale_std = float(scale_std)\n\n    def __len__(self):\n        return len(self.y)\n\n    def _augment(self, x):\n        # x: (T, C)\n        if self.do_noise:\n            noise = np.random.normal(0, self.noise_std, size=x.shape).astype(np.float32)\n            x = x + noise\n        if self.do_scale:\n            scale = (1.0 + np.random.normal(0, self.scale_std, size=(1, x.shape[1]))).astype(np.float32)\n            x = x * scale\n        return x\n\n    def __getitem__(self, idx):\n        x = self.X[idx]\n        if self.do_noise or self.do_scale:\n            x = self._augment(x.copy())\n        y = self.y[idx]\n        return torch.from_numpy(x), torch.tensor(y)\n\n\n# ======================\n# BiLSTM Model\n# ======================\nclass BiLSTMClassifier(nn.Module):\n    def __init__(self, n_channels, hidden=64, num_layers=2, dropout=0.3):\n        super().__init__()\n        self.lstm = nn.LSTM(\n            input_size=n_channels,\n            hidden_size=hidden,\n            num_layers=num_layers,\n            batch_first=True,\n            bidirectional=True,\n            dropout=dropout if num_layers > 1 else 0.0\n        )\n        self.norm = nn.LayerNorm(hidden * 2)\n        self.fc = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Linear(hidden * 2, 64),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(64, 1)\n        )\n\n    def forward(self, x):\n        # x: (B, T, C)\n        out, _ = self.lstm(x)       # (B, T, 2H)\n        out = out.mean(dim=1)       # mean pooling over time -> (B, 2H)\n        out = self.norm(out)\n        logits = self.fc(out).squeeze(1)  # (B,)\n        return logits\n\n\n# ======================\n# Normalization without leakage\n# ======================\ndef compute_train_norm_stats(X_train_seq):\n    flat = X_train_seq.reshape(-1, X_train_seq.shape[-1])  # (N*T, C)\n    mean = flat.mean(axis=0, keepdims=True)\n    std = flat.std(axis=0, keepdims=True) + 1e-6\n    return mean.astype(np.float32), std.astype(np.float32)\n\ndef apply_norm(X_seq, mean, std):\n    return ((X_seq - mean) / std).astype(np.float32)\n\n\n# ======================\n# Val split: دقیقاً K سابجکت رندوم از train\n# ======================\ndef make_subject_val_split_exact_k(train_subjects, k=2, seed=42):\n    uniq = np.unique(train_subjects)\n    if len(uniq) <= k:\n        # اگر تعداد سابجکت‌های train کم بود، مجبوریم 1 تا ولید کنیم\n        k = max(1, len(uniq) - 1)\n\n    rng = np.random.RandomState(seed)\n    rng.shuffle(uniq)\n    val_subj = set(uniq[:k])\n    val_mask = np.array([s in val_subj for s in train_subjects])\n    return val_mask, val_subj\n\n\n# ======================\n# Train with early stopping\n# ======================\ndef train_one_fold(X_train, y_train, subj_train, n_channels, fold_seed,\n                   noisy_validation=False):\n    # validation: 2 subject random\n    val_mask, val_subj = make_subject_val_split_exact_k(\n        subj_train, k=VAL_SUBJECT_COUNT, seed=fold_seed\n    )\n    tr_mask = ~val_mask\n\n    X_tr, y_tr = X_train[tr_mask], y_train[tr_mask]\n    X_val, y_val = X_train[val_mask], y_train[val_mask]\n\n    # class imbalance\n    n_pos = int((y_tr == 1).sum())\n    n_neg = int((y_tr == 0).sum())\n    pos_weight = torch.tensor([n_neg / max(1, n_pos)], device=device, dtype=torch.float32)\n\n    # sampler (balanced batches)\n    weights = np.where(y_tr == 1, n_neg / max(1, n_pos), 1.0).astype(np.float32)\n    sampler = WeightedRandomSampler(weights=weights, num_samples=len(weights), replacement=True)\n\n    # Train augmentation (معمولاً مفید برای LOSO)\n    ds_tr = EEGSegDataset(\n        X_tr, y_tr,\n        do_noise=True, do_scale=True,\n        noise_std=0.02, scale_std=0.05\n    )\n\n    # Validation:\n    # - حالت معمول: بدون augmentation\n    # - حالت نویزی: عمداً ولید را نویزی می‌کنیم\n    if noisy_validation:\n        ds_val = EEGSegDataset(\n            X_val, y_val,\n            do_noise=True, do_scale=True,\n            noise_std=VAL_NOISE_STD, scale_std=VAL_SCALE_STD\n        )\n    else:\n        ds_val = EEGSegDataset(X_val, y_val, do_noise=False, do_scale=False)\n\n    dl_tr = DataLoader(ds_tr, batch_size=BATCH_SIZE, sampler=sampler, num_workers=2, pin_memory=True)\n    dl_val = DataLoader(ds_val, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n\n    model = BiLSTMClassifier(\n        n_channels=n_channels, hidden=HIDDEN, num_layers=NUM_LAYERS, dropout=DROPOUT\n    ).to(device)\n\n    opt = torch.optim.Adam(model.parameters(), lr=LR)\n    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n\n    best_f1 = -1\n    best_state = None\n    patience = 0\n\n    print(f\"  Validation subjects: {sorted(list(val_subj))} | noisy_val={noisy_validation}\")\n\n    for epoch in range(1, EPOCHS + 1):\n        model.train()\n        tr_losses = []\n\n        for xb, yb in dl_tr:\n            xb = xb.to(device)\n            yb = yb.to(device)\n\n            opt.zero_grad()\n            logits = model(xb)\n            loss = criterion(logits, yb)\n            loss.backward()\n            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            opt.step()\n            tr_losses.append(loss.item())\n\n        # validation\n        model.eval()\n        all_pred, all_true = [], []\n        with torch.no_grad():\n            for xb, yb in dl_val:\n                xb = xb.to(device)\n                logits = model(xb)\n                prob = torch.sigmoid(logits).cpu().numpy()\n                pred = (prob >= 0.5).astype(int)\n                all_pred.append(pred)\n                all_true.append(yb.numpy().astype(int))\n\n        all_pred = np.concatenate(all_pred)\n        all_true = np.concatenate(all_true)\n\n        f1 = f1_score(all_true, all_pred, zero_division=0)\n        avg_loss = float(np.mean(tr_losses))\n\n        if epoch == 1 or epoch % 5 == 0:\n            print(f\"  Epoch {epoch:02d} | train_loss={avg_loss:.4f} | val_f1={f1:.4f}\")\n\n        if f1 > best_f1:\n            best_f1 = f1\n            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n            patience = 0\n        else:\n            patience += 1\n            if patience >= PATIENCE:\n                break\n\n    model.load_state_dict(best_state)\n    model.eval()\n    return model\n\n\n# ======================\n# LOSO Evaluation\n# ======================\ndef evaluate_loso_bilstm(seg_X, seg_y, seg_subjects, noisy_validation=False):\n    # (N, C, T) -> (N, T, C)\n    X_seq = np.transpose(seg_X, (0, 2, 1)).astype(np.float32)  # (Nseg, 256, 21)\n    y = seg_y.astype(int)\n    subjects = seg_subjects\n\n    uniq_subj = np.unique(subjects)\n    print(\"Unique subjects:\", len(uniq_subj))\n\n    seg_metrics = []\n    subj_metrics = []\n\n    for test_subj in uniq_subj:\n        print(\"\\n\" + \"=\"*60)\n        print(\"Test subject:\", test_subj)\n\n        test_mask = (subjects == test_subj)\n        train_mask = ~test_mask\n\n        X_train_raw = X_seq[train_mask]\n        y_train = y[train_mask]\n        subj_train = subjects[train_mask]\n\n        X_test_raw = X_seq[test_mask]\n        y_test = y[test_mask]\n\n        # normalization from TRAIN only\n        mean, std = compute_train_norm_stats(X_train_raw)\n        X_train = apply_norm(X_train_raw, mean, std)\n        X_test  = apply_norm(X_test_raw,  mean, std)\n\n        print(\"Train seg:\", X_train.shape[0], \"| Test seg:\", X_test.shape[0],\n              \"| Train dist:\", np.bincount(y_train))\n\n        # fold-specific seed (برای اینکه انتخاب 2 سابجکت ولید هر fold رندوم ولی reproducible باشد)\n        # SubjectXX -> XX\n        try:\n            sid_num = int(str(test_subj).replace(\"Subject\", \"\"))\n        except:\n            sid_num = 0\n        fold_seed = RANDOM_STATE + 1000 + sid_num\n\n        model = train_one_fold(\n            X_train, y_train, subj_train,\n            n_channels=X_train.shape[-1],\n            fold_seed=fold_seed,\n            noisy_validation=noisy_validation\n        )\n\n        # predict test\n        with torch.no_grad():\n            xb = torch.from_numpy(X_test).to(device)\n            logits = model(xb).cpu().numpy()\n            prob = 1.0 / (1.0 + np.exp(-logits))\n            y_pred = (prob >= 0.5).astype(int)\n\n        # segment-level metrics\n        acc = accuracy_score(y_test, y_pred)\n        prec = precision_score(y_test, y_pred, zero_division=0)\n        rec = recall_score(y_test, y_pred, zero_division=0)\n        f1 = f1_score(y_test, y_pred, zero_division=0)\n        print(f\"Segment-level -> acc={acc:.4f}, prec={prec:.4f}, rec={rec:.4f}, f1={f1:.4f}\")\n        seg_metrics.append([acc, prec, rec, f1])\n\n        # subject-level majority vote\n        true_label = int(y_test[0])\n        pred_label = int(np.argmax(np.bincount(y_pred)))\n        acc_subj = 1.0 if pred_label == true_label else 0.0\n        print(f\"Subject-level -> true={true_label}, pred={pred_label}, acc={acc_subj:.4f}\")\n        subj_metrics.append([acc_subj, acc_subj, acc_subj, acc_subj])\n\n    seg_metrics = np.array(seg_metrics)\n    subj_metrics = np.array(subj_metrics)\n\n    print(\"\\n\" + \"=\"*60)\n    print(\"Average SEGMENT-level (LOSO):\")\n    print(f\"Accuracy : {seg_metrics[:,0].mean():.4f}\")\n    print(f\"Precision: {seg_metrics[:,1].mean():.4f}\")\n    print(f\"Recall   : {seg_metrics[:,2].mean():.4f}\")\n    print(f\"F1-score : {seg_metrics[:,3].mean():.4f}\")\n\n    print(\"\\n\" + \"=\"*60)\n    print(\"Average SUBJECT-level (LOSO, majority vote):\")\n    print(f\"Accuracy : {subj_metrics[:,0].mean():.4f}\")\n    print(f\"Precision: {subj_metrics[:,1].mean():.4f}\")\n    print(f\"Recall   : {subj_metrics[:,2].mean():.4f}\")\n    print(f\"F1-score : {subj_metrics[:,3].mean():.4f}\")\n\n\n# ======================\n# main\n# ======================\ndef main():\n    X, y, subjects, sfreq = load_task_edf_with_baseline(\n        DATA_FOLDER, INFO_CSV,\n        resample_to=RESAMPLE_TO,\n        use_baseline=USE_BASELINE\n    )\n\n    seg_X, seg_y, seg_subjects = make_segments(\n        X, y, subjects, sfreq,\n        window_sec=WINDOW_SEC,\n        overlap_ratio=OVERLAP_RATIO\n    )\n\n    evaluate_loso_bilstm(seg_X, seg_y, seg_subjects, noisy_validation=NOISY_VALIDATION)\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}